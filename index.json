[{"content":"keepalived高可用介绍及改造方案 具体配置-passwd方式 192.168.5.31 # keepalived.conf ! Configuration File for keepalived global_defs { router_id db01 } vrrp_sync_group G1 { group { VI_1 } } vrrp_script check_run { script \u0026#34;/etc/keepalived/mysql_check.sh\u0026#34; interval 10 fall 3 } vrrp_instance VI_1 { state BACKUP interface ens32 virtual_router_id 51 priority 100 nopreempt advert_int 3 authentication { auth_type PASS auth_pass admin#2023 } track_interface { ens32 } unicast_src_ip 192.168.5.31 unicast_peer { 192.168.5.32 } track_script { check_run } virtual_ipaddress { 192.168.5.190 } notify_master \u0026#34;/etc/keepalived/change_to_master.sh\u0026#34; notify_backup \u0026#34;/etc/keepalived/change_to_backup.sh\u0026#34; notify_fault \u0026#34;/etc/keepalived/change_to_backup.sh\u0026#34; } # mysql_check.sh #!/bin/bash ############################################### #修改此处配置 #用户需要权限 #grant REPLICATION CLIENT on *.* to mon_user@\u0026#39;%\u0026#39; ; export HOME=\u0026#39;/root\u0026#39; master_ip=\u0026#39;192.168.5.31\u0026#39; master_port=\u0026#39;3306\u0026#39; master_mycom=\u0026#39;mysql\u0026#39; rep_ip=\u0026#39;192.168.5.32\u0026#39; rep_port=\u0026#39;3306\u0026#39; #记录日志位置 log_time=`date \u0026#39;+%Y%m%d\u0026#39;` check_log=\u0026#39;/etc/keepalived/keepalived_\u0026#39;${log_time}\u0026#39;.log\u0026#39; check_log_error=\u0026#39;/var/log/messages\u0026#39; find /etc/keepalived/ -name \u0026#34;keepalived_*.log\u0026#34; -mtime +7|xargs rm -rf ################################################# logging_log() #input:$1 输入警告级别 $2 输入警告日志字符串 { dt_time=`date \u0026#39;+%Y-%m-%d-%H:%M:%S\u0026#39;` echo \u0026#34;keepalive logging ${dt_time} $1:$2\u0026#34; \u0026gt;\u0026gt; ${check_log} } logging_log_error() #input:$1 输入警告级别 $2 输入警告日志字符串 { dt_time=`date \u0026#39;+%Y-%m-%d-%H:%M:%S\u0026#39;` echo \u0026#34;keepalive logging ${dt_time} $1:$2\u0026#34; \u0026gt;\u0026gt; ${check_log_error} } check_pid() #input:$1检查的进程关键字比如mysqld #return:1 检测成功，但是需要注意有多个mysqld进程(物理服务器跑了多个实例) 0 检测失败，没有mysqld进程 { mysqld_cnt=`ps -ef|grep -w $1|grep -v \u0026#39;grep\u0026#39;|wc -l` if test ${mysqld_cnt} -ge 2 ;then logging_log \u0026#39;warn\u0026#39; \u0026#39;more than one mysqld process!\u0026#39; return 1 #由于mysqld检测出有多个那么，只是告警而不做任何动作 fi if test ${mysqld_cnt} -lt 1 ;then logging_log \u0026#39;err\u0026#39; \u0026#39;mysqld process is crash!!\u0026#39; logging_log_error \u0026#39;err\u0026#39; \u0026#39;mysqld process is crash!!\u0026#39; return 0 #检测mysqld已经不存在则直接进入从库检测流程 fi logging_log \u0026#39;info\u0026#39; \u0026#39;one mysqld process check sucess.\u0026#39; return 1 #检测成功，进入语句检测 } check_sta() #input:NULL #return:0 失败，主库测试语句不能执行 1 成功，主库测试语句可以执行 { #将stderr 写入到日志文件中，比如连接失败 #${master_mycom} -h ${master_ip} -P ${master_port} -u ${master_user} -p${master_password} -e \u0026#39;select now()\u0026#39; ${master_mycom} -uroot -p123456 -h ${master_ip} -e \u0026#39;select now()\u0026#39; if [ $? -eq 0 ]; then logging_log \u0026#39;info\u0026#39; \u0026#39;master:test select now() sucess.\u0026#39; return 1 #检测成功，测试语句能够执行 else logging_log \u0026#39;err\u0026#39; master:\u0026#39;test select now() fail!!\u0026#39; logging_log_error \u0026#39;err\u0026#39; master:\u0026#39;test select now() fail!!\u0026#39; return 0 #检测失败，测试语句不能够执行 fi } check_rep_apply_all() #input:NULL #return value:0 失败，检测不能切换 1 成功，检测可以切换 { #先测试从库是否可以连接 ${master_mycom} -uroot -p123456 -h ${rep_ip} -e \u0026#39;select now()\u0026#39; if [ $? -eq 0 ]; then logging_log \u0026#39;info\u0026#39; \u0026#39;rep:test select now() sucess.\u0026#39; #检测成功，测试语句能够执行,继续执行 else logging_log \u0026#39;err\u0026#39; \u0026#39;rep:test select now() fail!!\u0026#39; logging_log_error \u0026#39;err\u0026#39; \u0026#39;rep:test select now() fail!!\u0026#39; return 0 #检测失败，测试语句不能够执行 fi #不放到一条语句进行检测，多次检测可以提高检测的准确性 Rep_Master_ip=`${master_mycom} -uroot -p123456 -h ${rep_ip} -e \u0026#34;show slave status\\G\u0026#34; | grep -w Master_Host | awk -F\u0026#34;: \u0026#34; \u0026#39;{print $2}\u0026#39;` Rep_Master_port=`${master_mycom} -uroot -p123456 -h ${rep_ip} -e \u0026#34;show slave status\\G\u0026#34; | grep -w Master_Port | awk -F\u0026#34;: \u0026#34; \u0026#39;{print $2}\u0026#39;` Slave_IO_Running=`${master_mycom} -uroot -p123456 -h ${rep_ip} -e \u0026#34;show slave status\\G\u0026#34; | grep -w Slave_IO_Running | awk -F\u0026#34;: \u0026#34; \u0026#39;{print $2}\u0026#39;` Master_Log_File=`${master_mycom} -uroot -p123456 -h ${rep_ip} -e \u0026#34;show slave status\\G\u0026#34; | grep -w Master_Log_File | awk -F\u0026#34;: \u0026#34; \u0026#39;{print $2}\u0026#39;` Relay_Master_Log_File=`${master_mycom} -uroot -p123456 -h ${rep_ip} -e \u0026#34;show slave status\\G\u0026#34; | grep -w Relay_Master_Log_File | awk -F\u0026#34;: \u0026#34; \u0026#39;{print $2}\u0026#39;` Read_Master_Log_Pos=`${master_mycom} -uroot -p123456 -h ${rep_ip} -e \u0026#34;show slave status\\G\u0026#34; | grep -w Read_Master_Log_Pos | awk -F\u0026#34;: \u0026#34; \u0026#39;{print $2}\u0026#39;` Exec_Master_Log_Pos=`${master_mycom} -uroot -p123456 -h ${rep_ip} -e \u0026#34;show slave status\\G\u0026#34; | grep -w Exec_Master_Log_Pos | awk -F\u0026#34;: \u0026#34; \u0026#39;{print $2}\u0026#39;` Seconds_Behind_Master=`${master_mycom} -uroot -p123456 -h ${rep_ip} -e \u0026#34;show slave status\\G\u0026#34; | grep -w Seconds_Behind_Master | awk -F\u0026#34;: \u0026#34; \u0026#39;{print $2}\u0026#39;` if [ ${Rep_Master_ip} != ${master_ip} ]; then logging_log \u0026#39;err\u0026#39; \u0026#34;rep:this rep ip ${Rep_Master_ip} is not master ip:${master_ip} !!\u0026#34; logging_log_error \u0026#39;err\u0026#39; \u0026#34;rep:this rep ip ${Rep_Master_ip} is not master ip:${master_ip} !!\u0026#34; return 0 #检测失败，从库连接主库的ip和填写的主库ip不一致 fi if [ ${Rep_Master_port} != ${master_port} ]; then logging_log \u0026#39;err\u0026#39; \u0026#34;rep:this rep port ${Rep_Master_port} is not master port:${master_port} !!\u0026#34; logging_log_error \u0026#39;err\u0026#39; \u0026#34;rep:this rep port ${Rep_Master_port} is not master port:${master_port} !!\u0026#34; return 0 #检测失败，从库连接主库的端口和填写的主库端口不一致 fi if [ ${Master_Log_File} != ${Relay_Master_Log_File} ]; then logging_log \u0026#39;err\u0026#39; \u0026#34;rep:this rep read binlog:${Master_Log_File} is not eq execute binlog:${Relay_Master_Log_File} !!\u0026#34; logging_log_error \u0026#39;err\u0026#39; \u0026#34;rep:this rep read binlog:${Master_Log_File} is not eq execute binlog:${Relay_Master_Log_File} !!\u0026#34; return 0 #检测失败，从库读取的binlog文件和执行的binlog文件不一致 fi if [ ${Read_Master_Log_Pos} != ${Exec_Master_Log_Pos} ]; then logging_log \u0026#39;err\u0026#39; \u0026#34;rep:this rep read pos:${Read_Master_Log_Pos} is not eq execute pos:${Exec_Master_Log_Pos} !!\u0026#34; logging_log_error \u0026#39;err\u0026#39; \u0026#34;rep:this rep read pos:${Read_Master_Log_Pos} is not eq execute pos:${Exec_Master_Log_Pos} !!\u0026#34; return 0 #检测失败，从库执行的binlog pos和执行的binlog pos不一致 fi sleep 5 ##是否需要检测IO线程状态，只有在IO线程运行异常的情况下进行切换 if [ ${Slave_IO_Running} == \u0026#39;Yes\u0026#39; ]; then logging_log \u0026#39;err\u0026#39; \u0026#34;rep:this rep IO thread is running !!\u0026#34; logging_log_error \u0026#39;err\u0026#39; \u0026#34;rep:this rep IO thread is running !!\u0026#34; return 0 fi if [ ${Seconds_Behind_Master} == \u0026#39;NULL\u0026#39; -o ${Seconds_Behind_Master} == \u0026#39;0\u0026#39; ] ; then #检测成功什么都不做 echo \u0026#39;normal\u0026#39; else logging_log \u0026#39;err\u0026#39; \u0026#34;rep:this rep Behind Master:${Seconds_Behind_Master}s blocking switch !!\u0026#34; logging_log_error \u0026#39;err\u0026#39; \u0026#34;rep:this rep Behind Master:${Seconds_Behind_Master}s blocking switch !!\u0026#34; return 0 #检测失败,延迟不为0 fi logging_log \u0026#39;info\u0026#39; \u0026#34;rep:check sucess.master ip:${Rep_Master_ip} master port:${Rep_Master_port} io status:${Slave_IO_Running} read_pos:${Master_Log_File} ${Read_Master_Log_Pos} exe_pos:${Relay_Master_Log_File} ${Exec_Master_Log_Pos} Seconds_Behind_Master:${Seconds_Behind_Master}.\u0026#34; return 1 #返回检测从库应用状态成功 } #main 脚本开始 #第一步检查主库状态 #1、检测主库进程是否存在，调用check_pid #2、检测主库是否可以跑简单的语句，调用check_sta #如果检测成功说明不需要切换，直接返回 0给 keepdalived #通常不会进入第二步判断 #第二步检测从库是否可以切换 #如果不能切换，同样返回 0给 keepalived #如果可以切换，则返回 1给 keepalived ret=0 echo \u0026#39;---\u0026#39; \u0026gt;\u0026gt; ${check_log} #第一步检查主库状态 check_pid \u0026#34;mysqld\u0026#34; if [ $? == \u0026#39;1\u0026#39; ] ;then #1.mysqld检测成功 2.多个mysqld实例 都返回为1 check_sta if [ $? == \u0026#39;0\u0026#39; ];then #检测sql命令失败 ret=1 #失败 else ret=0 #成功 fi else #检测没有mysqld存在 ret=1 #失败 fi #检测主库状态正常，不需要进行切换，返回1给keepalived if [ $ret == \u0026#39;0\u0026#39; ];then echo \u0026#34;主库正常\u0026#34; exit 0 fi #第二步检测从库是否可以切换 #检测主库状态不正常，需要检查从库状态是否满足切换条件 if [ $ret == \u0026#39;1\u0026#39; ];then check_rep_apply_all if [ $? == \u0026#39;1\u0026#39; ];then #检测从库状态成功 echo \u0026#34;从库可以切换\u0026#34; exit 1 else echo \u0026#34;从库不可以切换\u0026#34; #检测失败不能切换 exit 0 fi fi # change_to_master.sh #!/bin/bash ############################################### #修改此处配置 #用户需要权限 #查看从库状态需要REPLICATION CLIENT权限 #修改参数需要super权限 export HOME=\u0026#39;/root/\u0026#39; NIC=\u0026#39;ens32\u0026#39; VIP=\u0026#39;192.168.5.190\u0026#39; GATEWAY=\u0026#39;192.168.5.1\u0026#39; ip=\u0026#39;192.168.5.31\u0026#39; port=\u0026#39;3306\u0026#39; mycom=\u0026#39;mysql\u0026#39; #记录日志位置 log_time=`date \u0026#39;+%Y%m%d\u0026#39;` check_log=\u0026#39;/etc/keepalived/keepalived_\u0026#39;${log_time}\u0026#39;.log\u0026#39; check_log_error=\u0026#39;/var/log/messages\u0026#39; #循环检测次数 check_loop=10 #循环检测间隔秒数 loop_time=5 ################################################# logging_log() #input:$1 输入警告级别 $2 输入警告日志字符串 { dt_time=`date \u0026#39;+%Y-%m-%d-%H:%M:%S\u0026#39;` echo \u0026#34;keepalive loggong ${dt_time} $1:$2\u0026#34; \u0026gt;\u0026gt; ${check_log} } logging_log_error() #input:$1 输入警告级别 $2 输入警告日志字符串 { dt_time=`date \u0026#39;+%Y-%m-%d-%H:%M:%S\u0026#39;` echo \u0026#34;keepalive logging ${dt_time} $1:$2\u0026#34; \u0026gt;\u0026gt; ${check_log_error} } check_rep_apply_all() #input:NULL #return value:0 失败，检测不能切换 1 成功，检测可以切换 { #先测试从库是否可以连接 ${mycom} -uroot -p123456 -h ${ip} -e \u0026#39;select now()\u0026#39; if [ $? -eq 0 ]; then logging_log \u0026#39;info\u0026#39; \u0026#39;master check:test select now() sucess.\u0026#39; #检测成功，测试语句能够执行,继续执行 else logging_log \u0026#39;err\u0026#39; \u0026#39;master check:test select now() fail!!\u0026#39; logging_log_error \u0026#39;err\u0026#39; \u0026#39;master check:test select now() fail!!\u0026#39; return 0 #检测失败，测试语句不能够执行 fi #不放到一条语句进行检测，多次检测可以提高检测的准确性 Slave_IO_Running=`${mycom} -uroot -p123456 -h ${ip} -e \u0026#34;show slave status\\G\u0026#34; | grep -w Slave_IO_Running | awk -F\u0026#34;: \u0026#34; \u0026#39;{print $2}\u0026#39;` Master_Log_File=`${mycom} -uroot -p123456 -h ${ip} -e \u0026#34;show slave status\\G\u0026#34; | grep -w Master_Log_File | awk -F\u0026#34;: \u0026#34; \u0026#39;{print $2}\u0026#39;` Relay_Master_Log_File=`${mycom} -uroot -p123456 -h ${ip} -e \u0026#34;show slave status\\G\u0026#34; | grep -w Relay_Master_Log_File | awk -F\u0026#34;: \u0026#34; \u0026#39;{print $2}\u0026#39;` Read_Master_Log_Pos=`${mycom} -uroot -p123456 -h ${ip} -e \u0026#34;show slave status\\G\u0026#34; | grep -w Read_Master_Log_Pos | awk -F\u0026#34;: \u0026#34; \u0026#39;{print $2}\u0026#39;` Exec_Master_Log_Pos=`${mycom} -uroot -p123456 -h ${ip} -e \u0026#34;show slave status\\G\u0026#34; | grep -w Exec_Master_Log_Pos | awk -F\u0026#34;: \u0026#34; \u0026#39;{print $2}\u0026#39;` Seconds_Behind_Master=`${mycom} -uroot -p123456 -h ${ip} -e \u0026#34;show slave status\\G\u0026#34; | grep -w Seconds_Behind_Master | awk -F\u0026#34;: \u0026#34; \u0026#39;{print $2}\u0026#39;` if [ ${Master_Log_File} != ${Relay_Master_Log_File} ]; then logging_log \u0026#39;err\u0026#39; \u0026#34;rep:this rep read binlog:${Master_Log_File} is not eq execute binlog:${Relay_Master_Log_File} !!\u0026#34; logging_log_error \u0026#39;err\u0026#39; \u0026#34;rep:this rep read binlog:${Master_Log_File} is not eq execute binlog:${Relay_Master_Log_File} !!\u0026#34; return 0 #检测失败，从库读取的binlog文件和执行的binlog文件不一致 fi if [ ${Read_Master_Log_Pos} != ${Exec_Master_Log_Pos} ]; then logging_log \u0026#39;err\u0026#39; \u0026#34;master check:this rep read pos:${Read_Master_Log_Pos} is not eq execute pos:${Exec_Master_Log_Pos} !!\u0026#34; logging_log_error \u0026#39;err\u0026#39; \u0026#34;master check:this rep read pos:${Read_Master_Log_Pos} is not eq execute pos:${Exec_Master_Log_Pos} !!\u0026#34; return 0 #检测失败，从库执行的binlog pos和执行的binlog pos不一致 fi ##是否需要检测IO线程状态，只有在IO线程运行异常的情况下进行切换 if [ ${Slave_IO_Running} == \u0026#39;Yes\u0026#39; ]; then logging_log \u0026#39;err\u0026#39; \u0026#34;master check:this rep IO thread is running !!\u0026#34; logging_log_error \u0026#39;err\u0026#39; \u0026#34;master check:this rep IO thread is running !!\u0026#34; return 0 fi if [ ${Seconds_Behind_Master} == \u0026#39;NULL\u0026#39; -o ${Seconds_Behind_Master} == \u0026#39;0\u0026#39; ] ; then #检测成功什么都不做 echo \u0026#34;1\u0026#34; else logging_log \u0026#39;err\u0026#39; \u0026#34;rep:this rep Behind Master:${Seconds_Behind_Master}s blocking switch !!\u0026#34; logging_log_error \u0026#39;err\u0026#39; \u0026#34;rep:this rep Behind Master:${Seconds_Behind_Master}s blocking switch !!\u0026#34; return 0 #检测失败,延迟不为0 fi logging_log \u0026#39;info\u0026#39; \u0026#34;master check:check sucess.io status:${Slave_IO_Running} read_pos:${Master_Log_File} ${Read_Master_Log_Pos} exe_pos:${Relay_Master_Log_File} ${Exec_Master_Log_Pos} Seconds_Behind_Master:${Seconds_Behind_Master}.\u0026#34; return 1 #返回从库检测状态正常 } with_out_readonly() #input:NULL #returnvalue:NULL { logging_log \u0026#39;warn\u0026#39; \u0026#34;set read_only=off\u0026#34; logging_log_error \u0026#39;warn\u0026#39; \u0026#34;set read_only=off\u0026#34; ${mycom} -uroot -p123456 -h ${ip} -e \u0026#34;set global read_only=0\u0026#34; ${mycom} -uroot -p123456 -h ${ip} -e \u0026#34;set global super_read_only=0\u0026#34; } #main 脚本开始 #用于检测切换后是否可以打开read only选项 echo \u0026#39;---master-check---\u0026#39; \u0026gt;\u0026gt;${check_log} i=1 while [ $i -le ${check_loop} ] ##循环次数 do check_rep_apply_all if [ $? -eq 1 ];then with_out_readonly #打开readonly选项 break #跳出循环 else echo \u0026#34;--check fail,loop $i times--\u0026#34; \u0026gt;\u0026gt;${check_log} #输出检测失败次数 sleep ${loop_time} #睡眠时间 fi let i+=1 #判断值+1 done logging_log \u0026#39;info\u0026#39; \u0026#39;start to arping on change_to_master\u0026#39; logging_log_error \u0026#39;info\u0026#39; \u0026#39;start to arping on change_to_master\u0026#39; /sbin/arping -I $NIC -c 5 -s $VIP $GATEWAY # change_to_backup.sh #!/bin/bash export HOME=\u0026#39;/root/\u0026#39; NIC=\u0026#39;ens32\u0026#39; VIP=\u0026#39;192.168.5.190\u0026#39; GATEWAY=\u0026#39;192.168.5.1\u0026#39; ip=\u0026#39;192.168.5.31\u0026#39; port=\u0026#39;3306\u0026#39; mycom=\u0026#39;mysql\u0026#39; log_time=`date \u0026#39;+%Y%m%d\u0026#39;` check_log=\u0026#39;/etc/keepalived/keepalived_\u0026#39;${log_time}\u0026#39;.log\u0026#39; check_log_error=\u0026#39;/var/log/messages\u0026#39; logging_log() #input:$1 输入警告级别 $2 输入警告日志字符串 { dt_time=`date \u0026#39;+%Y-%m-%d-%H:%M:%S\u0026#39;` echo \u0026#34;keepalive logging ${dt_time} $1:$2\u0026#34; \u0026gt;\u0026gt; ${check_log} } logging_log_error() #input:$1 输入警告级别 $2 输入警告日志字符串 { dt_time=`date \u0026#39;+%Y-%m-%d-%H:%M:%S\u0026#39;` echo \u0026#34;keepalive logging ${dt_time} $1:$2\u0026#34; \u0026gt;\u0026gt; ${check_log_error} } with_readonly() #input:NULL #returnvalue:NULL { logging_log \u0026#39;warn\u0026#39; \u0026#34;set read_only=on\u0026#34; logging_log_error \u0026#39;warn\u0026#39; \u0026#34;set read_only=on\u0026#34; ${mycom} -uroot -p123456 -h ${ip} -e \u0026#34;set global read_only=1\u0026#34; ${mycom} -uroot -p123456 -h ${ip} -e \u0026#34;set global super_read_only=1\u0026#34; } #main 脚本开始 #用于检测切换后是否可以打开read only选项 echo \u0026#39;---backup-set---\u0026#39; \u0026gt;\u0026gt;${check_log} with_readonly #设置readonly logging_log \u0026#39;info\u0026#39; \u0026#39;start to arping on change_to_backup\u0026#39; logging_log_error \u0026#39;info\u0026#39; \u0026#39;start to arping on change_to_backup\u0026#39; /sbin/arping -I $NIC -c 5 -s $VIP $GATEWAY 192.168.5.32 # keepalived.conf ! Configuration File for keepalived global_defs { router_id db02 } vrrp_sync_group G1 { group { VI_1 } } vrrp_script check_run { script \u0026#34;/etc/keepalived/mysql_check.sh\u0026#34; interval 10 fall 3 } vrrp_instance VI_1 { state BACKUP interface ens32 virtual_router_id 51 priority 90 nopreempt advert_int 3 authentication { auth_type PASS auth_pass admin#2023 } track_interface { ens32 } unicast_src_ip 192.168.5.32 unicast_peer { 192.168.5.31 } track_script { check_run } virtual_ipaddress { 192.168.5.190 } notify_master \u0026#34;/etc/keepalived/change_to_master.sh\u0026#34; notify_backup \u0026#34;/etc/keepalived/change_to_backup.sh\u0026#34; notify_fault \u0026#34;/etc/keepalived/change_to_backup.sh\u0026#34; } # mysql_check.sh #!/bin/bash ############################################### #修改此处配置 #用户需要权限 #grant REPLICATION CLIENT on *.* to mon_user@\u0026#39;%\u0026#39; ; export HOME=\u0026#39;/root\u0026#39; master_ip=\u0026#39;192.168.5.32\u0026#39; master_port=\u0026#39;3306\u0026#39; master_mycom=\u0026#39;mysql\u0026#39; rep_ip=\u0026#39;192.168.5.31\u0026#39; rep_port=\u0026#39;3306\u0026#39; #记录日志位置 log_time=`date \u0026#39;+%Y%m%d\u0026#39;` check_log=\u0026#39;/etc/keepalived/keepalived_\u0026#39;${log_time}\u0026#39;.log\u0026#39; check_log_error=\u0026#39;/var/log/messages\u0026#39; find /etc/keepalived/ -name \u0026#34;keepalived_*.log\u0026#34; -mtime +7|xargs rm -rf ################################################# logging_log() #input:$1 输入警告级别 $2 输入警告日志字符串 { dt_time=`date \u0026#39;+%Y-%m-%d-%H:%M:%S\u0026#39;` echo \u0026#34;keepalive logging ${dt_time} $1:$2\u0026#34; \u0026gt;\u0026gt; ${check_log} } logging_log_error() #input:$1 输入警告级别 $2 输入警告日志字符串 { dt_time=`date \u0026#39;+%Y-%m-%d-%H:%M:%S\u0026#39;` echo \u0026#34;keepalive logging ${dt_time} $1:$2\u0026#34; \u0026gt;\u0026gt; ${check_log_error} } check_pid() #input:$1检查的进程关键字比如mysqld #return:1 检测成功，但是需要注意有多个mysqld进程(物理服务器跑了多个实例) 0 检测失败，没有mysqld进程 { mysqld_cnt=`ps -ef|grep -w $1|grep -v \u0026#39;grep\u0026#39;|wc -l` if test ${mysqld_cnt} -ge 2 ;then logging_log \u0026#39;warn\u0026#39; \u0026#39;more than one mysqld process!\u0026#39; return 1 #由于mysqld检测出有多个那么，只是告警而不做任何动作 fi if test ${mysqld_cnt} -lt 1 ;then logging_log \u0026#39;err\u0026#39; \u0026#39;mysqld process is crash!!\u0026#39; logging_log_error \u0026#39;err\u0026#39; \u0026#39;mysqld process is crash!!\u0026#39; return 0 #检测mysqld已经不存在则直接进入从库检测流程 fi logging_log \u0026#39;info\u0026#39; \u0026#39;one mysqld process check sucess.\u0026#39; return 1 #检测成功，进入语句检测 } check_sta() #input:NULL #return:0 失败，主库测试语句不能执行 1 成功，主库测试语句可以执行 { #将stderr 写入到日志文件中，比如连接失败 #${master_mycom} -h ${master_ip} -P ${master_port} -u ${master_user} -p${master_password} -e \u0026#39;select now()\u0026#39; ${master_mycom} -uroot -p123456 -h ${master_ip} -e \u0026#39;select now()\u0026#39; if [ $? -eq 0 ]; then logging_log \u0026#39;info\u0026#39; \u0026#39;master:test select now() sucess.\u0026#39; return 1 #检测成功，测试语句能够执行 else logging_log \u0026#39;err\u0026#39; master:\u0026#39;test select now() fail!!\u0026#39; logging_log_error \u0026#39;err\u0026#39; master:\u0026#39;test select now() fail!!\u0026#39; return 0 #检测失败，测试语句不能够执行 fi } check_rep_apply_all() #input:NULL #return value:0 失败，检测不能切换 1 成功，检测可以切换 { #先测试从库是否可以连接 ${master_mycom} -uroot -p123456 -h ${rep_ip} -e \u0026#39;select now()\u0026#39; if [ $? -eq 0 ]; then logging_log \u0026#39;info\u0026#39; \u0026#39;rep:test select now() sucess.\u0026#39; #检测成功，测试语句能够执行,继续执行 else logging_log \u0026#39;err\u0026#39; \u0026#39;rep:test select now() fail!!\u0026#39; logging_log_error \u0026#39;err\u0026#39; \u0026#39;rep:test select now() fail!!\u0026#39; return 0 #检测失败，测试语句不能够执行 fi #不放到一条语句进行检测，多次检测可以提高检测的准确性 Rep_Master_ip=`${master_mycom} -uroot -p123456 -h ${rep_ip} -e \u0026#34;show slave status\\G\u0026#34; | grep -w Master_Host | awk -F\u0026#34;: \u0026#34; \u0026#39;{print $2}\u0026#39;` Rep_Master_port=`${master_mycom} -uroot -p123456 -h ${rep_ip} -e \u0026#34;show slave status\\G\u0026#34; | grep -w Master_Port | awk -F\u0026#34;: \u0026#34; \u0026#39;{print $2}\u0026#39;` Slave_IO_Running=`${master_mycom} -uroot -p123456 -h ${rep_ip} -e \u0026#34;show slave status\\G\u0026#34; | grep -w Slave_IO_Running | awk -F\u0026#34;: \u0026#34; \u0026#39;{print $2}\u0026#39;` Master_Log_File=`${master_mycom} -uroot -p123456 -h ${rep_ip} -e \u0026#34;show slave status\\G\u0026#34; | grep -w Master_Log_File | awk -F\u0026#34;: \u0026#34; \u0026#39;{print $2}\u0026#39;` Relay_Master_Log_File=`${master_mycom} -uroot -p123456 -h ${rep_ip} -e \u0026#34;show slave status\\G\u0026#34; | grep -w Relay_Master_Log_File | awk -F\u0026#34;: \u0026#34; \u0026#39;{print $2}\u0026#39;` Read_Master_Log_Pos=`${master_mycom} -uroot -p123456 -h ${rep_ip} -e \u0026#34;show slave status\\G\u0026#34; | grep -w Read_Master_Log_Pos | awk -F\u0026#34;: \u0026#34; \u0026#39;{print $2}\u0026#39;` Exec_Master_Log_Pos=`${master_mycom} -uroot -p123456 -h ${rep_ip} -e \u0026#34;show slave status\\G\u0026#34; | grep -w Exec_Master_Log_Pos | awk -F\u0026#34;: \u0026#34; \u0026#39;{print $2}\u0026#39;` Seconds_Behind_Master=`${master_mycom} -uroot -p123456 -h ${rep_ip} -e \u0026#34;show slave status\\G\u0026#34; | grep -w Seconds_Behind_Master | awk -F\u0026#34;: \u0026#34; \u0026#39;{print $2}\u0026#39;` if [ ${Rep_Master_ip} != ${master_ip} ]; then logging_log \u0026#39;err\u0026#39; \u0026#34;rep:this rep ip ${Rep_Master_ip} is not master ip:${master_ip} !!\u0026#34; logging_log_error \u0026#39;err\u0026#39; \u0026#34;rep:this rep ip ${Rep_Master_ip} is not master ip:${master_ip} !!\u0026#34; return 0 #检测失败，从库连接主库的ip和填写的主库ip不一致 fi if [ ${Rep_Master_port} != ${master_port} ]; then logging_log \u0026#39;err\u0026#39; \u0026#34;rep:this rep port ${Rep_Master_port} is not master port:${master_port} !!\u0026#34; logging_log_error \u0026#39;err\u0026#39; \u0026#34;rep:this rep port ${Rep_Master_port} is not master port:${master_port} !!\u0026#34; return 0 #检测失败，从库连接主库的端口和填写的主库端口不一致 fi if [ ${Master_Log_File} != ${Relay_Master_Log_File} ]; then logging_log \u0026#39;err\u0026#39; \u0026#34;rep:this rep read binlog:${Master_Log_File} is not eq execute binlog:${Relay_Master_Log_File} !!\u0026#34; logging_log_error \u0026#39;err\u0026#39; \u0026#34;rep:this rep read binlog:${Master_Log_File} is not eq execute binlog:${Relay_Master_Log_File} !!\u0026#34; return 0 #检测失败，从库读取的binlog文件和执行的binlog文件不一致 fi if [ ${Read_Master_Log_Pos} != ${Exec_Master_Log_Pos} ]; then logging_log \u0026#39;err\u0026#39; \u0026#34;rep:this rep read pos:${Read_Master_Log_Pos} is not eq execute pos:${Exec_Master_Log_Pos} !!\u0026#34; logging_log_error \u0026#39;err\u0026#39; \u0026#34;rep:this rep read pos:${Read_Master_Log_Pos} is not eq execute pos:${Exec_Master_Log_Pos} !!\u0026#34; return 0 #检测失败，从库执行的binlog pos和执行的binlog pos不一致 fi sleep 5 ##是否需要检测IO线程状态，只有在IO线程运行异常的情况下进行切换 if [ ${Slave_IO_Running} == \u0026#39;Yes\u0026#39; ]; then logging_log \u0026#39;err\u0026#39; \u0026#34;rep:this rep IO thread is running !!\u0026#34; logging_log_error \u0026#39;err\u0026#39; \u0026#34;rep:this rep IO thread is running !!\u0026#34; return 0 fi if [ ${Seconds_Behind_Master} == \u0026#39;NULL\u0026#39; -o ${Seconds_Behind_Master} == \u0026#39;0\u0026#39; ] ; then #检测成功什么都不做 echo \u0026#39;normal\u0026#39; else logging_log \u0026#39;err\u0026#39; \u0026#34;rep:this rep Behind Master:${Seconds_Behind_Master}s blocking switch !!\u0026#34; logging_log_error \u0026#39;err\u0026#39; \u0026#34;rep:this rep Behind Master:${Seconds_Behind_Master}s blocking switch !!\u0026#34; return 0 #检测失败,延迟不为0 fi logging_log \u0026#39;info\u0026#39; \u0026#34;rep:check sucess.master ip:${Rep_Master_ip} master port:${Rep_Master_port} io status:${Slave_IO_Running} read_pos:${Master_Log_File} ${Read_Master_Log_Pos} exe_pos:${Relay_Master_Log_File} ${Exec_Master_Log_Pos} Seconds_Behind_Master:${Seconds_Behind_Master}.\u0026#34; return 1 #返回检测从库应用状态成功 } #main 脚本开始 #第一步检查主库状态 #1、检测主库进程是否存在，调用check_pid #2、检测主库是否可以跑简单的语句，调用check_sta #如果检测成功说明不需要切换，直接返回 0给 keepdalived #通常不会进入第二步判断 #第二步检测从库是否可以切换 #如果不能切换，同样返回 0给 keepalived #如果可以切换，则返回 1给 keepalived ret=0 echo \u0026#39;---\u0026#39; \u0026gt;\u0026gt; ${check_log} #第一步检查主库状态 check_pid \u0026#34;mysqld\u0026#34; if [ $? == \u0026#39;1\u0026#39; ] ;then #1.mysqld检测成功 2.多个mysqld实例 都返回为1 check_sta if [ $? == \u0026#39;0\u0026#39; ];then #检测sql命令失败 ret=1 #失败 else ret=0 #成功 fi else #检测没有mysqld存在 ret=1 #失败 fi #检测主库状态正常，不需要进行切换，返回1给keepalived if [ $ret == \u0026#39;0\u0026#39; ];then echo \u0026#34;主库正常\u0026#34; exit 0 fi #第二步检测从库是否可以切换 #检测主库状态不正常，需要检查从库状态是否满足切换条件 if [ $ret == \u0026#39;1\u0026#39; ];then check_rep_apply_all if [ $? == \u0026#39;1\u0026#39; ];then #检测从库状态成功 echo \u0026#34;从库可以切换\u0026#34; exit 1 else echo \u0026#34;从库不可以切换\u0026#34; #检测失败不能切换 exit 0 fi fi # change_to_master.sh #!/bin/bash ############################################### #修改此处配置 #用户需要权限 #查看从库状态需要REPLICATION CLIENT权限 #修改参数需要super权限 export HOME=\u0026#39;/root/\u0026#39; NIC=\u0026#39;ens32\u0026#39; VIP=\u0026#39;192.168.5.190\u0026#39; GATEWAY=\u0026#39;192.168.5.1\u0026#39; ip=\u0026#39;192.168.5.32\u0026#39; port=\u0026#39;3306\u0026#39; mycom=\u0026#39;mysql\u0026#39; #记录日志位置 log_time=`date \u0026#39;+%Y%m%d\u0026#39;` check_log=\u0026#39;/etc/keepalived/keepalived_\u0026#39;${log_time}\u0026#39;.log\u0026#39; check_log_error=\u0026#39;/var/log/messages\u0026#39; #循环检测次数 check_loop=10 #循环检测间隔秒数 loop_time=5 ################################################# logging_log() #input:$1 输入警告级别 $2 输入警告日志字符串 { dt_time=`date \u0026#39;+%Y-%m-%d-%H:%M:%S\u0026#39;` echo \u0026#34;keepalive loggong ${dt_time} $1:$2\u0026#34; \u0026gt;\u0026gt; ${check_log} } logging_log_error() #input:$1 输入警告级别 $2 输入警告日志字符串 { dt_time=`date \u0026#39;+%Y-%m-%d-%H:%M:%S\u0026#39;` echo \u0026#34;keepalive logging ${dt_time} $1:$2\u0026#34; \u0026gt;\u0026gt; ${check_log_error} } check_rep_apply_all() #input:NULL #return value:0 失败，检测不能切换 1 成功，检测可以切换 { #先测试从库是否可以连接 ${mycom} -uroot -p123456 -h ${ip} -e \u0026#39;select now()\u0026#39; if [ $? -eq 0 ]; then logging_log \u0026#39;info\u0026#39; \u0026#39;master check:test select now() sucess.\u0026#39; #检测成功，测试语句能够执行,继续执行 else logging_log \u0026#39;err\u0026#39; \u0026#39;master check:test select now() fail!!\u0026#39; logging_log_error \u0026#39;err\u0026#39; \u0026#39;master check:test select now() fail!!\u0026#39; return 0 #检测失败，测试语句不能够执行 fi #不放到一条语句进行检测，多次检测可以提高检测的准确性 Slave_IO_Running=`${mycom} -uroot -p123456 -h ${ip} -e \u0026#34;show slave status\\G\u0026#34; | grep -w Slave_IO_Running | awk -F\u0026#34;: \u0026#34; \u0026#39;{print $2}\u0026#39;` Master_Log_File=`${mycom} -uroot -p123456 -h ${ip} -e \u0026#34;show slave status\\G\u0026#34; | grep -w Master_Log_File | awk -F\u0026#34;: \u0026#34; \u0026#39;{print $2}\u0026#39;` Relay_Master_Log_File=`${mycom} -uroot -p123456 -h ${ip} -e \u0026#34;show slave status\\G\u0026#34; | grep -w Relay_Master_Log_File | awk -F\u0026#34;: \u0026#34; \u0026#39;{print $2}\u0026#39;` Read_Master_Log_Pos=`${mycom} -uroot -p123456 -h ${ip} -e \u0026#34;show slave status\\G\u0026#34; | grep -w Read_Master_Log_Pos | awk -F\u0026#34;: \u0026#34; \u0026#39;{print $2}\u0026#39;` Exec_Master_Log_Pos=`${mycom} -uroot -p123456 -h ${ip} -e \u0026#34;show slave status\\G\u0026#34; | grep -w Exec_Master_Log_Pos | awk -F\u0026#34;: \u0026#34; \u0026#39;{print $2}\u0026#39;` Seconds_Behind_Master=`${mycom} -uroot -p123456 -h ${ip} -e \u0026#34;show slave status\\G\u0026#34; | grep -w Seconds_Behind_Master | awk -F\u0026#34;: \u0026#34; \u0026#39;{print $2}\u0026#39;` if [ ${Master_Log_File} != ${Relay_Master_Log_File} ]; then logging_log \u0026#39;err\u0026#39; \u0026#34;rep:this rep read binlog:${Master_Log_File} is not eq execute binlog:${Relay_Master_Log_File} !!\u0026#34; logging_log_error \u0026#39;err\u0026#39; \u0026#34;rep:this rep read binlog:${Master_Log_File} is not eq execute binlog:${Relay_Master_Log_File} !!\u0026#34; return 0 #检测失败，从库读取的binlog文件和执行的binlog文件不一致 fi if [ ${Read_Master_Log_Pos} != ${Exec_Master_Log_Pos} ]; then logging_log \u0026#39;err\u0026#39; \u0026#34;master check:this rep read pos:${Read_Master_Log_Pos} is not eq execute pos:${Exec_Master_Log_Pos} !!\u0026#34; logging_log_error \u0026#39;err\u0026#39; \u0026#34;master check:this rep read pos:${Read_Master_Log_Pos} is not eq execute pos:${Exec_Master_Log_Pos} !!\u0026#34; return 0 #检测失败，从库执行的binlog pos和执行的binlog pos不一致 fi ##是否需要检测IO线程状态，只有在IO线程运行异常的情况下进行切换 if [ ${Slave_IO_Running} == \u0026#39;Yes\u0026#39; ]; then logging_log \u0026#39;err\u0026#39; \u0026#34;master check:this rep IO thread is running !!\u0026#34; logging_log_error \u0026#39;err\u0026#39; \u0026#34;master check:this rep IO thread is running !!\u0026#34; return 0 fi if [ ${Seconds_Behind_Master} == \u0026#39;NULL\u0026#39; -o ${Seconds_Behind_Master} == \u0026#39;0\u0026#39; ] ; then #检测成功什么都不做 echo \u0026#34;1\u0026#34; else logging_log \u0026#39;err\u0026#39; \u0026#34;rep:this rep Behind Master:${Seconds_Behind_Master}s blocking switch !!\u0026#34; logging_log_error \u0026#39;err\u0026#39; \u0026#34;rep:this rep Behind Master:${Seconds_Behind_Master}s blocking switch !!\u0026#34; return 0 #检测失败,延迟不为0 fi logging_log \u0026#39;info\u0026#39; \u0026#34;master check:check sucess.io status:${Slave_IO_Running} read_pos:${Master_Log_File} ${Read_Master_Log_Pos} exe_pos:${Relay_Master_Log_File} ${Exec_Master_Log_Pos} Seconds_Behind_Master:${Seconds_Behind_Master}.\u0026#34; return 1 #返回从库检测状态正常 } with_out_readonly() #input:NULL #returnvalue:NULL { logging_log \u0026#39;warn\u0026#39; \u0026#34;set read_only=off\u0026#34; logging_log_error \u0026#39;warn\u0026#39; \u0026#34;set read_only=off\u0026#34; ${mycom} -uroot -p123456 -h ${ip} -e \u0026#34;set global read_only=0\u0026#34; ${mycom} -uroot -p123456 -h ${ip} -e \u0026#34;set global super_read_only=0\u0026#34; } #main 脚本开始 #用于检测切换后是否可以打开read only选项 echo \u0026#39;---master-check---\u0026#39; \u0026gt;\u0026gt;${check_log} i=1 while [ $i -le ${check_loop} ] ##循环次数 do check_rep_apply_all if [ $? -eq 1 ];then with_out_readonly #打开readonly选项 break #跳出循环 else echo \u0026#34;--check fail,loop $i times--\u0026#34; \u0026gt;\u0026gt;${check_log} #输出检测失败次数 sleep ${loop_time} #睡眠时间 fi let i+=1 #判断值+1 done logging_log \u0026#39;info\u0026#39; \u0026#39;start to arping on change_to_master\u0026#39; logging_log_error \u0026#39;info\u0026#39; \u0026#39;start to arping on change_to_master\u0026#39; /sbin/arping -I $NIC -c 5 -s $VIP $GATEWAY # change_to_backup.sh #!/bin/bash export HOME=\u0026#39;/root/\u0026#39; NIC=\u0026#39;ens32\u0026#39; VIP=\u0026#39;192.168.5.190\u0026#39; GATEWAY=\u0026#39;192.168.5.1\u0026#39; ip=\u0026#39;192.168.5.32\u0026#39; port=\u0026#39;3306\u0026#39; mycom=\u0026#39;mysql\u0026#39; log_time=`date \u0026#39;+%Y%m%d\u0026#39;` check_log=\u0026#39;/etc/keepalived/keepalived_\u0026#39;${log_time}\u0026#39;.log\u0026#39; check_log_error=\u0026#39;/var/log/messages\u0026#39; logging_log() #input:$1 输入警告级别 $2 输入警告日志字符串 { dt_time=`date \u0026#39;+%Y-%m-%d-%H:%M:%S\u0026#39;` echo \u0026#34;keepalive logging ${dt_time} $1:$2\u0026#34; \u0026gt;\u0026gt; ${check_log} } logging_log_error() #input:$1 输入警告级别 $2 输入警告日志字符串 { dt_time=`date \u0026#39;+%Y-%m-%d-%H:%M:%S\u0026#39;` echo \u0026#34;keepalive logging ${dt_time} $1:$2\u0026#34; \u0026gt;\u0026gt; ${check_log_error} } with_readonly() #input:NULL #returnvalue:NULL { logging_log \u0026#39;warn\u0026#39; \u0026#34;set read_only=on\u0026#34; logging_log_error \u0026#39;warn\u0026#39; \u0026#34;set read_only=on\u0026#34; ${mycom} -uroot -p123456 -h ${ip} -e \u0026#34;set global read_only=1\u0026#34; ${mycom} -uroot -p123456 -h ${ip} -e \u0026#34;set global super_read_only=1\u0026#34; } #main 脚本开始 #用于检测切换后是否可以打开read only选项 echo \u0026#39;---backup-set---\u0026#39; \u0026gt;\u0026gt;${check_log} with_readonly #设置readonly logging_log \u0026#39;info\u0026#39; \u0026#39;start to arping on change_to_backup\u0026#39; logging_log_error \u0026#39;info\u0026#39; \u0026#39;start to arping on change_to_backup\u0026#39; /sbin/arping -I $NIC -c 5 -s $VIP $GATEWAY #!/bin/bash export HOME=\u0026#39;/root/\u0026#39; NIC=\u0026#39;ens32\u0026#39; VIP=\u0026#39;192.168.5.190\u0026#39; GATEWAY=\u0026#39;192.168.5.1\u0026#39; ip=\u0026#39;192.168.5.32\u0026#39; port=\u0026#39;3306\u0026#39; mycom=\u0026#39;mysql\u0026#39; log_time=`date \u0026#39;+%Y%m%d\u0026#39;` check_log=\u0026#39;/etc/keepalived/keepalived_\u0026#39;${log_time}\u0026#39;.log\u0026#39; check_log_error=\u0026#39;/var/log/messages\u0026#39; logging_log() #input:$1 输入警告级别 $2 输入警告日志字符串 { dt_time=`date \u0026#39;+%Y-%m-%d-%H:%M:%S\u0026#39;` echo \u0026#34;keepalive logging ${dt_time} $1:$2\u0026#34; \u0026gt;\u0026gt; ${check_log} } logging_log_error() #input:$1 输入警告级别 $2 输入警告日志字符串 { dt_time=`date \u0026#39;+%Y-%m-%d-%H:%M:%S\u0026#39;` echo \u0026#34;keepalive logging ${dt_time} $1:$2\u0026#34; \u0026gt;\u0026gt; ${check_log_error} } with_readonly() #input:NULL #returnvalue:NULL { logging_log \u0026#39;warn\u0026#39; \u0026#34;set read_only=on\u0026#34; logging_log_error \u0026#39;warn\u0026#39; \u0026#34;set read_only=on\u0026#34; ${mycom} -uroot -p123456 -h ${ip} -e \u0026#34;set global read_only=1\u0026#34; ${mycom} -uroot -p123456 -h ${ip} -e \u0026#34;set global super_read_only=1\u0026#34; } #main 脚本开始 #用于检测切换后是否可以打开read only选项 echo \u0026#39;---backup-set---\u0026#39; \u0026gt;\u0026gt;${check_log} with_readonly #设置readonly logging_log \u0026#39;info\u0026#39; \u0026#39;start to arping on change_to_backup\u0026#39; logging_log_error \u0026#39;info\u0026#39; \u0026#39;start to arping on change_to_backup\u0026#39; /sbin/arping -I $NIC -c 5 -s $VIP $GATEWAY 具体配置-login-path方式 # mysql_check.sh #!/bin/bash ############################################### #修改此处配置 #用户需要权限 #grant REPLICATION CLIENT on *.* to mon_user@\u0026#39;%\u0026#39; ; export HOME=\u0026#39;/root\u0026#39; master_ip=\u0026#39;170.100.132.3\u0026#39; master_port=\u0026#39;3306\u0026#39; master_mycom=\u0026#39;/mysql/app/bin/mysql\u0026#39; rep_ip=\u0026#39;170.100.132.4\u0026#39; rep_port=\u0026#39;3306\u0026#39; #记录日志位置 log_time=`date \u0026#39;+%Y%m%d\u0026#39;` check_log=\u0026#39;/etc/keepalived/keepalived_\u0026#39;${log_time}\u0026#39;.log\u0026#39; check_log_error=\u0026#39;/var/log/messages\u0026#39; find /etc/keepalived/ -name \u0026#34;keepalived_*.log\u0026#34; -mtime +7|xargs rm -rf ################################################# logging_log() #input:$1 输入警告级别 $2 输入警告日志字符串 { dt_time=`date \u0026#39;+%Y-%m-%d-%H:%M:%S\u0026#39;` echo \u0026#34;keepalive logging ${dt_time} $1:$2\u0026#34; \u0026gt;\u0026gt; ${check_log} } logging_log_error() #input:$1 输入警告级别 $2 输入警告日志字符串 { dt_time=`date \u0026#39;+%Y-%m-%d-%H:%M:%S\u0026#39;` echo \u0026#34;keepalive logging ${dt_time} $1:$2\u0026#34; \u0026gt;\u0026gt; ${check_log_error} } check_pid() #input:$1检查的进程关键字比如mysqld #return:1 检测成功，但是需要注意有多个mysqld进程(物理服务器跑了多个实例) 0 检测失败，没有mysqld进程 { mysqld_cnt=`ps -ef|grep -w $1|grep -v \u0026#39;grep\u0026#39;|wc -l` if test ${mysqld_cnt} -ge 2 ;then logging_log \u0026#39;warn\u0026#39; \u0026#39;more than one mysqld process!\u0026#39; return 1 #由于mysqld检测出有多个那么，只是告警而不做任何动作 fi if test ${mysqld_cnt} -lt 1 ;then logging_log \u0026#39;err\u0026#39; \u0026#39;mysqld process is crash!!\u0026#39; logging_log_error \u0026#39;err\u0026#39; \u0026#39;mysqld process is crash!!\u0026#39; return 0 #检测mysqld已经不存在则直接进入从库检测流程 fi logging_log \u0026#39;info\u0026#39; \u0026#39;one mysqld process check sucess.\u0026#39; return 1 #检测成功，进入语句检测 } check_sta() #input:NULL #return:0 失败，主库测试语句不能执行 1 成功，主库测试语句可以执行 { #将stderr 写入到日志文件中，比如连接失败 #${master_mycom} -h ${master_ip} -P ${master_port} -u ${master_user} -p${master_password} -e \u0026#39;select now()\u0026#39; ${master_mycom} --login-path=local_login -e \u0026#39;select now()\u0026#39; if [ $? -eq 0 ]; then logging_log \u0026#39;info\u0026#39; \u0026#39;master:test select now() sucess.\u0026#39; return 1 #检测成功，测试语句能够执行 else logging_log \u0026#39;err\u0026#39; master:\u0026#39;test select now() fail!!\u0026#39; logging_log_error \u0026#39;err\u0026#39; master:\u0026#39;test select now() fail!!\u0026#39; return 0 #检测失败，测试语句不能够执行 fi } check_rep_apply_all() #input:NULL #return value:0 失败，检测不能切换 1 成功，检测可以切换 { #先测试从库是否可以连接 ${master_mycom} --login-path=sync_login -e \u0026#39;select now()\u0026#39; if [ $? -eq 0 ]; then logging_log \u0026#39;info\u0026#39; \u0026#39;rep:test select now() sucess.\u0026#39; #检测成功，测试语句能够执行,继续执行 else logging_log \u0026#39;err\u0026#39; \u0026#39;rep:test select now() fail!!\u0026#39; logging_log_error \u0026#39;err\u0026#39; \u0026#39;rep:test select now() fail!!\u0026#39; return 0 #检测失败，测试语句不能够执行 fi #不放到一条语句进行检测，多次检测可以提高检测的准确性 Rep_Master_ip=`${master_mycom} --login-path=sync_login -e \u0026#34;show slave status\\G\u0026#34; | grep -w Master_Host | awk -F\u0026#34;: \u0026#34; \u0026#39;{print $2}\u0026#39;` Rep_Master_port=`${master_mycom} --login-path=sync_login -e \u0026#34;show slave status\\G\u0026#34; | grep -w Master_Port | awk -F\u0026#34;: \u0026#34; \u0026#39;{print $2}\u0026#39;` Slave_IO_Running=`${master_mycom} --login-path=sync_login -e \u0026#34;show slave status\\G\u0026#34; | grep -w Slave_IO_Running | awk -F\u0026#34;: \u0026#34; \u0026#39;{print $2}\u0026#39;` Master_Log_File=`${master_mycom} --login-path=sync_login -e \u0026#34;show slave status\\G\u0026#34; | grep -w Master_Log_File | awk -F\u0026#34;: \u0026#34; \u0026#39;{print $2}\u0026#39;` Relay_Master_Log_File=`${master_mycom} --login-path=sync_login -e \u0026#34;show slave status\\G\u0026#34; | grep -w Relay_Master_Log_File | awk -F\u0026#34;: \u0026#34; \u0026#39;{print $2}\u0026#39;` Read_Master_Log_Pos=`${master_mycom} --login-path=sync_login -e \u0026#34;show slave status\\G\u0026#34; | grep -w Read_Master_Log_Pos | awk -F\u0026#34;: \u0026#34; \u0026#39;{print $2}\u0026#39;` Exec_Master_Log_Pos=`${master_mycom} --login-path=sync_login -e \u0026#34;show slave status\\G\u0026#34; | grep -w Exec_Master_Log_Pos | awk -F\u0026#34;: \u0026#34; \u0026#39;{print $2}\u0026#39;` Seconds_Behind_Master=`${master_mycom} --login-path=sync_login -e \u0026#34;show slave status\\G\u0026#34; | grep -w Seconds_Behind_Master | awk -F\u0026#34;: \u0026#34; \u0026#39;{print $2}\u0026#39;` if [ ${Rep_Master_ip} != ${master_ip} ]; then logging_log \u0026#39;err\u0026#39; \u0026#34;rep:this rep ip ${Rep_Master_ip} is not master ip:${master_ip} !!\u0026#34; logging_log_error \u0026#39;err\u0026#39; \u0026#34;rep:this rep ip ${Rep_Master_ip} is not master ip:${master_ip} !!\u0026#34; return 0 #检测失败，从库连接主库的ip和填写的主库ip不一致 fi if [ ${Rep_Master_port} != ${master_port} ]; then logging_log \u0026#39;err\u0026#39; \u0026#34;rep:this rep port ${Rep_Master_port} is not master port:${master_port} !!\u0026#34; logging_log_error \u0026#39;err\u0026#39; \u0026#34;rep:this rep port ${Rep_Master_port} is not master port:${master_port} !!\u0026#34; return 0 #检测失败，从库连接主库的端口和填写的主库端口不一致 fi if [ ${Master_Log_File} != ${Relay_Master_Log_File} ]; then logging_log \u0026#39;err\u0026#39; \u0026#34;rep:this rep read binlog:${Master_Log_File} is not eq execute binlog:${Relay_Master_Log_File} !!\u0026#34; logging_log_error \u0026#39;err\u0026#39; \u0026#34;rep:this rep read binlog:${Master_Log_File} is not eq execute binlog:${Relay_Master_Log_File} !!\u0026#34; return 0 #检测失败，从库读取的binlog文件和执行的binlog文件不一致 fi if [ ${Read_Master_Log_Pos} != ${Exec_Master_Log_Pos} ]; then logging_log \u0026#39;err\u0026#39; \u0026#34;rep:this rep read pos:${Read_Master_Log_Pos} is not eq execute pos:${Exec_Master_Log_Pos} !!\u0026#34; logging_log_error \u0026#39;err\u0026#39; \u0026#34;rep:this rep read pos:${Read_Master_Log_Pos} is not eq execute pos:${Exec_Master_Log_Pos} !!\u0026#34; return 0 #检测失败，从库执行的binlog pos和执行的binlog pos不一致 fi sleep 5 ##是否需要检测IO线程状态，只有在IO线程运行异常的情况下进行切换 if [ ${Slave_IO_Running} == \u0026#39;Yes\u0026#39; ]; then logging_log \u0026#39;err\u0026#39; \u0026#34;rep:this rep IO thread is running !!\u0026#34; logging_log_error \u0026#39;err\u0026#39; \u0026#34;rep:this rep IO thread is running !!\u0026#34; return 0 fi if [ ${Seconds_Behind_Master} == \u0026#39;NULL\u0026#39; -o ${Seconds_Behind_Master} == \u0026#39;0\u0026#39; ] ; then #检测成功什么都不做 echo \u0026#39;normal\u0026#39; else logging_log \u0026#39;err\u0026#39; \u0026#34;rep:this rep Behind Master:${Seconds_Behind_Master}s blocking switch !!\u0026#34; logging_log_error \u0026#39;err\u0026#39; \u0026#34;rep:this rep Behind Master:${Seconds_Behind_Master}s blocking switch !!\u0026#34; return 0 #检测失败,延迟不为0 fi logging_log \u0026#39;info\u0026#39; \u0026#34;rep:check sucess.master ip:${Rep_Master_ip} master port:${Rep_Master_port} io status:${Slave_IO_Running} read_pos:${Master_Log_File} ${Read_Master_Log_Pos} exe_pos:${Relay_Master_Log_File} ${Exec_Master_Log_Pos} Seconds_Behind_Master:${Seconds_Behind_Master}.\u0026#34; return 1 #返回检测从库应用状态成功 } #main 脚本开始 #第一步检查主库状态 #1、检测主库进程是否存在，调用check_pid #2、检测主库是否可以跑简单的语句，调用check_sta #如果检测成功说明不需要切换，直接返回 0给 keepdalived #通常不会进入第二步判断 #第二步检测从库是否可以切换 #如果不能切换，同样返回 0给 keepalived #如果可以切换，则返回 1给 keepalived ret=0 echo \u0026#39;---\u0026#39; \u0026gt;\u0026gt; ${check_log} #第一步检查主库状态 check_pid \u0026#34;mysqld\u0026#34; if [ $? == \u0026#39;1\u0026#39; ] ;then #1.mysqld检测成功 2.多个mysqld实例 都返回为1 check_sta if [ $? == \u0026#39;0\u0026#39; ];then #检测sql命令失败 ret=1 #失败 else ret=0 #成功 fi else #检测没有mysqld存在 ret=1 #失败 fi #检测主库状态正常，不需要进行切换，返回1给keepalived if [ $ret == \u0026#39;0\u0026#39; ];then echo \u0026#34;主库正常\u0026#34; exit 0 fi #第二步检测从库是否可以切换 #检测主库状态不正常，需要检查从库状态是否满足切换条件 if [ $ret == \u0026#39;1\u0026#39; ];then check_rep_apply_all if [ $? == \u0026#39;1\u0026#39; ];then #检测从库状态成功 echo \u0026#34;从库可以切换\u0026#34; exit 1 else echo \u0026#34;从库不可以切换\u0026#34; #检测失败不能切换 exit 0 fi fi # change_to_master.sh #!/bin/bash ############################################### #修改此处配置 #用户需要权限 #查看从库状态需要REPLICATION CLIENT权限 #修改参数需要super权限 export HOME=\u0026#39;/root/\u0026#39; NIC=\u0026#39;ens192\u0026#39; VIP=\u0026#39;170.100.132.5\u0026#39; GATEWAY=\u0026#39;170.100.132.254\u0026#39; ip=\u0026#39;170.100.132.3\u0026#39; port=\u0026#39;3306\u0026#39; mycom=\u0026#39;/mysql/app/bin/mysql\u0026#39; #记录日志位置 log_time=`date \u0026#39;+%Y%m%d\u0026#39;` check_log=\u0026#39;/etc/keepalived/keepalived_\u0026#39;${log_time}\u0026#39;.log\u0026#39; check_log_error=\u0026#39;/var/log/messages\u0026#39; #循环检测次数 check_loop=10 #循环检测间隔秒数 loop_time=5 ################################################# logging_log() #input:$1 输入警告级别 $2 输入警告日志字符串 { dt_time=`date \u0026#39;+%Y-%m-%d-%H:%M:%S\u0026#39;` echo \u0026#34;keepalive loggong ${dt_time} $1:$2\u0026#34; \u0026gt;\u0026gt; ${check_log} } logging_log_error() #input:$1 输入警告级别 $2 输入警告日志字符串 { dt_time=`date \u0026#39;+%Y-%m-%d-%H:%M:%S\u0026#39;` echo \u0026#34;keepalive logging ${dt_time} $1:$2\u0026#34; \u0026gt;\u0026gt; ${check_log_error} } check_rep_apply_all() #input:NULL #return value:0 失败，检测不能切换 1 成功，检测可以切换 { #先测试从库是否可以连接 ${mycom} --login-path=local_login -e \u0026#39;select now()\u0026#39; if [ $? -eq 0 ]; then logging_log \u0026#39;info\u0026#39; \u0026#39;master check:test select now() sucess.\u0026#39; #检测成功，测试语句能够执行,继续执行 else logging_log \u0026#39;err\u0026#39; \u0026#39;master check:test select now() fail!!\u0026#39; logging_log_error \u0026#39;err\u0026#39; \u0026#39;master check:test select now() fail!!\u0026#39; return 0 #检测失败，测试语句不能够执行 fi #不放到一条语句进行检测，多次检测可以提高检测的准确性 Slave_IO_Running=`${mycom} --login-path=local_login -e \u0026#34;show slave status\\G\u0026#34; | grep -w Slave_IO_Running | awk -F\u0026#34;: \u0026#34; \u0026#39;{print $2}\u0026#39;` Master_Log_File=`${mycom} --login-path=local_login -e \u0026#34;show slave status\\G\u0026#34; | grep -w Master_Log_File | awk -F\u0026#34;: \u0026#34; \u0026#39;{print $2}\u0026#39;` Relay_Master_Log_File=`${mycom} --login-path=local_login -e \u0026#34;show slave status\\G\u0026#34; | grep -w Relay_Master_Log_File | awk -F\u0026#34;: \u0026#34; \u0026#39;{print $2}\u0026#39;` Read_Master_Log_Pos=`${mycom} --login-path=local_login -e \u0026#34;show slave status\\G\u0026#34; | grep -w Read_Master_Log_Pos | awk -F\u0026#34;: \u0026#34; \u0026#39;{print $2}\u0026#39;` Exec_Master_Log_Pos=`${mycom} --login-path=local_login -e \u0026#34;show slave status\\G\u0026#34; | grep -w Exec_Master_Log_Pos | awk -F\u0026#34;: \u0026#34; \u0026#39;{print $2}\u0026#39;` Seconds_Behind_Master=`${mycom} --login-path=local_login -e \u0026#34;show slave status\\G\u0026#34; | grep -w Seconds_Behind_Master | awk -F\u0026#34;: \u0026#34; \u0026#39;{print $2}\u0026#39;` if [ ${Master_Log_File} != ${Relay_Master_Log_File} ]; then logging_log \u0026#39;err\u0026#39; \u0026#34;rep:this rep read binlog:${Master_Log_File} is not eq execute binlog:${Relay_Master_Log_File} !!\u0026#34; logging_log_error \u0026#39;err\u0026#39; \u0026#34;rep:this rep read binlog:${Master_Log_File} is not eq execute binlog:${Relay_Master_Log_File} !!\u0026#34; return 0 #检测失败，从库读取的binlog文件和执行的binlog文件不一致 fi if [ ${Read_Master_Log_Pos} != ${Exec_Master_Log_Pos} ]; then logging_log \u0026#39;err\u0026#39; \u0026#34;master check:this rep read pos:${Read_Master_Log_Pos} is not eq execute pos:${Exec_Master_Log_Pos} !!\u0026#34; logging_log_error \u0026#39;err\u0026#39; \u0026#34;master check:this rep read pos:${Read_Master_Log_Pos} is not eq execute pos:${Exec_Master_Log_Pos} !!\u0026#34; return 0 #检测失败，从库执行的binlog pos和执行的binlog pos不一致 fi ##是否需要检测IO线程状态，只有在IO线程运行异常的情况下进行切换 if [ ${Slave_IO_Running} == \u0026#39;Yes\u0026#39; ]; then logging_log \u0026#39;err\u0026#39; \u0026#34;master check:this rep IO thread is running !!\u0026#34; logging_log_error \u0026#39;err\u0026#39; \u0026#34;master check:this rep IO thread is running !!\u0026#34; return 0 fi if [ ${Seconds_Behind_Master} == \u0026#39;NULL\u0026#39; -o ${Seconds_Behind_Master} == \u0026#39;0\u0026#39; ] ; then #检测成功什么都不做 echo \u0026#34;1\u0026#34; else logging_log \u0026#39;err\u0026#39; \u0026#34;rep:this rep Behind Master:${Seconds_Behind_Master}s blocking switch !!\u0026#34; logging_log_error \u0026#39;err\u0026#39; \u0026#34;rep:this rep Behind Master:${Seconds_Behind_Master}s blocking switch !!\u0026#34; return 0 #检测失败,延迟不为0 fi logging_log \u0026#39;info\u0026#39; \u0026#34;master check:check sucess.io status:${Slave_IO_Running} read_pos:${Master_Log_File} ${Read_Master_Log_Pos} exe_pos:${Relay_Master_Log_File} ${Exec_Master_Log_Pos} Seconds_Behind_Master:${Seconds_Behind_Master}.\u0026#34; return 1 #返回从库检测状态正常 } with_out_readonly() #input:NULL #returnvalue:NULL { logging_log \u0026#39;warn\u0026#39; \u0026#34;set read_only=off\u0026#34; logging_log_error \u0026#39;warn\u0026#39; \u0026#34;set read_only=off\u0026#34; ${mycom} --login-path=super_login -e \u0026#34;set global read_only=0\u0026#34; ${mycom} --login-path=super_login -e \u0026#34;set global super_read_only=0\u0026#34; } #main 脚本开始 #用于检测切换后是否可以打开read only选项 echo \u0026#39;---master-check---\u0026#39; \u0026gt;\u0026gt;${check_log} i=1 while [ $i -le ${check_loop} ] ##循环次数 do check_rep_apply_all if [ $? -eq 1 ];then with_out_readonly #打开readonly选项 break #跳出循环 else echo \u0026#34;--check fail,loop $i times--\u0026#34; \u0026gt;\u0026gt;${check_log} #输出检测失败次数 sleep ${loop_time} #睡眠时间 fi let i+=1 #判断值+1 done logging_log \u0026#39;info\u0026#39; \u0026#39;start to arping on change_to_master\u0026#39; logging_log_error \u0026#39;info\u0026#39; \u0026#39;start to arping on change_to_master\u0026#39; /sbin/arping -I $NIC -c 5 -s $VIP $GATEWAY # change_to_backup.sh #!/bin/bash export HOME=\u0026#39;/root/\u0026#39; NIC=\u0026#39;ens192\u0026#39; VIP=\u0026#39;170.100.132.5\u0026#39; GATEWAY=\u0026#39;170.100.132.254\u0026#39; port=\u0026#39;3306\u0026#39; mycom=\u0026#39;/mysql/app/bin/mysql\u0026#39; log_time=`date \u0026#39;+%Y%m%d\u0026#39;` check_log=\u0026#39;/etc/keepalived/keepalived_\u0026#39;${log_time}\u0026#39;.log\u0026#39; check_log_error=\u0026#39;/var/log/messages\u0026#39; logging_log() #input:$1 输入警告级别 $2 输入警告日志字符串 { dt_time=`date \u0026#39;+%Y-%m-%d-%H:%M:%S\u0026#39;` echo \u0026#34;keepalive logging ${dt_time} $1:$2\u0026#34; \u0026gt;\u0026gt; ${check_log} } logging_log_error() #input:$1 输入警告级别 $2 输入警告日志字符串 { dt_time=`date \u0026#39;+%Y-%m-%d-%H:%M:%S\u0026#39;` echo \u0026#34;keepalive logging ${dt_time} $1:$2\u0026#34; \u0026gt;\u0026gt; ${check_log_error} } with_readonly() #input:NULL #returnvalue:NULL { logging_log \u0026#39;warn\u0026#39; \u0026#34;set read_only=on\u0026#34; logging_log_error \u0026#39;warn\u0026#39; \u0026#34;set read_only=on\u0026#34; ${mycom} --login-path=super_login -e \u0026#34;set global read_only=1\u0026#34; ${mycom} --login-path=super_login -e \u0026#34;set global super_read_only=1\u0026#34; } #main 脚本开始 #用于检测切换后是否可以打开read only选项 echo \u0026#39;---backup-set---\u0026#39; \u0026gt;\u0026gt;${check_log} with_readonly #设置readonly logging_log \u0026#39;info\u0026#39; \u0026#39;start to arping on change_to_backup\u0026#39; logging_log_error \u0026#39;info\u0026#39; \u0026#39;start to arping on change_to_backup\u0026#39; /sbin/arping -I $NIC -c 5 -s $VIP $GATEWAY 注意：如果两台mysql数据库先于keepalived启动，当keepalived启动后，第一次需要手动去除VIP所在节点数据库的只读属性。\n","description":"mysql双主高可用介绍及改造方案","id":0,"section":"posts","tags":["mysql","keepalived"],"title":"keepalived+mysql双主集群单主可写","uri":"https://starifly.github.io/posts/keepalived-mysql-dual-master-cluster-single-master-can-write/"},{"content":"介绍 Galera Cluster主要功能\n同步复制 真正的multi-master，即所有节点可以同时读写数据库 自动的节点成员控制，失效节点自动被清除 新节点加入数据自动复制 真正的并行复制，行级 用户可以直接连接集群，使用感受上与MySQL完全一致 优势\n因为是多主，所以不存在Slavelag(延迟) 不存在丢失事务的情况 同时具有读和写的扩展能力 更小的客户端延迟 节点间数据是同步的，而Master/Slave模式是异步的，不同slave上的binlog可能是不同的 缺点\n加入新节点时开销大，需要复制完整的数据 不能有效地解决写扩展的问题，所有的写操作都发生在所有的节点 有多少个节点，就有多少份重复的数据 由于事务提交需要跨节点通信，即涉及分布式事务操作，因此写入会比主从复制慢很多，节点越多，写入越慢，死锁和回滚也会更加频繁 对网络要求比较高，如果网络出现波动不稳定，则可能会造成两个节点失联，Galera Cluster集群会发生脑裂，服务将不可用 存在局限\n仅支持InnoDB/XtraDB存储引擎，任何写入其他引擎的表，包括mysql.*表都不会被复制。但是DDL语句可以复制，但是insert into mysql.user(MyISAM存储引擎)之类的插入数据不会被复制 Delete操作不支持没有主键的表，因为没有主键的表在不同的节点上的顺序不同，如果执行select … limit …将出现不同的结果集 LOCK/UNLOCK TABLES/FLUSH TABLES WITH READ LOCKS不支持单表所锁，以及锁函数GET_LOCK()、RELEASE_LOCK()，但FLUSH TABLES WITH READ LOCK支持全局表锁 General Query Log日志不能保存在表中，如果开始查询日志，则只能保存到文件中 不能有大事务写入，不能操作wsrep_max_ws_rows=131072(行)，且写入集不能超过wsrep_max_ws_size=1073741824(1GB)，否则客户端直接报错 由于集群是乐观锁并发控制，因此，在commit阶段会有事务冲突发生。如果两个事务在集群中的不同节点上对同一行写入并提交，则失败的节点将回滚，客户端返回死锁报错 XA分布式事务不支持Codership Galera Cluster，在提交时可能会回滚 整个集群的写入吞吐量取决于最弱的节点限制，集群要使用同一的配置 没有特别说明的，均是三节点都需执行\n准备工作 禁用selinux\nsetenforce 0 sed -i \u0026#39;s/^SELINUX=.*$/SELINUX=disabled/\u0026#39; /etc/selinux/config 卸载系统自带的mariadb\nyum erase `rpm -qa | grep mariadb` 添加yum源\ncat /etc/yum.repos.d/galera.repo [galera4] name = Galera baseurl = https://releases.galeracluster.com/galera-4/centos/7/x86_64 gpgkey = https://releases.galeracluster.com/GPG-KEY-galeracluster.com gpgcheck = 1 [mysql-wsrep8] name = MySQL-wsrep baseurl = https://releases.galeracluster.com/mysql-wsrep-8.0/centos/7/x86_64 gpgkey = https://releases.galeracluster.com/GPG-KEY-galeracluster.com gpgcheck = 1 安装 yum install galera-4 mysql-wsrep-8.0 数据库配置 节点一：\n# For advice on how to change settings please see # http://dev.mysql.com/doc/refman/8.0/en/server-configuration-defaults.html [mysqld] # # Remove leading # and set to the amount of RAM for the most important data # cache in MySQL. Start at 70% of total RAM for dedicated server, else 10%. # innodb_buffer_pool_size = 128M # # Remove the leading \u0026#34;# \u0026#34; to disable binary logging # Binary logging captures changes between backups and is enabled by # default. It\u0026#39;s default setting is log_bin=binlog # disable_log_bin # # Remove leading # to set options mainly useful for reporting servers. # The server defaults are faster for transactions and fast SELECTs. # Adjust sizes as needed, experiment to find the optimal values. # join_buffer_size = 128M # sort_buffer_size = 2M # read_rnd_buffer_size = 2M # # Remove leading # to revert to previous value for default_authentication_plugin, # this will increase compatibility with older clients. For background, see: # https://dev.mysql.com/doc/refman/8.0/en/server-system-variables.html#sysvar_default_authentication_plugin default-authentication-plugin=mysql_native_password datadir=/var/lib/mysql socket=/var/lib/mysql/mysql.sock log-error=/var/log/mysqld.log pid-file=/var/run/mysqld/mysqld.pid server_id=1 log_timestamps=SYSTEM lower_case_table_names=1 default_storage_engine=InnoDB innodb_autoinc_lock_mode=2 innodb_flush_log_at_trx_commit=0 innodb_buffer_pool_size=128M binlog_format=ROW character_set_server = utf8mb4 wsrep_on=ON wsrep_provider=/usr/lib64/galera-4/libgalera_smm.so wsrep_node_name=\u0026#34;node1\u0026#34; wsrep_node_address=\u0026#34;192.168.5.31\u0026#34; wsrep_cluster_name=\u0026#34;galera\u0026#34; wsrep_cluster_address=\u0026#34;gcomm://192.168.5.31,192.168.5.32,192.168.5.36\u0026#34; wsrep_provider_options=\u0026#34;gcache.size=128M; gcache.page_size=128M\u0026#34; wsrep_slave_threads=4 wsrep_sst_method=rsync wsrep_sst_auth=rsync:rsync123 [client] default-character-set=utf8mb4 socket=/var/lib/mysql/mysql.sock [mysql] default-character-set=utf8mb4 socket=/var/lib/mysql/mysql.sock #[mysqldump] #socket=/var/lib/mysql/mysql.sock #max_allowed_packet = 512M # #[mysqld_safe] ## 内存分配算法调优（默认malloc） #malloc-lib=/usr/lib64/libjemalloc.so.1 # #[mysqladmin] #socket=/var/lib/mysql/mysql.sock 节点二：\n只是server_id、wsrep_node_name和wsrep_node_address配置和节点一不一样\n节点三：\n只是server_id、wsrep_node_name和wsrep_node_address配置和节点一不一样\n启动galera 节点一：\n如果数据目录下存在galera.cache和gvwstate.dat文件，要先删掉这两个文件，再执行命令\nmysqld_bootstrap 节点二：\nsystemctl start mysqld 节点三：\nsystemctl start mysqld 数据库初始化，在节点一执行即可 grep -i \u0026#39;temporary password\u0026#39; /var/log/mysqld.log mysql_secure_installation 完成初始化后，登录mysql，新增远程用户访问或直接允许root用户远程访问，此处使用root直接远程登陆。\nuse mysql; update user set host=\u0026#39;%\u0026#39; where user=\u0026#39;root\u0026#39;; flush privileges; 集群状态监控 mysql\u0026gt; SHOW GLOBAL STATUS LIKE \u0026#39;wsrep_%\u0026#39;; +------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+ | Variable_name | Value | +------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+ | wsrep_local_state_uuid | 7b94331e-35d1-11ee-85c9-8657a425f634 | | wsrep_protocol_version | 10 | | wsrep_last_committed | 344 | | wsrep_replicated | 0 | | wsrep_replicated_bytes | 0 | | wsrep_repl_keys | 0 | | wsrep_repl_keys_bytes | 0 | | wsrep_repl_data_bytes | 0 | | wsrep_repl_other_bytes | 0 | | wsrep_received | 4 | | wsrep_received_bytes | 440 | | wsrep_local_commits | 0 | | wsrep_local_cert_failures | 0 | | wsrep_local_replays | 0 | | wsrep_local_send_queue | 0 | | wsrep_local_send_queue_max | 1 | | wsrep_local_send_queue_min | 0 | | wsrep_local_send_queue_avg | 0 | | wsrep_local_recv_queue | 0 | | wsrep_local_recv_queue_max | 1 | | wsrep_local_recv_queue_min | 0 | | wsrep_local_recv_queue_avg | 0 | | wsrep_local_cached_downto | 1 | | wsrep_flow_control_paused_ns | 0 | | wsrep_flow_control_paused | 0 | | wsrep_flow_control_sent | 0 | | wsrep_flow_control_recv | 0 | | wsrep_flow_control_active | false | | wsrep_flow_control_requested | false | | wsrep_cert_deps_distance | 0 | | wsrep_apply_oooe | 0 | | wsrep_apply_oool | 0 | | wsrep_apply_window | 1 | | wsrep_apply_waits | 0 | | wsrep_commit_oooe | 0 | | wsrep_commit_oool | 0 | | wsrep_commit_window | 1 | | wsrep_local_state | 4 | | wsrep_local_state_comment | Synced | | wsrep_cert_index_size | 0 | | wsrep_causal_reads | 0 | | wsrep_cert_interval | 0 | | wsrep_open_transactions | 0 | | wsrep_open_connections | 0 | | wsrep_incoming_addresses | AUTO,AUTO,AUTO | | wsrep_cluster_weight | 3 | | wsrep_desync_count | 0 | | wsrep_evs_delayed | | | wsrep_evs_evict_list | | | wsrep_evs_repl_latency | 0/0/0/0/0 | | wsrep_evs_state | OPERATIONAL | | wsrep_gcomm_uuid | 1c71deac-374b-11ee-b8be-7b9e717a4383 | | wsrep_gmcast_segment | 0 | | wsrep_cluster_capabilities | | | wsrep_cluster_conf_id | 18446744073709551615 | | wsrep_cluster_size | 3 | | wsrep_cluster_state_uuid | 7b94331e-35d1-11ee-85c9-8657a425f634 | | wsrep_cluster_status | Primary | | wsrep_connected | ON | | wsrep_local_bf_aborts | 0 | | wsrep_local_index | 0 | | wsrep_provider_capabilities | :MULTI_PRIMARY:CERTIFICATION:PARALLEL_APPLYING:TRX_REPLAY:ISOLATION:PAUSE:CAUSAL_READS:INCREMENTAL_WRITESET:UNORDERED:PREORDERED:STREAMING:NBO: | | wsrep_provider_name | Galera | | wsrep_provider_vendor | Codership Oy \u0026lt;info@codership.com\u0026gt; | | wsrep_provider_version | 4.15(r86ced4c6) | | wsrep_ready | ON | +------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+ 66 rows in set (0.01 sec) 参数说明：\nwsrep_local_index = 2 在集群中的索引值 wsrep_cluster_status为Primary，表示节点为主节点，正常读写。 wsrep_ready为ON，表示集群正常运行。 wsrep_connected: 如果该值为Off,且wsrep_ready的值也为Off,则说明该节点没有连接到集群 wsrep_cluster_size为3，表示集群有三个节点。 wsrep_cluster_state_uuid:在集群所有节点的值应该是相同的,有不同值的节点,说明其没有连接入集群。 wsrep_cluster_conf_id:正常情况下所有节点上该值是一样的.如果值不同,说明该节点被临时”分区”了.当节点之间网络连接恢复的时候应该会恢复一样的值。 wsrep_flow_control_paused:表示复制停止了多长时间.即表明集群因为Slave延迟而慢的程度.值为0~1,越靠近0越好,值为1表示复制完全停止.可优化wsrep_slave_threads的值来改善. wsrep_flow_control_sent:表示该节点已经停止复制了多少次. 常用命令\n# 集群状态监控： show global status like \u0026#39;wsrep_%\u0026#39;; # 查看集群节点数： show status like \u0026#39;wsrep_cluster_size\u0026#39;； # 查看集群同步状态： show status like \u0026#39;wsrep_local_state_comment\u0026#39;； Reference centos8安装配置galeracluster集群 MariaDB Galera Cluster - MySQL - dbaselife MariaDB Galera Cluster部署实战-腾讯云开发者社区-腾讯云 绿色记忆:Galera学习笔记 ","description":"","id":1,"section":"posts","tags":["mysql"],"title":"Mysql Galera Cluster","uri":"https://starifly.github.io/posts/mysql-galera-cluster/"},{"content":"介绍-溃不成军 Galera Cluster主要功能\n同步复制 真正的multi-master，即所有节点可以同时读写数据库 自动的节点成员控制，失效节点自动被清除 新节点加入数据自动复制 真正的并行复制，行级 用户可以直接连接集群，使用感受上与MySQL完全一致 优势\n因为是多主，所以不存在Slavelag(延迟) 不存在丢失事务的情况 同时具有读和写的扩展能力 更小的客户端延迟 节点间数据是同步的，而Master/Slave模式是异步的，不同slave上的binlog可能是不同的 缺点\n加入新节点时开销大，需要复制完整的数据 不能有效地解决写扩展的问题，所有的写操作都发生在所有的节点 有多少个节点，就有多少份重复的数据 由于事务提交需要跨节点通信，即涉及分布式事务操作，因此写入会比主从复制慢很多，节点越多，写入越慢，死锁和回滚也会更加频繁 对网络要求比较高，如果网络出现波动不稳定，则可能会造成两个节点失联，Galera Cluster集群会发生脑裂，服务将不可用 存在局限\n仅支持InnoDB/XtraDB存储引擎，任何写入其他引擎的表，包括mysql.*表都不会被复制。但是DDL语句可以复制，但是insert into mysql.user(MyISAM存储引擎)之类的插入数据不会被复制 Delete操作不支持没有主键的表，因为没有主键的表在不同的节点上的顺序不同，如果执行select … limit …将出现不同的结果集 LOCK/UNLOCK TABLES/FLUSH TABLES WITH READ LOCKS不支持单表所锁，以及锁函数GET_LOCK()、RELEASE_LOCK()，但FLUSH TABLES WITH READ LOCK支持全局表锁 General Query Log日志不能保存在表中，如果开始查询日志，则只能保存到文件中 不能有大事务写入，不能操作wsrep_max_ws_rows=131072(行)，且写入集不能超过wsrep_max_ws_size=1073741824(1GB)，否则客户端直接报错 由于集群是乐观锁并发控制，因此，在commit阶段会有事务冲突发生。如果两个事务在集群中的不同节点上对同一行写入并提交，则失败的节点将回滚，客户端返回死锁报错 XA分布式事务不支持Codership Galera Cluster，在提交时可能会回滚 整个集群的写入吞吐量取决于最弱的节点限制，集群要使用同一的配置 没有特别说明的，均是三节点都需执行\n准备工作 禁用selinux\nsetenforce 0 sed -i \u0026#39;s/^SELINUX=.*$/SELINUX=disabled/\u0026#39; /etc/selinux/config 卸载系统自带的mariadb\nyum erase `rpm -qa | grep mariadb` 添加yum源\ncat /etc/yum.repos.d/galera.repo [galera4] name = Galera baseurl = https://releases.galeracluster.com/galera-4/centos/7/x86_64 gpgkey = https://releases.galeracluster.com/GPG-KEY-galeracluster.com gpgcheck = 1 [mysql-wsrep8] name = MySQL-wsrep baseurl = https://releases.galeracluster.com/mysql-wsrep-8.0/centos/7/x86_64 gpgkey = https://releases.galeracluster.com/GPG-KEY-galeracluster.com gpgcheck = 1 安装 yum install galera-4 mysql-wsrep-8.0 数据库配置 节点一：\n# For advice on how to change settings please see # http://dev.mysql.com/doc/refman/8.0/en/server-configuration-defaults.html [mysqld] # # Remove leading # and set to the amount of RAM for the most important data # cache in MySQL. Start at 70% of total RAM for dedicated server, else 10%. # innodb_buffer_pool_size = 128M # # Remove the leading \u0026#34;# \u0026#34; to disable binary logging # Binary logging captures changes between backups and is enabled by # default. It\u0026#39;s default setting is log_bin=binlog # disable_log_bin # # Remove leading # to set options mainly useful for reporting servers. # The server defaults are faster for transactions and fast SELECTs. # Adjust sizes as needed, experiment to find the optimal values. # join_buffer_size = 128M # sort_buffer_size = 2M # read_rnd_buffer_size = 2M # # Remove leading # to revert to previous value for default_authentication_plugin, # this will increase compatibility with older clients. For background, see: # https://dev.mysql.com/doc/refman/8.0/en/server-system-variables.html#sysvar_default_authentication_plugin default-authentication-plugin=mysql_native_password datadir=/var/lib/mysql socket=/var/lib/mysql/mysql.sock log-error=/var/log/mysqld.log pid-file=/var/run/mysqld/mysqld.pid server_id=1 log_timestamps=SYSTEM lower_case_table_names=1 default_storage_engine=InnoDB innodb_autoinc_lock_mode=2 innodb_flush_log_at_trx_commit=0 innodb_buffer_pool_size=128M binlog_format=ROW character_set_server = utf8mb4 wsrep_on=ON wsrep_provider=/usr/lib64/galera-4/libgalera_smm.so wsrep_node_name=\u0026#34;node1\u0026#34; wsrep_node_address=\u0026#34;192.168.5.31\u0026#34; wsrep_cluster_name=\u0026#34;galera\u0026#34; wsrep_cluster_address=\u0026#34;gcomm://192.168.5.31,192.168.5.32,192.168.5.36\u0026#34; wsrep_provider_options=\u0026#34;gcache.size=128M; gcache.page_size=128M\u0026#34; wsrep_slave_threads=4 wsrep_sst_method=rsync wsrep_sst_auth=rsync:rsync123 [client] default-character-set=utf8mb4 socket=/var/lib/mysql/mysql.sock [mysql] default-character-set=utf8mb4 socket=/var/lib/mysql/mysql.sock #[mysqldump] #socket=/var/lib/mysql/mysql.sock #max_allowed_packet = 512M # #[mysqld_safe] ## 内存分配算法调优（默认malloc） #malloc-lib=/usr/lib64/libjemalloc.so.1 # #[mysqladmin] #socket=/var/lib/mysql/mysql.sock 节点二：\n只是server_id、wsrep_node_name和wsrep_node_address配置和节点一不一样\n节点三：\n只是server_id、wsrep_node_name和wsrep_node_address配置和节点一不一样\n启动galera 节点一：\n如果数据目录下存在galera.cache和gvwstate.dat文件，要先删掉这两个文件，再执行命令\nmysqld_bootstrap 节点二：\nsystemctl start mysqld 节点三：\nsystemctl start mysqld 数据库初始化，在节点一执行即可 grep -i \u0026#39;temporary password\u0026#39; /var/log/mysqld.log mysql_secure_installation 完成初始化后，登录mysql，新增远程用户访问或直接允许root用户远程访问，此处使用root直接远程登陆。\nuse mysql; update user set host=\u0026#39;%\u0026#39; where user=\u0026#39;root\u0026#39;; flush privileges; 集群状态监控 mysql\u0026gt; SHOW GLOBAL STATUS LIKE \u0026#39;wsrep_%\u0026#39;; +------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+ | Variable_name | Value | +------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+ | wsrep_local_state_uuid | 7b94331e-35d1-11ee-85c9-8657a425f634 | | wsrep_protocol_version | 10 | | wsrep_last_committed | 344 | | wsrep_replicated | 0 | | wsrep_replicated_bytes | 0 | | wsrep_repl_keys | 0 | | wsrep_repl_keys_bytes | 0 | | wsrep_repl_data_bytes | 0 | | wsrep_repl_other_bytes | 0 | | wsrep_received | 4 | | wsrep_received_bytes | 440 | | wsrep_local_commits | 0 | | wsrep_local_cert_failures | 0 | | wsrep_local_replays | 0 | | wsrep_local_send_queue | 0 | | wsrep_local_send_queue_max | 1 | | wsrep_local_send_queue_min | 0 | | wsrep_local_send_queue_avg | 0 | | wsrep_local_recv_queue | 0 | | wsrep_local_recv_queue_max | 1 | | wsrep_local_recv_queue_min | 0 | | wsrep_local_recv_queue_avg | 0 | | wsrep_local_cached_downto | 1 | | wsrep_flow_control_paused_ns | 0 | | wsrep_flow_control_paused | 0 | | wsrep_flow_control_sent | 0 | | wsrep_flow_control_recv | 0 | | wsrep_flow_control_active | false | | wsrep_flow_control_requested | false | | wsrep_cert_deps_distance | 0 | | wsrep_apply_oooe | 0 | | wsrep_apply_oool | 0 | | wsrep_apply_window | 1 | | wsrep_apply_waits | 0 | | wsrep_commit_oooe | 0 | | wsrep_commit_oool | 0 | | wsrep_commit_window | 1 | | wsrep_local_state | 4 | | wsrep_local_state_comment | Synced | | wsrep_cert_index_size | 0 | | wsrep_causal_reads | 0 | | wsrep_cert_interval | 0 | | wsrep_open_transactions | 0 | | wsrep_open_connections | 0 | | wsrep_incoming_addresses | AUTO,AUTO,AUTO | | wsrep_cluster_weight | 3 | | wsrep_desync_count | 0 | | wsrep_evs_delayed | | | wsrep_evs_evict_list | | | wsrep_evs_repl_latency | 0/0/0/0/0 | | wsrep_evs_state | OPERATIONAL | | wsrep_gcomm_uuid | 1c71deac-374b-11ee-b8be-7b9e717a4383 | | wsrep_gmcast_segment | 0 | | wsrep_cluster_capabilities | | | wsrep_cluster_conf_id | 18446744073709551615 | | wsrep_cluster_size | 3 | | wsrep_cluster_state_uuid | 7b94331e-35d1-11ee-85c9-8657a425f634 | | wsrep_cluster_status | Primary | | wsrep_connected | ON | | wsrep_local_bf_aborts | 0 | | wsrep_local_index | 0 | | wsrep_provider_capabilities | :MULTI_PRIMARY:CERTIFICATION:PARALLEL_APPLYING:TRX_REPLAY:ISOLATION:PAUSE:CAUSAL_READS:INCREMENTAL_WRITESET:UNORDERED:PREORDERED:STREAMING:NBO: | | wsrep_provider_name | Galera | | wsrep_provider_vendor | Codership Oy \u0026lt;info@codership.com\u0026gt; | | wsrep_provider_version | 4.15(r86ced4c6) | | wsrep_ready | ON | +------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+ 66 rows in set (0.01 sec) 参数说明：\nwsrep_local_index = 2 在集群中的索引值 wsrep_cluster_status为Primary，表示节点为主节点，正常读写。 wsrep_ready为ON，表示集群正常运行。 wsrep_connected: 如果该值为Off,且wsrep_ready的值也为Off,则说明该节点没有连接到集群 wsrep_cluster_size为3，表示集群有三个节点。 wsrep_cluster_state_uuid:在集群所有节点的值应该是相同的,有不同值的节点,说明其没有连接入集群。 wsrep_cluster_conf_id:正常情况下所有节点上该值是一样的.如果值不同,说明该节点被临时”分区”了.当节点之间网络连接恢复的时候应该会恢复一样的值。 wsrep_flow_control_paused:表示复制停止了多长时间.即表明集群因为Slave延迟而慢的程度.值为0~1,越靠近0越好,值为1表示复制完全停止.可优化wsrep_slave_threads的值来改善. wsrep_flow_control_sent:表示该节点已经停止复制了多少次. 常用命令\n# 集群状态监控： show global status like \u0026#39;wsrep_%\u0026#39;; # 查看集群节点数： show status like \u0026#39;wsrep_cluster_size\u0026#39;； # 查看集群同步状态： show status like \u0026#39;wsrep_local_state_comment\u0026#39;； Reference centos8安装配置galeracluster集群 MariaDB Galera Cluster - MySQL - dbaselife MariaDB Galera Cluster部署实战-腾讯云开发者社区-腾讯云 绿色记忆:Galera学习笔记 ","description":"","id":2,"section":"posts","tags":["demo"],"title":"原文件仿造新建","uri":"https://starifly.github.io/posts/yuyu/"},{"content":"mysql通过root账号登录，新建账户并授予权限会报错：\nmysql\u0026gt; GRANT SELECT ON *.* TO \u0026#39;select_user\u0026#39;@\u0026#39;%\u0026#39;; ERROR 1045 (28000): Access denied for user \u0026#39;root\u0026#39;@\u0026#39;%\u0026#39; (using password: YES) 解决方案：\nmysql\u0026gt; update mysql.user set Grant_priv=\u0026#39;Y\u0026#39; where user = \u0026#39;root\u0026#39; and host = \u0026#39;%\u0026#39;; Query OK, 1 row affected (0.00 sec) Rows matched: 1 Changed: 1 Warnings: 0 mysql\u0026gt; flush privileges; Query OK, 0 rows affected (0.00 sec) 然后退出登录，重启mysql服务，之后就能正常授权了。\nReference MySQL错误：Access denied for user 'root'@'%' to database 'xxx' - 我想养只狗 - 博客园 ","description":"mysql授予账户权限问题","id":3,"section":"posts","tags":["mysql"],"title":"mysql授予账户权限问题","uri":"https://starifly.github.io/posts/mysql-account-permission-grant-problem/"},{"content":"刚开始网上查教程，大部分都说使用UltraISO刻录U盘启动盘，但是坑爹的是通过这种方式刻录的启动盘在安装过程中会找不到安装介质，所以不能使用这种方式。\n后面查询其它的教程，得知可以使用Rufus制作，先格式化U盘，卷标名称位数不要超过11位，具体的刻录过程这里就不说明了。\nReference U盘安装红帽 Redhat Linux 8.4 有感_redhat8.4制作u盘启动_元宇宙隐市的博客-CSDN博客 U盘安装 Redhat Enterprise Linux ( RHEL ) 6.10_我是超级用户的博客-CSDN博客 OS: Redhat8.4实体机的安装 - lnlidawei - 博客园 RHEL 8 使用总结/避坑指南（一） 安装及配置SSH和防火墙 - Chuantao ","description":"","id":4,"section":"posts","tags":["linux","redhat"],"title":"U盘安装RedHat","uri":"https://starifly.github.io/posts/install-redhat-via-usb-disk/"},{"content":"使用k8s中的cronjob备份数据库发现一个问题，设置一个固定的时间点执行job，但是到了设定的时间后却没有执行。\n经过查资料才得知，是k8s中时区的问题，所以需要修改/etc/kubernetes/manifests/kube-scheduler.yaml配置文件，增加相应的时区设置：\nvolumeMounts: - name: localtime mountPath: /etc/localtime readOnly: true volumes: - name: localtime hostPath: path: /etc/localtime 再重启k8s，就可以解决\nsystemctl restart kubelet Reference [k8s时区问题解决方案](\rk8s时区问题解决方案 - 码农教程) kubernetes cronjob 时区问题_k8s cronjob 时区_Magiceses的博客-CSDN博客 ","description":"","id":5,"section":"posts","tags":["k8s","cronjob"],"title":"k8s cronjob执行时区问题","uri":"https://starifly.github.io/posts/k8s-cronjob-time-zone/"},{"content":"在dockerfile中添加ENV LANG=en_US.utf8\n","description":"","id":6,"section":"posts","tags":["java","docker"],"title":"docker部署java应用乱码问题","uri":"https://starifly.github.io/posts/java-application-garbled-code-in-docker/"},{"content":"Loki 架构 Loki：主服务，用于存储日志和处理查询。 Promtail：代理服务，用于采集日志，并转发给 Loki。 Grafana：通过 Web 界面来提供数据展示、查询、告警等功能。 安装 Loki 1）创建 RBAC 授权\n[root@node1 loki]# cat loki-rbac.yaml apiVersion: v1 kind: Namespace metadata: name: logging --- apiVersion: v1 kind: ServiceAccount metadata: name: loki namespace: logging --- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: loki namespace: logging rules: - apiGroups: [\u0026#34;extensions\u0026#34;] resources: [\u0026#34;podsecuritypolicies\u0026#34;] verbs: [\u0026#34;use\u0026#34;] resourceNames: [loki] --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: loki namespace: logging roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: loki subjects: - kind: ServiceAccount name: loki 2）创建 ConfigMap 文件\n[root@node1 loki]# cat loki-configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: loki namespace: logging labels: app: loki data: loki.yaml: | auth_enabled: false ingester: chunk_idle_period: 3m chunk_block_size: 262144 chunk_retain_period: 1m max_transfer_retries: 0 lifecycler: ring: kvstore: store: inmemory replication_factor: 1 limits_config: enforce_metric_name: false reject_old_samples: true reject_old_samples_max_age: 168h schema_config: configs: - from: \u0026#34;2022-05-15\u0026#34; store: boltdb-shipper object_store: filesystem schema: v11 index: prefix: index_ period: 24h server: http_listen_port: 3100 storage_config: boltdb_shipper: active_index_directory: /data/loki/boltdb-shipper-active cache_location: /data/loki/boltdb-shipper-cache cache_ttl: 24h shared_store: filesystem filesystem: directory: /data/loki/chunks chunk_store_config: max_look_back_period: 0s table_manager: retention_deletes_enabled: true retention_period: 48h compactor: working_directory: /data/loki/boltdb-shipper-compactor shared_store: filesystem 3）创建 StatefulSet\n[root@node1 loki]# cat loki-statefulset.yaml apiVersion: v1 kind: Service metadata: name: loki namespace: logging labels: app: loki spec: type: NodePort ports: - port: 3100 protocol: TCP name: http-metrics targetPort: http-metrics nodePort: 30100 selector: app: loki --- apiVersion: apps/v1 kind: StatefulSet metadata: name: loki namespace: logging labels: app: loki spec: podManagementPolicy: OrderedReady replicas: 1 selector: matchLabels: app: loki serviceName: loki updateStrategy: type: RollingUpdate template: metadata: labels: app: loki spec: serviceAccountName: loki initContainers: - name: chmod-data image: busybox:1.28.4 imagePullPolicy: IfNotPresent command: [\u0026#34;chmod\u0026#34;,\u0026#34;-R\u0026#34;,\u0026#34;777\u0026#34;,\u0026#34;/loki/data\u0026#34;] volumeMounts: - name: storage mountPath: /loki/data containers: - name: loki image: grafana/loki:2.3.0 imagePullPolicy: IfNotPresent args: - -config.file=/etc/loki/loki.yaml volumeMounts: - name: config mountPath: /etc/loki - name: storage mountPath: /data ports: - name: http-metrics containerPort: 3100 protocol: TCP livenessProbe: httpGet: path: /ready port: http-metrics scheme: HTTP initialDelaySeconds: 45 readinessProbe: httpGet: path: /ready port: http-metrics scheme: HTTP initialDelaySeconds: 45 securityContext: readOnlyRootFilesystem: true terminationGracePeriodSeconds: 4800 volumes: - name: config configMap: name: loki - name: storage hostPath: path: /app/loki 安装 Promtail 1）创建 RBAC 授权文件\n[root@node1 promtail]# cat promtail-rbac.yaml apiVersion: v1 kind: ServiceAccount metadata: name: loki-promtail labels: app: promtail namespace: logging --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: labels: app: promtail name: promtail-clusterrole namespace: logging rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;nodes\u0026#34;,\u0026#34;nodes/proxy\u0026#34;,\u0026#34;services\u0026#34;,\u0026#34;endpoints\u0026#34;,\u0026#34;pods\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: promtail-clusterrolebinding labels: app: promtail namespace: logging subjects: - kind: ServiceAccount name: loki-promtail namespace: logging roleRef: kind: ClusterRole name: promtail-clusterrole apiGroup: rbac.authorization.k8s.io 2）创建 ConfigMap 文件\n[root@node1 promtail]# cat promtail-configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: loki-promtail namespace: logging labels: app: promtail data: promtail.yaml: | client: backoff_config: max_period: 5m max_retries: 10 min_period: 500ms batchsize: 1048576 batchwait: 1s external_labels: {} timeout: 10s positions: filename: /run/promtail/positions.yaml server: http_listen_port: 3101 target_config: sync_period: 10s scrape_configs: - job_name: kubernetes-pods-name pipeline_stages: - docker: {} kubernetes_sd_configs: - role: pod relabel_configs: - source_labels: - __meta_kubernetes_pod_label_name target_label: __service__ - source_labels: - __meta_kubernetes_pod_node_name target_label: __host__ - action: drop regex: \u0026#39;\u0026#39; source_labels: - __service__ - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - action: replace replacement: $1 separator: / source_labels: - __meta_kubernetes_namespace - __service__ target_label: job - action: replace source_labels: - __meta_kubernetes_namespace target_label: namespace - action: replace source_labels: - __meta_kubernetes_pod_name target_label: pod - action: replace source_labels: - __meta_kubernetes_pod_container_name target_label: container - replacement: /var/log/pods/*$1/*.log separator: / source_labels: - __meta_kubernetes_pod_uid - __meta_kubernetes_pod_container_name target_label: __path__ - job_name: kubernetes-pods-app pipeline_stages: - docker: {} kubernetes_sd_configs: - role: pod relabel_configs: - action: drop regex: .+ source_labels: - __meta_kubernetes_pod_label_name - source_labels: - __meta_kubernetes_pod_label_app target_label: __service__ - source_labels: - __meta_kubernetes_pod_node_name target_label: __host__ - action: drop regex: \u0026#39;\u0026#39; source_labels: - __service__ - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - action: replace replacement: $1 separator: / source_labels: - __meta_kubernetes_namespace - __service__ target_label: job - action: replace source_labels: - __meta_kubernetes_namespace target_label: namespace - action: replace source_labels: - __meta_kubernetes_pod_name target_label: pod - action: replace source_labels: - __meta_kubernetes_pod_container_name target_label: container - replacement: /var/log/pods/*$1/*.log separator: / source_labels: - __meta_kubernetes_pod_uid - __meta_kubernetes_pod_container_name target_label: __path__ - job_name: kubernetes-pods-direct-controllers pipeline_stages: - docker: {} kubernetes_sd_configs: - role: pod relabel_configs: - action: drop regex: .+ separator: \u0026#39;\u0026#39; source_labels: - __meta_kubernetes_pod_label_name - __meta_kubernetes_pod_label_app - action: drop regex: \u0026#39;[0-9a-z-.]+-[0-9a-f]{8,10}\u0026#39; source_labels: - __meta_kubernetes_pod_controller_name - source_labels: - __meta_kubernetes_pod_controller_name target_label: __service__ - source_labels: - __meta_kubernetes_pod_node_name target_label: __host__ - action: drop regex: \u0026#39;\u0026#39; source_labels: - __service__ - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - action: replace replacement: $1 separator: / source_labels: - __meta_kubernetes_namespace - __service__ target_label: job - action: replace source_labels: - __meta_kubernetes_namespace target_label: namespace - action: replace source_labels: - __meta_kubernetes_pod_name target_label: pod - action: replace source_labels: - __meta_kubernetes_pod_container_name target_label: container - replacement: /var/log/pods/*$1/*.log separator: / source_labels: - __meta_kubernetes_pod_uid - __meta_kubernetes_pod_container_name target_label: __path__ - job_name: kubernetes-pods-indirect-controller pipeline_stages: - docker: {} kubernetes_sd_configs: - role: pod relabel_configs: - action: drop regex: .+ separator: \u0026#39;\u0026#39; source_labels: - __meta_kubernetes_pod_label_name - __meta_kubernetes_pod_label_app - action: keep regex: \u0026#39;[0-9a-z-.]+-[0-9a-f]{8,10}\u0026#39; source_labels: - __meta_kubernetes_pod_controller_name - action: replace regex: \u0026#39;([0-9a-z-.]+)-[0-9a-f]{8,10}\u0026#39; source_labels: - __meta_kubernetes_pod_controller_name target_label: __service__ - source_labels: - __meta_kubernetes_pod_node_name target_label: __host__ - action: drop regex: \u0026#39;\u0026#39; source_labels: - __service__ - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - action: replace replacement: $1 separator: / source_labels: - __meta_kubernetes_namespace - __service__ target_label: job - action: replace source_labels: - __meta_kubernetes_namespace target_label: namespace - action: replace source_labels: - __meta_kubernetes_pod_name target_label: pod - action: replace source_labels: - __meta_kubernetes_pod_container_name target_label: container - replacement: /var/log/pods/*$1/*.log separator: / source_labels: - __meta_kubernetes_pod_uid - __meta_kubernetes_pod_container_name target_label: __path__ - job_name: kubernetes-pods-static pipeline_stages: - docker: {} kubernetes_sd_configs: - role: pod relabel_configs: - action: drop regex: \u0026#39;\u0026#39; source_labels: - __meta_kubernetes_pod_annotation_kubernetes_io_config_mirror - action: replace source_labels: - __meta_kubernetes_pod_label_component target_label: __service__ - source_labels: - __meta_kubernetes_pod_node_name target_label: __host__ - action: drop regex: \u0026#39;\u0026#39; source_labels: - __service__ - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - action: replace replacement: $1 separator: / source_labels: - __meta_kubernetes_namespace - __service__ target_label: job - action: replace source_labels: - __meta_kubernetes_namespace target_label: namespace - action: replace source_labels: - __meta_kubernetes_pod_name target_label: pod - action: replace source_labels: - __meta_kubernetes_pod_container_name target_label: container - replacement: /var/log/pods/*$1/*.log separator: / source_labels: - __meta_kubernetes_pod_annotation_kubernetes_io_config_mirror - __meta_kubernetes_pod_container_name target_label: __path__ 3）创建 DaemonSet 文件\n[root@node1 promtail]# cat promtail-daemonset.yaml apiVersion: apps/v1 kind: DaemonSet metadata: name: loki-promtail namespace: logging labels: app: promtail spec: selector: matchLabels: app: promtail updateStrategy: rollingUpdate: maxUnavailable: 1 type: RollingUpdate template: metadata: labels: app: promtail spec: serviceAccountName: loki-promtail containers: - name: promtail image: grafana/promtail:2.3.0 imagePullPolicy: IfNotPresent args: - -config.file=/etc/promtail/promtail.yaml - -client.url=http://loki:3100/loki/api/v1/push env: - name: HOSTNAME valueFrom: fieldRef: apiVersion: v1 fieldPath: spec.nodeName volumeMounts: - mountPath: /etc/promtail name: config - mountPath: /run/promtail name: run - mountPath: /var/lib/docker/containers name: docker readOnly: true - mountPath: /var/log/pods name: pods readOnly: true ports: - containerPort: 3101 name: http-metrics protocol: TCP securityContext: readOnlyRootFilesystem: true runAsGroup: 0 runAsUser: 0 readinessProbe: failureThreshold: 5 httpGet: path: /ready port: http-metrics scheme: HTTP initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 tolerations: - effect: NoSchedule key: node-role.kubernetes.io/master operator: Exists volumes: - name: config configMap: name: loki-promtail - name: run hostPath: path: /run/promtail type: \u0026#34;\u0026#34; - name: docker hostPath: path: /var/lib/docker/containers - name: pods hostPath: path: /var/log/pods 安装 Grafana 1）创建 ConfigMap 文件\n[root@node1 grafana]# cat grafana-configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: grafana-datasources namespace: logging data: prometheus.yaml: |- { \u0026#34;apiVersion\u0026#34;: 1, \u0026#34;datasources\u0026#34;: [ { \u0026#34;access\u0026#34;:\u0026#34;proxy\u0026#34;, \u0026#34;editable\u0026#34;: true, \u0026#34;name\u0026#34;: \u0026#34;loki\u0026#34;, \u0026#34;orgId\u0026#34;: 1, \u0026#34;type\u0026#34;: \u0026#34;loki\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;http://loki:3100\u0026#34;, \u0026#34;version\u0026#34;: 1 } ] } 2）创建 Deployment 文件\n[root@node1 grafana]# cat grafana-deploy.yaml apiVersion: apps/v1 kind: Deployment metadata: name: grafana labels: app: grafana namespace: logging spec: replicas: 1 selector: matchLabels: app: grafana template: metadata: labels: app: grafana spec: containers: - name: grafana image: grafana/grafana:8.4.7 imagePullPolicy: IfNotPresent ports: - name: grafana containerPort: 3000 env: - name: GF_AUTH_BASIC_ENABLED value: \u0026#34;true\u0026#34; - name: GF_AUTH_ANONYMOUS_ENABLED value: \u0026#34;false\u0026#34; resources: limits: memory: \u0026#34;1Gi\u0026#34; cpu: \u0026#34;1000m\u0026#34; requests: memory: 500M cpu: \u0026#34;500m\u0026#34; readinessProbe: httpGet: path: /login port: 3000 volumeMounts: - mountPath: /var/lib/grafana name: grafana-storage - mountPath: /etc/grafana/provisioning/datasources name: grafana-datasources readOnly: false volumes: - name: grafana-storage emptyDir: {} - name: grafana-datasources configMap: defaultMode: 420 name: grafana-datasources --- apiVersion: v1 kind: Service metadata: name: grafana labels: app: grafana namespace: logging spec: type: NodePort ports: - port: 3000 targetPort: 3000 nodePort: 30030 selector: app: grafana Reference 使用 Loki 实现 Kubernetes 容器日志监控 K8S搭建日志和监控平台 ","description":"","id":7,"section":"posts","tags":["k8s","loki"],"title":"k8s中部署loki","uri":"https://starifly.github.io/posts/loki/"},{"content":"工具 draw.io 这个网站是大家一提到思维导图流程图啥的就会搬出来的“大佬级”的网站，几乎全平台都可用，Windows、MAC、在线网站都可以直接使用。\r干掉visio，这个画图神器真的绝了！！！\ndraw.io\nExcalidraw Excalidraw 是一款虚拟白板开源在线应用，方便画出流程图、示意图等图表。多语言支持(包含中文)。白板为手绘风格的图画，可导入第三方图形素材库，支持多人协作、支持快捷键，可从 Excel 粘贴表格等功能。\n功能特色\n界面简洁，交互细致，上手简单 免注册，支持中文界面，打开浏览器就能开始画图 支持众多键盘快捷键操作，配合鼠标能快速画图 支持通过网页Web链接共享协作 Excalidraw\nGliffy Gliffy是一款在线绘图工具，专注于帮助用户创建各种类型的图表，包括流程图、组织架构图、思维导图等。\n丰富的图形库：Gliffy提供了多样化的图标、符号和形状，你可以轻松地将其拖放到绘图区域，以表示不同的职位、部门和人员。 拖放式编辑：Gliffy的拖放式编辑使绘图过程变得简单和直观。你可以轻松添加、移动和调整元素，从而创建出清晰的组织架构图。 自定义样式：你可以自由调整元素的颜色、大小、字体等样式，以适应你的设计需求，使图表更具个性化。 实时协作：Gliffy支持多人实时协作，团队成员可以同时编辑图表，共同完善组织架构图，有效地协同工作。 模板选择：Gliffy提供了多种组织架构图的预设模板，你可以基于这些模板快速开始绘制，节省时间并确保图表的一致性。 导出和分享：绘制完成后，你可以将图表导出为常见的文件格式（如PNG、PDF、SVG），方便与团队、同事或其他人分享和交流。 云存储支持：Gliffy允许你将绘制的图表保存在云端，随时随地进行访问和编辑，使工作更加灵活。 Gliffy\nboardmix boardmix是一款国产的协作性绘图工具，使用方便，功能强大。\n实时多人协作：提供投票、留言、贴图、音视频交流等多种功能，支持多人实时协作编辑，团队成员能够同时参与图表设计和修改。 丰富的图形库：提供丰富多样的图标、符号、形状和模板，可用于创建独特的组织架构图。 灵活的绘图工具：提供圆珠笔、钢笔、虚线笔、套索、纸胶带等多种画笔工具，以及线条、形状、图标、文本等工具，自由绘制，高颜值创作！ 导出和分享：完成图表后，你可以将其导出为多种文件格式，如PNG、PDF，链接分享，网页使用，无需下载即可查看。 协作工具：除了绘图，boardmix还提供了丰富的协作工具，如评论、讨论区和实时聊天，有助于团队成员在图表中交流想法和提供建议。 云端存储：你的绘图内容可以保存在boardmix的云端，随时在不同设备上进行访问和编辑。 AIGC生成能力：内置强大AIGC生成能力，支持调用AI一键生成流程图、思维导图、SWOT分析等，高效灵感来源！ boardmix\nVisual Paradigm Online 这个网站最大的一个优点就是直截了当的给你提供了各方面各类型的流程图模板，能够直接选择并使用。多功能设计及制图工具，包括信息图编辑器、电子书编辑器、图表编辑器、拼贴画编辑器等。在线创建惊人的视觉效果、UML、BPMN、ArchiMate。\n产品特色\n在线 PDF 工具套件：使用这款高效的多合一 PDF 编辑套件管理您的 PDF。您还可以将 PDF 文件转换为各种文档格式，反之亦然。 设计编辑器：利用 Visual Paradigm 直观的在线图形设计工具，轻松自如地创建伟大的在线设计。 设计资源：用我们直观的在线图形设计编辑工具创建令人惊叹的设计，它具有各种各样的图形资源。 免费翻页电子书工具：为你的受众提供一个无缝的在线图书阅读体验，用一个强大的书架来组织你的目录。 统计图表工具：用免费的图表制作工具在线制作漂亮的图表。在几分钟内创建具有自定义风格的图表。 照片编辑：一个具有强大功能的简单编辑工具。轻松地创建、增强和编辑照片图像。 表单设计工具：收集数据。获得反馈。分享结果。不需要编码。开始使用我们易于使用的表格生成器。 试算表编辑器：用一个在线电子表格编辑器来组织、存储和分析数据。 visual paradigm online\n迅捷画图 要论模板，论制作简单，迅捷画图也拿得出手！国产的软件，可能大家更能接受。但是，比较差的体验就是好多模板要付费，这也是我最讨厌的地方。\n迅捷画图\nVisio Visio是微软推出的画图工具。\nMicrosoft Visio\nle5le 在线绘图（微服务架构图、网络拓扑图、流程图）工具，le5le-topology是一个开源、易扩展、方便集成的在线绘图（微服务架构图、网络拓扑图、流程图、活动图、时序图、类图等）工具。\n乐吾乐\n飞书文档 支持文档、脑图、流程图、表格等等，还可以很好的外链分享和协作。\n飞书文档\nZen Flowchart zen flowchart是一个在线画流程图的工具，英文界面\nzen flowchart\nD2 D2: 一款专用文本绘制流程图的神器\nPDDON PDDON是一款完全免费的轻量级专业在线画图和低代码工具。\nPDDON\n思维导图 xmind gitmind 图标 https://github.com/cncf/artwork 阿里巴巴矢量图标库 https://github.com/edent/SuperTinyIcons https://icons8.com/icons https://github.com/kubernetes/community/tree/master/icons 一些案例 ","description":"超好用的画图工具介绍","id":8,"section":"posts","tags":["画图"],"title":"画图","uri":"https://starifly.github.io/posts/draw/"},{"content":"elasticsearch cat elasticsearch.yaml apiVersion: apps/v1 kind: StatefulSet metadata: name: es-cluster spec: serviceName: elasticsearch replicas: 1 selector: matchLabels: app: elasticsearch template: metadata: labels: app: elasticsearch spec: containers: - name: elasticsearch #image: docker.elastic.co/elasticsearch/elasticsearch:7.5.0 image: elasticsearch:7.5.0 resources: limits: cpu: 1000m requests: cpu: 100m ports: - containerPort: 9200 name: rest protocol: TCP - containerPort: 9300 name: inter-node protocol: TCP # volumeMounts: # - name: data # mountPath: /usr/share/elasticsearch/data env: - name: cluster.name value: k8s-logs - name: node.name valueFrom: fieldRef: fieldPath: metadata.name - name: discovery.seed_hosts value: \u0026#34;es-cluster-0.elasticsearch\u0026#34; - name: cluster.initial_master_nodes value: \u0026#34;es-cluster-0\u0026#34; - name: ES_JAVA_OPTS value: \u0026#34;-Xms512m -Xmx512m\u0026#34; initContainers: - name: increase-vm-max-map image: busybox command: [\u0026#34;sysctl\u0026#34;, \u0026#34;-w\u0026#34;, \u0026#34;vm.max_map_count=262144\u0026#34;] securityContext: privileged: true - name: increase-fd-ulimit image: busybox command: [\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;ulimit -n 65536\u0026#34;] securityContext: privileged: true # volumeClaimTemplates: # - metadata: # name: data # labels: # app: elasticsearch # spec: # accessModes: [ \u0026#34;ReadWriteOnce\u0026#34; ] # # storageClassName: \u0026#34;\u0026#34; # resources: # requests: # storage: 3Gi --- kind: Service apiVersion: v1 metadata: name: elasticsearch labels: app: elasticsearch spec: type: NodePort ports: - name: rest nodePort: 32321 port: 9200 protocol: TCP targetPort: 9200 - name: inter-node nodePort: 32322 port: 9300 protocol: TCP targetPort: 9300 selector: app: elasticsearch kibana cat kibana.yaml apiVersion: apps/v1 kind: Deployment metadata: name: kibana labels: app: kibana spec: replicas: 1 selector: matchLabels: app: kibana template: metadata: labels: app: kibana spec: containers: - name: kibana image: kibana:7.5.0 resources: limits: cpu: 1000m requests: cpu: 100m env: - name: ELASTICSEARCH_URL value: http://elasticsearch:9200 ports: - containerPort: 5601 --- apiVersion: v1 kind: Service metadata: name: kibana-np spec: selector: app: kibana type: NodePort ports: - port: 8080 targetPort: 5601 nodePort: 30000 fluentd cat fluentd.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: fluentd labels: app: fluentd rules: - apiGroups: - \u0026#34;\u0026#34; resources: - pods - namespaces verbs: - get - list - watch --- apiVersion: v1 kind: ServiceAccount metadata: name: fluentd labels: app: fluentd --- on: rbac.authorization.k8s.io/v1 metadata: name: fluentd roleRef: kind: ClusterRole name: fluentd apiGroup: rbac.authorization.k8s.io subjects: - kind: ServiceAccount name: fluentd namespace: default --- apiVersion: apps/v1 kind: DaemonSet metadata: name: fluentd labels: app: fluentd spec: selector: matchLabels: app: fluentd template: metadata: labels: app: fluentd spec: serviceAccount: fluentd serviceAccountName: fluentd containers: - name: fluentd image: fluent/fluentd-kubernetes-daemonset:v1.4.2-debian-elasticsearch-1.1 env: - name: FLUENT_ELASTICSEARCH_HOST value: \u0026#34;elasticsearch.default.svc.cluster.local\u0026#34; - name: FLUENT_ELASTICSEARCH_PORT value: \u0026#34;9200\u0026#34; - name: FLUENT_ELASTICSEARCH_SCHEME value: \u0026#34;http\u0026#34; - name: FLUENTD_SYSTEMD_CONF value: disable resources: limits: memory: 512Mi requests: cpu: 100m memory: 200Mi volumeMounts: - name: varlog mountPath: /var/log - name: varlibdockercontainers mountPath: /var/lib/docker/containers readOnly: true terminationGracePeriodSeconds: 30 volumes: - name: varlog hostPath: path: /var/log - name: varlibdockercontainers hostPath: path: /var/lib/docker/containers Reference Kubernetes 集群日志和 EFK架构日志方案 基于 EFK 的 Kubernetes 日志采集方案 ","description":"本文介绍了如何在k8s中部署efk","id":9,"section":"posts","tags":["k8s","efk"],"title":"k8s部署EFK","uri":"https://starifly.github.io/posts/k8s-efk/"},{"content":"1、网络拓扑参考\n2、设备位置图参考\n3、服务器配置信息及运行服务统计\n一般来说，内存越多越好。\n对于一个中等规模的集群，监视/管理器节点可以使用64GB；对于具有数百个osd的大型集群，128GB是一个合理的目标。\nOSD节点一般情况下每1T硬盘对应1G内存，详见egon整理的项目硬件参数附件，或者参考下述配置也可以\n最低硬件建议，详解见\rhttps://docs.ceph.com/en/latest/start/hardware-recommendations/\nReference 附录2、ceph安装配置介绍与主机优化 ","description":"","id":10,"section":"posts","tags":["ceph"],"title":"ceph硬件布置参考","uri":"https://starifly.github.io/posts/ceph-hardware-layout-reference/"},{"content":"准备 1、执行ceph -s确认存储集群状态，保证为健康状态。\n[root@ceph001 ~]# ceph -s cluster: id: d00c744a-17f6-4768-95de-1243202557f2 health: HEALTH_OK services: mon: 3 daemons, quorum ceph001,ceph002,ceph003 (age 41m) mgr: ceph001(active, since 43m), standbys: ceph003 mds: cephfs:1 {0=ceoh002=up:active} 2 up:standby osd: 6 osds: 6 up (since 3m), 6 in (since 3m) rgw: 3 daemons active (ceph001, ceph002, ceph003) task status: data: pools: 10 pools, 96 pgs objects: 803 objects, 2.1 GiB usage: 13 GiB used, 62 GiB / 75 GiB avail pgs: 96 active+clean 2、执行ceph osd tree ,记录变更前的结构。以及存储池pool及其他信息。\n[root@ceph001 ~]# ceph osd tree ID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 0.07315 root default -3 0.02438 host ceph001 0 hdd 0.01949 osd.0 up 1.00000 1.00000 3 hdd 0.00490 osd.3 up 1.00000 1.00000 -5 0.02438 host ceph002 1 hdd 0.01949 osd.1 up 1.00000 1.00000 4 hdd 0.00490 osd.4 up 1.00000 1.00000 -7 0.02438 host ceph003 2 hdd 0.01949 osd.2 up 1.00000 1.00000 5 hdd 0.00490 osd.5 up 1.00000 1.00000 查看存储池pool规则及其他详细信息\n[root@ceph001 ~]# ceph osd pool ls detail pool 1 \u0026#39;.rgw.root\u0026#39; replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 8 pgp_num 8 autoscale_mode warn last_change 1108 lfor 0/1108/1106 flags hashpspool stripe_width 0 application rgw pool 2 \u0026#39;default.rgw.control\u0026#39; replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 8 pgp_num 8 autoscale_mode warn last_change 1472 lfor 0/1472/1470 flags hashpspool stripe_width 0 application rbd pool 3 \u0026#39;default.rgw.meta\u0026#39; replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 8 pgp_num 8 autoscale_mode warn last_change 1691 lfor 0/1691/1689 flags hashpspool stripe_width 0 application rgw pool 4 \u0026#39;default.rgw.log\u0026#39; replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 8 pgp_num 8 autoscale_mode warn last_change 1580 lfor 0/1580/1578 flags hashpspool stripe_width 0 application rgw pool 5 \u0026#39;cephfs_data\u0026#39; replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 8 pgp_num 8 autoscale_mode warn last_change 29 flags hashpspool stripe_width 0 application cephfs pool 6 \u0026#39;cephfs_metadata\u0026#39; replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 8 pgp_num 8 autoscale_mode warn last_change 29 flags hashpspool stripe_width 0 pg_autoscale_bias 4 pg_num_min 16 recovery_priority 5 application cephfs pool 7 \u0026#39;testpool\u0026#39; replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 8 pgp_num 8 autoscale_mode warn last_change 908 lfor 0/908/906 flags hashpspool,selfmanaged_snaps stripe_width 0 application rbd removed_snaps [1~3] pool 11 \u0026#39;default.rgw.buckets.index\u0026#39; replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 16 pgp_num 16 autoscale_mode warn last_change 1247 lfor 0/1247/1245 flags hashpspool stripe_width 0 application rgw pool 12 \u0026#39;default.rgw.buckets.data\u0026#39; replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 16 pgp_num 16 autoscale_mode warn last_change 1177 lfor 0/1177/1175 flags hashpspool stripe_width 0 application rgw pool 13 \u0026#39;default.rgw.buckets.non-ec\u0026#39; replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 8 pgp_num 8 autoscale_mode warn last_change 1362 lfor 0/1362/1360 flags hashpspool stripe_width 0 application rgw 3、备份集群的crushmap文件。\nceph osd getcrushmap -o crushmap.bak 配置 crush class 默认情况下，所有 osd 都会 class 的类型是 hdd：\n[root@ceph001 ~]# ceph osd crush class ls [ \u0026#34;hdd\u0026#34; ] 当前3个节点，每个节点上有2个OSD，因为没有挂SSD硬盘，所以把OSD3-5模拟为一组SSD。首先，需要将 osd3-5 从 hdd 组中去除掉：\n[root@ceph001 ~]# for i in {3..5};do ceph osd crush rm-device-class osd.$i;done done removing class of osd(s): 3 done removing class of osd(s): 4 done removing class of osd(s): 5 查看 osd\n[root@ceph001 ~]# ceph osd tree ID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 0.07315 root default -3 0.02438 host ceph001 3 0.00490 osd.3 up 1.00000 1.00000 0 hdd 0.01949 osd.0 up 1.00000 1.00000 -5 0.02438 host ceph002 4 0.00490 osd.4 up 1.00000 1.00000 1 hdd 0.01949 osd.1 up 1.00000 1.00000 -7 0.02438 host ceph003 5 0.00490 osd.5 up 1.00000 1.00000 2 hdd 0.01949 osd.2 up 1.00000 1.00000 可以看到 osd3-5 class 列已经没有 hdd 标识了。此时就可以通过命令将osd3-5添加到 ssd 组了，如下：\n[root@ceph001 ~]# for i in {3..5}; do ceph osd crush set-device-class ssd osd.$i;done set osd(s) 3 to class \u0026#39;ssd\u0026#39; set osd(s) 4 to class \u0026#39;ssd\u0026#39; set osd(s) 5 to class \u0026#39;ssd\u0026#39; # 查看 osd [root@ceph001 ~]# ceph osd tree ID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 0.07315 root default -3 0.02438 host ceph001 0 hdd 0.01949 osd.0 up 1.00000 1.00000 3 ssd 0.00490 osd.3 up 1.00000 1.00000 -5 0.02438 host ceph002 1 hdd 0.01949 osd.1 up 1.00000 1.00000 4 ssd 0.00490 osd.4 up 1.00000 1.00000 -7 0.02438 host ceph003 2 hdd 0.01949 osd.2 up 1.00000 1.00000 5 ssd 0.00490 osd.5 up 1.00000 1.00000 # 查看 class [root@ceph001 ~]# ceph osd crush class ls [ \u0026#34;hdd\u0026#34;, \u0026#34;ssd\u0026#34; ] 可以发现 osd3-5 的 class 列都变为 ssd，查看 crush class 也多出一个 ssd 的组，接下来就需要创建ssd 的规则。\n命令生成osd树形结构 # 创建数据中心：datacenter0 ceph osd crush add-bucket datacenter0 datacenter ​ # 创建机房：room0 ceph osd crush add-bucket room0 room ​ # 创建机架：rack0、rack1、rack2（模拟3个机架） ceph osd crush add-bucket rack0 rack ceph osd crush add-bucket rack1 rack ceph osd crush add-bucket rack2 rack ​ # 把机房room0移动到数据中心datacenter0下 ceph osd crush move room0 datacenter=datacenter0 ​ # 把机架rack0、rack1、rack2移动到机房room0下 ceph osd crush move rack0 room=room0 ceph osd crush move rack1 room=room0 ceph osd crush move rack2 room=room0 ​ # 把主机ceph001移动到：datacenter0/room0/rack0下 ceph osd crush move ceph001 datacenter=datacenter0 room=room0 rack=rack0 ​ # 把主机ceph002移动到：datacenter0/room0/rack1下 ceph osd crush move ceph002 datacenter=datacenter0 room=room0 rack=rack1 ​ # 把主机ceph003移动到：datacenter0/room0/rack2下 ceph osd crush move ceph003 datacenter=datacenter0 room=room0 rack=rack2 查看osd\n[root@ceph001 ~]# ceph osd tree ID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -13 0.07315 datacenter datacenter0 -14 0.07315 room room0 -15 0.02438 rack rack0 -3 0.02438 host ceph001 0 hdd 0.01949 osd.0 up 1.00000 1.00000 3 ssd 0.00490 osd.3 up 1.00000 1.00000 -16 0.02438 rack rack1 -5 0.02438 host ceph002 1 hdd 0.01949 osd.1 up 1.00000 1.00000 4 ssd 0.00490 osd.4 up 1.00000 1.00000 -17 0.02438 rack rack2 -7 0.02438 host ceph003 2 hdd 0.01949 osd.2 up 1.00000 1.00000 5 ssd 0.00490 osd.5 up 1.00000 1.00000 -1 0 root default 规则 crushmap配置中最核心的当属rule了，crush rule决定了三点重要事项：\n1、从OSDMap中的哪个节点开始查找 2、使用那个节点作为故障隔离域 3、定位副本的搜索模式（广度优先 or 深度优先）。 pg 选择osd的过程，首先要知道在rules中 指明从osdmap中哪个节点开始查找，入口点默认为default也就是root节点，\n然后隔离域为host节点(也就是同一个host下面不能选择两个子节点)。由default到3个host的选择过程，\n这里由default根据节点的bucket类型选择下一个子节点，由子节点再根据本身的类型继续选择，直到选择到host，然后在host下选择一个osd。\n这里创建如下两个规则：\nreplicated_rule_rack：用于hdd分组的osd，以rack为故障域的应用规则；\nreplicated_rule_rack_ssd：用于普通ssd分区为osd，以rack为故障域的应用规则；\nceph osd crush rule create-replicated {name} {root} {failure-domain-type} {class} //root，The name of the node under which data should be placed.即应该放置数据的root bucket的名称,例如default。 ceph osd crush rule create-replicated replicated_rule_rack datacenter0 rack hdd ceph osd crush rule create-replicated replicated_rule_rack_ssd datacenter0 rack ssd 查看 rule\n[root@ceph001 ~]# ceph osd crush rule ls replicated_rule replicated_rule_rack replicated_rule_rack_ssd # 查看规则具体信息 ceph osd crush rule dump rule_name for i in $(rados lspools | grep -v ssdpool);do ceph osd pool set $i crush_rule replicated_rule_rack ;done\n存储池应用规则\n应用上一步创建的replicated_rule_rack_ssd规则到除了数据池以外的存储池\nfor i in $(rados lspools | grep -v cephfs_data | grep -v default.rgw.buckets.data | grep -v testpool);do ceph osd pool set $i crush_rule replicated_rule_rack_ssd ;done 应用上一步创建的replicated_rule_rack规则到数据池\nceph osd pool set testpool crush_rule replicated_rule_rack ceph osd pool set default.rgw.buckets.data crush_rule replicated_rule_rack ceph osd pool set cephfs_data crush_rule replicated_rule_rack Reference ceph rack故障域调整 [ ceph ] CEPH 部署完整版（CentOS 7 + luminous） 03 分布式存储ceph之crush规则配置÷ ","description":"","id":11,"section":"posts","tags":["ceph"],"title":"ceph故障域","uri":"https://starifly.github.io/posts/ceph-crushmap/"},{"content":"\rhttps://stor.51cto.com/art/202006/618273.htm#topx\n","description":"Ceph如何扩展到超过十亿个对象","id":12,"section":"posts","tags":["ceph"],"title":"Ceph如何扩展到超过十亿个对象","uri":"https://starifly.github.io/posts/how-ceph-scales-to-more-than-one-billion-objects/"},{"content":"ceph 在同一个集群配置多个zone，但不同 zone 之间的 rgw 不复制，这种情形应该可以适应多租户环境，因为希望每个租户之间的数据是相互独立的，具体配置可以参考 https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/3/html-single/object_gateway_guide_for_red_hat_enterprise_linux/index#configuring-multiple-zones-without-replication-rgw\n","description":"","id":13,"section":"posts","tags":["ceph","rgw"],"title":"ceph在不复制rgw的情况下配置多个区域","uri":"https://starifly.github.io/posts/ceph-configure-multi-regions-without-replicating-rgw/"},{"content":"环境 源 ceph 集群（192.168.5.203:20003） 基于源集群创建一个新的 rgw 端点(192.168.5.128:7480)，用于将数据同步到另一个 S3提供者 S3 目标（192.168.4.13:7480） 操作步骤 准备存储池\nfor pool in sync.rgw.meta sync.rgw.log sync.rgw.control sync.rgw.buckets.non-ec sync.rgw.buckets.index sync.rgw.buckets.data; do ceph osd pool create $pool 16 16 replicated; done 创建新区域\nradosgw-admin zone create --rgw-zonegroup=default --rgw-zone=sync --endpoints=http://192.168.5.128:7480/ --tier-type=cloud 修改现有区域\nradosgw-admin zone modify --rgw-zonegroup=default --rgw-zone=default --endpoints=http://192.168.5.203:20003 配置同步区域以使用此系统用户\n我们将更改两个区域以使用我们的新系统用户。\nradosgw-admin zone modify --rgw-zonegroup=default --rgw-zone=default --access-key=GVGIA33TY7G86W1QDYKV --secret=0t1IyppTAdQHfpdrUJh1NfPJPBTF9Qb4weByuK8L radosgw-admin zone modify --rgw-zonegroup=default --rgw-zone=sync --access-key=GVGIA33TY7G86W1QDYKV --secret=0t1IyppTAdQHfpdrUJh1NfPJPBTF9Qb4weByuK8L 确保默认区域是主区域\nradosgw-admin zonegroup get，实际上虽然查询default zone是master，但是下面这条语句还是要执行一次。 如果defaultzone 不是 master，则可以通过执行强制它radosgw-admin zone modify \u0026ndash;rgw-zonegroup=default \u0026ndash;rgw-zone=default \u0026ndash;master \u0026ndash;default\n提交更改并验证配置\nradosgw-admin period update --commit 配置新区域以同步数据到目的集群\nradosgw-admin zone modify --rgw-zonegroup=default --rgw-zone=sync --tier-config=connection.endpoint=http://192.168.4.13:7480,connection.access_key=JO4RQ1787A6OGI6XMFDW,connection.secret=Dx5kKGUUeR0DaSRYueBWhV6oDRvJ9oXH2gPcVJ6s，target_path=\\$\\{bucket\\} 其中的 target_path 代表同步的目的位置，这里我们配置成源 bucket 对应的位置。\n提交更改\nradosgw-admin zone get --rgw-zone=sync 配置 RGW\n我们须修改源集群 rgw（192.168.5.203） 的配置和 同步 rgw（192.168.5.128） 实例的配置\n在 ceph 配置 rgw 段增加如下配置：\nhost = ceph001 rgw zone = default [client.rgw.vm128] host = vm128 rgw zone = sync 重启 rgw 以使更改生效。\n至此我们就可以测试两个集群之间的数据同步了。\nReference 【大咖专栏】如何配置CEPH RGW对象存储与公有云同步 https://docs.ceph.com/en/latest/radosgw/multisite/ https://docs.ceph.com/en/latest/radosgw/cloud-sync-module/#cloud-sync-tier-type-configuration SETTING UP CEPH CLOUD SYNC MODULE ","description":"","id":14,"section":"posts","tags":["ceph","rgw"],"title":"ceph rgw同步","uri":"https://starifly.github.io/posts/ceph-rgw-sync/"},{"content":"\rhttps://blog.csdn.net/Michaelwubo/article/details/113109341\n","description":"Ceph与其它存储系统对比","id":15,"section":"posts","tags":["ceph"],"title":"Ceph与其它存储系统对比","uri":"https://starifly.github.io/posts/ceph-vs-other-storage/"},{"content":"之前的文章已经介绍过ingress，这里说明怎么部署新版的ingress\n版本说明 inrress-nginx版本为1.4.0，k8s版本为1.23.6，具体版本匹配参考\rhttps://github.com/kubernetes/ingress-nginx。\n部署ingress-nginx 首先去官网下载1.4.0版本的deploy.yaml文件，这个文件内包含了ingress pod和service，文件内容如下（注意修改镜像地址为dockerhub上的地址）：\napiVersion: v1 kind: Namespace metadata: labels: app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx name: ingress-nginx --- apiVersion: v1 automountServiceAccountToken: true kind: ServiceAccount metadata: labels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.4.0 name: ingress-nginx namespace: ingress-nginx --- apiVersion: v1 kind: ServiceAccount metadata: labels: app.kubernetes.io/component: admission-webhook app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.4.0 name: ingress-nginx-admission namespace: ingress-nginx --- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: labels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.4.0 name: ingress-nginx namespace: ingress-nginx rules: - apiGroups: - \u0026#34;\u0026#34; resources: - namespaces verbs: - get - apiGroups: - \u0026#34;\u0026#34; resources: - configmaps - pods - secrets - endpoints verbs: - get - list - watch - apiGroups: - \u0026#34;\u0026#34; resources: - services verbs: - get - list - watch - apiGroups: - networking.k8s.io resources: - ingresses verbs: - get - list - watch - apiGroups: - networking.k8s.io resources: - ingresses/status verbs: - update - apiGroups: - networking.k8s.io resources: - ingressclasses verbs: - get - list - watch - apiGroups: - \u0026#34;\u0026#34; resourceNames: - ingress-controller-leader resources: - configmaps verbs: - get - update - apiGroups: - \u0026#34;\u0026#34; resources: - configmaps verbs: - create - apiGroups: - coordination.k8s.io resourceNames: - ingress-controller-leader resources: - leases verbs: - get - update - apiGroups: - coordination.k8s.io resources: - leases verbs: - create - apiGroups: - \u0026#34;\u0026#34; resources: - events verbs: - create - patch - apiGroups: - discovery.k8s.io resources: - endpointslices verbs: - list - watch - get --- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: labels: app.kubernetes.io/component: admission-webhook app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.4.0 name: ingress-nginx-admission namespace: ingress-nginx rules: - apiGroups: - \u0026#34;\u0026#34; resources: - secrets verbs: - get - create --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.4.0 name: ingress-nginx rules: - apiGroups: - \u0026#34;\u0026#34; resources: - configmaps - endpoints - nodes - pods - secrets - namespaces verbs: - list - watch - apiGroups: - coordination.k8s.io resources: - leases verbs: - list - watch - apiGroups: - \u0026#34;\u0026#34; resources: - nodes verbs: - get - apiGroups: - \u0026#34;\u0026#34; resources: - services verbs: - get - list - watch - apiGroups: - networking.k8s.io resources: - ingresses verbs: - get - list - watch - apiGroups: - \u0026#34;\u0026#34; resources: - events verbs: - create - patch - apiGroups: - networking.k8s.io resources: - ingresses/status verbs: - update - apiGroups: - networking.k8s.io resources: - ingressclasses verbs: - get - list - watch - apiGroups: - discovery.k8s.io resources: - endpointslices verbs: - list - watch - get --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: app.kubernetes.io/component: admission-webhook app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.4.0 name: ingress-nginx-admission rules: - apiGroups: - admissionregistration.k8s.io resources: - validatingwebhookconfigurations verbs: - get - update --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: labels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.4.0 name: ingress-nginx namespace: ingress-nginx roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: ingress-nginx subjects: - kind: ServiceAccount name: ingress-nginx namespace: ingress-nginx --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: labels: app.kubernetes.io/component: admission-webhook app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.4.0 name: ingress-nginx-admission namespace: ingress-nginx roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: ingress-nginx-admission subjects: - kind: ServiceAccount name: ingress-nginx-admission namespace: ingress-nginx --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: labels: app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.4.0 name: ingress-nginx roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: ingress-nginx subjects: - kind: ServiceAccount name: ingress-nginx namespace: ingress-nginx --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: labels: app.kubernetes.io/component: admission-webhook app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.4.0 name: ingress-nginx-admission roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: ingress-nginx-admission subjects: - kind: ServiceAccount name: ingress-nginx-admission namespace: ingress-nginx --- apiVersion: v1 data: allow-snippet-annotations: \u0026#34;true\u0026#34; kind: ConfigMap metadata: labels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.4.0 name: ingress-nginx-controller namespace: ingress-nginx --- apiVersion: v1 kind: Service metadata: labels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.4.0 name: ingress-nginx-controller namespace: ingress-nginx spec: ipFamilies: - IPv4 ipFamilyPolicy: SingleStack ports: - appProtocol: http name: http port: 80 protocol: TCP targetPort: http - appProtocol: https name: https port: 443 protocol: TCP targetPort: https selector: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx type: NodePort --- apiVersion: v1 kind: Service metadata: labels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.4.0 name: ingress-nginx-controller-admission namespace: ingress-nginx spec: ports: - appProtocol: https name: https-webhook port: 443 targetPort: webhook selector: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx type: ClusterIP --- apiVersion: apps/v1 kind: Deployment metadata: labels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.4.0 name: ingress-nginx-controller namespace: ingress-nginx spec: minReadySeconds: 0 revisionHistoryLimit: 10 selector: matchLabels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx template: metadata: labels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx spec: containers: - args: - /nginx-ingress-controller - --election-id=ingress-controller-leader - --controller-class=k8s.io/ingress-nginx - --ingress-class=nginx - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller - --validating-webhook=:8443 - --validating-webhook-certificate=/usr/local/certificates/cert - --validating-webhook-key=/usr/local/certificates/key env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: LD_PRELOAD value: /usr/local/lib/libmimalloc.so image: dyrnq/ingress-nginx-controller:v1.4.0 imagePullPolicy: IfNotPresent lifecycle: preStop: exec: command: - /wait-shutdown livenessProbe: failureThreshold: 5 httpGet: path: /healthz port: 10254 scheme: HTTP initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 name: controller ports: - containerPort: 80 name: http protocol: TCP - containerPort: 443 name: https protocol: TCP - containerPort: 8443 name: webhook protocol: TCP readinessProbe: failureThreshold: 3 httpGet: path: /healthz port: 10254 scheme: HTTP initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 resources: requests: cpu: 100m memory: 90Mi securityContext: allowPrivilegeEscalation: true capabilities: add: - NET_BIND_SERVICE drop: - ALL runAsUser: 101 volumeMounts: - mountPath: /usr/local/certificates/ name: webhook-cert readOnly: true dnsPolicy: ClusterFirst nodeSelector: kubernetes.io/os: linux serviceAccountName: ingress-nginx terminationGracePeriodSeconds: 300 volumes: - name: webhook-cert secret: secretName: ingress-nginx-admission --- apiVersion: batch/v1 kind: Job metadata: labels: app.kubernetes.io/component: admission-webhook app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.4.0 name: ingress-nginx-admission-create namespace: ingress-nginx spec: template: metadata: labels: app.kubernetes.io/component: admission-webhook app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.4.0 name: ingress-nginx-admission-create spec: containers: - args: - create - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc - --namespace=$(POD_NAMESPACE) - --secret-name=ingress-nginx-admission env: - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace image: dyrnq/kube-webhook-certgen:v20220916-gd32f8c343 imagePullPolicy: IfNotPresent name: create securityContext: allowPrivilegeEscalation: false nodeSelector: kubernetes.io/os: linux restartPolicy: OnFailure securityContext: fsGroup: 2000 runAsNonRoot: true runAsUser: 2000 serviceAccountName: ingress-nginx-admission --- apiVersion: batch/v1 kind: Job metadata: labels: app.kubernetes.io/component: admission-webhook app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.4.0 name: ingress-nginx-admission-patch namespace: ingress-nginx spec: template: metadata: labels: app.kubernetes.io/component: admission-webhook app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.4.0 name: ingress-nginx-admission-patch spec: containers: - args: - patch - --webhook-name=ingress-nginx-admission - --namespace=$(POD_NAMESPACE) - --patch-mutating=false - --secret-name=ingress-nginx-admission - --patch-failure-policy=Fail env: - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace image: dyrnq/kube-webhook-certgen:v20220916-gd32f8c343 imagePullPolicy: IfNotPresent name: patch securityContext: allowPrivilegeEscalation: false nodeSelector: kubernetes.io/os: linux restartPolicy: OnFailure securityContext: fsGroup: 2000 runAsNonRoot: true runAsUser: 2000 serviceAccountName: ingress-nginx-admission --- apiVersion: networking.k8s.io/v1 kind: IngressClass metadata: labels: app.kubernetes.io/component: controller app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.4.0 name: nginx spec: controller: k8s.io/ingress-nginx --- apiVersion: admissionregistration.k8s.io/v1 kind: ValidatingWebhookConfiguration metadata: labels: app.kubernetes.io/component: admission-webhook app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginx app.kubernetes.io/version: 1.4.0 name: ingress-nginx-admission webhooks: - admissionReviewVersions: - v1 clientConfig: service: name: ingress-nginx-controller-admission namespace: ingress-nginx path: /networking/v1/ingresses failurePolicy: Fail matchPolicy: Equivalent name: validate.nginx.ingress.kubernetes.io rules: - apiGroups: - networking.k8s.io apiVersions: - v1 operations: - CREATE - UPDATE resources: - ingresses sideEffects: None kubectl create -f deploy.yaml 部署pod和svc java.yaml\napiVersion: apps/v1 kind: Deployment metadata: name: java-deployment namespace: dev spec: replicas: 3 selector: matchLabels: app: java-pod template: metadata: labels: app: java-pod spec: containers: - name: java imagePullPolicy: IfNotPresent image: 10.0.4.15:5000/jar/project:jar01_31 ports: - containerPort: 8080 --- apiVersion: v1 kind: Service metadata: name: java-service namespace: dev spec: selector: app: java-pod type: ClusterIP ports: - port: 8080 targetPort: 8080 protocol: TCP kubectl create -f java.yaml 创建ingress ingress-http.yaml\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-http namespace: dev spec: ingressClassName: nginx rules: - host: java.k8s.com http: paths: - path: / pathType: Prefix backend: service: name: java-service port: number: 8080 注意根据查询k8s中的classname配置ingressClassName\nkubectl create -f ingress-http 这样配置好后我们在系统中配置好相应的域名和端口就能正常访问应用了。\n","description":"","id":16,"section":"posts","tags":["k8s","ingress"],"title":"k8s ingress 2022部署方案","uri":"https://starifly.github.io/posts/k8s-ingress-2022/"},{"content":"Ingress简介 service的作用 对集群内部，它不断跟踪pod的变化，更新endpoint中对应pod的对象，提供了ip不断变化的pod的服务发现机制 对集群外部，他类似负载均衡器，可以在集群内外部对pod进行访问 外部访问k8s集群内的服务 NodePort: 测试环境使用还行，当有几十上百的服务在集群中运行时，NodePort的端口管理就是个灾难 LoadBalancer: 受限于云平台，且通常在云平台部署LoadBalancer还需要额外的费用 Ingress: 可以简单理解为service的service，它其实就是一组基于域名和URL路径，把用户的请求转发到一个或多个service的规则 Ingress组成 ingress ingress是一个API对象，通过yaml文件来配置，ingress对象的作用是定义请求如何转发到service的规则，可以理解为配置模板 ingress通过http或https暴露集群内部service，给service提供外部URL、负载均衡、SSL/TLs能力以及基于域名的反向代理。ingress要依靠ingress-controller来具体实现以上功能 ingress-controller ingress-controller是具体实现反向代理及负载均衡的程序，对ingress定义的规则进行解析，根据配置的规则来实现请求转发 ingress -controller并不是k8s自带的组件，实际上ingress-controller只是一个统称，用户可以选择不同的ingress-controller实现，目前，由k8s维护的ingress-controller只有google云的ccz与ingress-nginx两个，其他还有很多第三方维护的ingres-controller，具体可以参考官方文档。但是不管哪一种ingress-controller，实现的机制都大同小异，只是在具体配置上有差异\n3.一般来说，ingress-controller的形式都是一个pod,里面跑着daemon程序和反向代理程序。daemon负责不断监控集群的变化，根据ingress对象生成配置并应用新配置到反向代理，比如ingress -nginx就是动态生成nginx配置，动态更新upstream，并在需要的时候reload程序应用新配置。为了方便，后面的例子都以k8s官方维护的ingress-nginx为例 Ingress工作原理 ingress-controller通过和 kubernetes APIServer 交互，动态的去感知集群中ingress规则变化 然后读取它，按照自定义的规则，规则就是写明了哪个域名对应哪个service，生成一段nginx配置 再写到nginx-ingress-controller的pod里，这个ingress-controller的pod里运行着一个nginx服务，控制器会把生成的nginx配置写入/etc/nginx.conf文件中 然后reload一下使配置生效。以此达到域名区分配置和动态更新的作用 ingress暴露服务的方式 Deployment+LoadBalancer模式的Service 如果要把ingress部署在公有云，那用这种方式比较合适。用Deployment部署ingress-controller，创建一个type为 LoadBalancer的 service关联这组pod。大部分公有云，都会为 LoadBalancer的 service自动创建一个负载均衡器，通常还绑定了公网地址。只要把域名解析指向该地址，就实现了集群服务的对外暴露\nDaemonSet+HostNetwork+nodeselector 用DaemonSet结合nodeselector来部署ingress-controller到特定的node 上，然后使用Hostiletwork直接把该pod与宿主机node的网络打通，直接使用宿主机的80/433端口就能访问服务。这时，ingress-controller所在的node机器就很类似传统架构的边缘节点，比如机房入口的nginx服务器。该方式整个请求链路最简单，性能相对NodePort模式更好。缺点是由于直接利用宿主机节点的网络和端口，一个node只能部署一个ingress-controller pod。比较适合大并发的生产环境使用\nDeployment+NodePort模式的Service 同样用deployment模式部署ingres-controller，并创建对应的服务，但是type为NodePort。这样，ingress就会暴露在集群节点ip的特定端口上 由于nodeport暴露的端口是随机端口，一般会在前面再搭建一套负载均衡器来转发请求。该方式一般用于宿主机是相对固定的环境ip地址不变的场景 NodePort方式暴露ingress虽然简单方便，但是NodePort多了一层NAT，在请求量级很大时可能对性能会有一定影响 采用方式三：Deployment+NodePort模式的Service 本节以方式三举例说明如何使用ingress\n下载nginx-ingress-controller和ingress-nginx暴露端口配置文件 在主节点 mkdir /opt/ingress-nodeport cd /opt/ingress-nodeport 官方下载地址： wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.30.0/deploy/static/mandatory.yaml wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.30.0/deploy/static/provider/baremetal/service-nodeport.yaml 国内 gitee 资源地址： wget https://gitee.com/mirrors/ingress-nginx/raw/nginx-0.30.0/deploy/static/mandatory.yaml wget https://gitee.com/mirrors/ingress-nginx/raw/nginx-0.30.0/deploy/static/provider/baremetal/service-nodeport.yaml 修改mandatory.yaml镜像拉取地址为（image处）： registry.cn-hangzhou.aliyuncs.com/google_containers/nginx-ingress-controller:0.30.0 启动nginx-ingress-controller kubectl apply -f mandatory.yaml kubectl apply -f service-nodeport.yaml # 查看ingress-nginx [root@k8s-master01 ingress-controller]# kubectl get pod -n ingress-nginx NAME READY STATUS RESTARTS AGE pod/nginx-ingress-controller-fbf967dd5-4qpbp 1/1 Running 0 12h # 查看service [root@k8s-master01 ingress-controller]# kubectl get svc -n ingress-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-nginx NodePort 10.98.75.163 \u0026lt;none\u0026gt; 80:32240/TCP,443:31335/TCP 11h 准备service和pod 创建tomcat-nginx.yaml\napiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment namespace: dev spec: replicas: 3 selector: matchLabels: app: nginx-pod template: metadata: labels: app: nginx-pod spec: containers: - name: nginx image: nginx ports: - containerPort: 80 --- apiVersion: apps/v1 kind: Deployment metadata: name: tomcat-deployment namespace: dev spec: replicas: 3 selector: matchLabels: app: tomcat-pod template: metadata: labels: app: tomcat-pod spec: containers: - name: tomcat image: tomcat:8.5-jre10-slim ports: - containerPort: 8080 --- apiVersion: v1 kind: Service metadata: name: nginx-service namespace: dev spec: selector: app: nginx-pod type: ClusterIP ports: - port: 80 targetPort: 80 protocol: TCP --- apiVersion: v1 kind: Service metadata: name: tomcat-service namespace: dev spec: selector: app: tomcat-pod type: ClusterIP ports: - port: 8080 targetPort: 8080 protocol: TCP # 创建 [root@k8s-master01 ~]# kubectl create -f tomcat-nginx.yaml # 查看 [root@k8s-master01 ~]# kubectl get svc -n dev NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx-service ClusterIP None \u0026lt;none\u0026gt; 80/TCP 48s tomcat-service ClusterIP None \u0026lt;none\u0026gt; 8080/TCP 48s Http代理 创建ingress-http.yaml\napiVersion: extensions/v1beta1 kind: Ingress metadata: name: ingress-http namespace: dev spec: rules: - host: nginx.k8s.com http: paths: - path: / backend: serviceName: nginx-service servicePort: 80 - host: tomcat.k8s.com http: paths: - path: / backend: serviceName: tomcat-service servicePort: 8080 # 创建 [root@k8s-master01 ~]# kubectl create -f ingress-http.yaml ingress.extensions/ingress-http created # 查看 [root@k8s-master01 ~]# kubectl get ing ingress-http -n dev NAME HOSTS ADDRESS PORTS AGE ingress-http nginx.k8s.com,tomcat.k8s.com 80 22s # 查看详情 [root@k8s-master01 ~]# kubectl describe ing ingress-http -n dev ... Rules: Host Path Backends ---- ---- -------- nginx.k8s.com / nginx-service:80 (10.244.1.96:80,10.244.1.97:80,10.244.2.112:80) tomcat.k8s.com / tomcat-service:8080(10.244.1.94:8080,10.244.1.95:8080,10.244.2.111:8080) ... # 接下来,在电脑上配置host文件,解析上面的两个域名到(master)上 # 然后,就可以分别访问tomcat.k8s.com:32240 和 nginx.k8s.com:32240 查看效果了 Reference k8s——ingress https://gitee.com/yooome/golang/blob/main/k8s%E8%AF%A6%E7%BB%86%E6%95%99%E7%A8%8B-%E8%B0%83%E6%95%B4%E7%89%88/k8s%E8%AF%A6%E7%BB%86%E6%95%99%E7%A8%8B.md#74-ingress%E4%BB%8B%E7%BB%8D k8s 对外服务之ingress\n","description":"","id":17,"section":"posts","tags":["k8s","ingress"],"title":"k8s ingress","uri":"https://starifly.github.io/posts/k8s-ingress/"},{"content":"\r07 分布式存储ceph运维操作\n","description":"","id":18,"section":"posts","tags":["ceph"],"title":"Ceph运维操作","uri":"https://starifly.github.io/posts/ceph-opetations/"},{"content":"\r01 分布式存储Ceph理论篇\n","description":"","id":19,"section":"posts","tags":["ceph"],"title":"Ceph理论篇","uri":"https://starifly.github.io/posts/ceph-theory/"},{"content":"ceps s3 java开发相关：\nAWS s3 java api使用 - https://blog.csdn.net/smallhujiu/article/details/87204090\nhttps://hub.fastgit.org/aws-samples/aws-java-sample\n使用 AWS SDK for Java 的 Amazon S3示例 - https://docs.aws.amazon.com/zh_cn/sdk-for-java/v1/developer-guide/examples-s3.html\nhttps://hub.fastgit.org/awsdocs/aws-doc-sdk-examples/tree/master/java/example_code/s3\nhttps://hub.fastgit.org/aws/aws-sdk-java\n适用于 Java 的 AWS 开发工具包 - https://aws.amazon.com/cn/sdk-for-java/\nhttps://docs.ceph.com/en/latest/radosgw/s3/java/\nUploading and copying objects using multipart upload - https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html\n另外可以使用 https://github.com/minio/minio-java 开发\n","description":"本文介绍了ceph对象存储使用java开发的方法和工具","id":20,"section":"posts","tags":["ceph","rgw","s3","java"],"title":"ceph s3 java开发相关","uri":"https://starifly.github.io/posts/ceph-s3-java-related/"},{"content":"\rCeph使用系列之——Ceph RGW使用\nCeph-对象存储：S3使用手册实践 中的文件夹部分\n06 分布式存储ceph创建rgw接口\n","description":"","id":21,"section":"posts","tags":["ceph","rgw"],"title":"Ceph RGW使用","uri":"https://starifly.github.io/posts/ceph-rgw-use/"},{"content":"\rCeph总结\n","description":"","id":22,"section":"posts","tags":["ceph"],"title":"Ceph总结","uri":"https://starifly.github.io/posts/ceph-summary/"},{"content":"\rCeph运维告诉你分布式存储的那些“坑”\n以Ceph、VSAN为代表的软件定义存储（Software Defined Storage，SDS）是一个横向扩展、自动均衡、自愈合的分布式存储系统，将商用x86服务器、固态硬盘、机械硬盘等硬件资源整合为一个瘦供给的资源池，并以块存储、文件存储、对象存储、Restful API等多种接口方式提供存储服务。\n无论是Ceph、VSAN，或者其演化版本，都有一个共同的技术特征，即采用网络RAID方式实现数据保护，以3副本或纠删码为代表。其中3副本用于对小块数据读写性能有一定要求的应用场景，而纠删码则适用于视频数据、备份及归档等大文件场景。\n以3副本为例，业务数据被分割为固定大小的数据块，通常为4MB，每个数据块在不同的节点上保存3个副本（如图1所示），其分布机制是依照一致性哈希算法（Consistent Hashing）或CRUSH算法，将各个副本数据随机分布在不同节点、不同磁盘中，以实现数据自动平衡和横向扩展。当磁盘或节点遭遇故障或损坏时，系统会自动根据预先设定的规则，重新建立一个新的数据副本，称之为数据重建。\n虽然分布式存储的SDS理念很好，横向扩展能力不错，自动添加和删除节点都是优势，但与传统集中式存储（磁盘阵列）相比，其稳定性和性能仍然存在明显的短板。\n首先，在性能方面，三副本分布式存储容易受到IO分布不均匀和木桶效应的影响，导致大延迟和响应迟钝的现象。以Ceph为例，多个存储基本单元，Placement Group （PG），封装为一个OSD，每个OSD直接对应于某一个机械硬盘HDD；主流的7200转HDD，受到机械臂寻址限制，其单盘的读写性能仅为120 IOPS左右；由于数据在OSD上随机分布，因而单个硬盘上的IO负载不会固定在平均值上，而是总体呈现为正态分布，少数HDD上因正态分布的尾部效应，导致其IO负载远超平均值，以及远超单盘的性能阀值，造成拥堵。此外，分布式存储为保证数据完整性，必须定时进行数据完整性校验，即数据scrub/deep-scrub操作，而这些操作产生额外的IO负载，可能会加重磁盘阻塞现象。根据木桶效应原理，系统的性能取决于集群中表现最差的磁盘，因此个别慢盘严重拖累整个系统的性能，其可能的后果，就是带来大延迟、OSD假死，以及触发数据非必需的重建。\n其次，三副本分布式存储还面临稳定性问题。当存储扩容、硬盘或节点损坏、网络故障、OSD假死、 Deep-scrub等多种因素叠加，可能导致多个OSD同时重建，引发重建风暴。在数据重建过程中，重建任务不仅消耗系统的内存、CPU、网络资源，而且还给存储系统带来额外的IO工作负载，挤占用户工作负载的存储资源。在此情形下，用户时常观察到，系统IO延迟大，响应迟钝，轻者引起业务中断，严重时系统可能会陷入不稳定的状态，OSD反复死机重启，甚至会导致数据丢失，系统崩溃。\n此外，三副本分布式存储还面临数据丢失的风险。三副本最大可抵御两个HDD同时损坏。当系统处于扩容状态、或一个节点处于维护模式时，此时出现一个HDD故障，则该系统就会进入紧急状态，出现两个HDD同时故障，则可能导致数据丢失。对于一个具有一定规模的存储系统而言，同时出现两个机械硬盘故障的事件不可避免，尤其是当系统运行两三年之后，随着硬件的老化，出现Double、或Triple磁盘故障的概率急剧上升。此外，当系统出现大规模掉电或存储节点意外宕机时，也可能会导致多个机械硬盘同时出现损坏，危及三副本分布式存储的数据安全。\n","description":"Ceph运维告诉你分布式存储的那些“坑”","id":23,"section":"posts","tags":["ceph"],"title":"Ceph运维告诉你分布式存储的那些“坑”","uri":"https://starifly.github.io/posts/ceph-op-tells-you-the-pits-of-distribute-storage/"},{"content":"环境配置 安装依赖\nyum install pcre-devel.x86_64 openssl-devel zlib-devel systemd-devel gcc gcc-c++\n内核调优\n#vim /etc/sysctl.conf vm.swappiness = 0 net.ipv4.ip_forward = 1 net.ipv4.ip_nonlocal_bind = 1 net.ipv4.tcp_tw_reuse = 1 net.ipv4.ip_local_port_range = 1025 65023 net.ipv4.tcp_max_syn_backlog = 10240 检查\n/usr/sbin/sysctl net.ipv4.ip_nonlocal_bind /usr/sbin/sysctl net.ipv4.ip_forward cat /proc/sys/net/ipv4/ip_forward 查看是否开启了ip转发功能\n如果上述文件中的值为0,说明禁止进行IP转发；如果是1,则说明IP转发功能已经打开。\nhaproxy 下载安装包\nwget https://src.fedoraproject.org/repo/pkgs/haproxy/haproxy-2.0.1.tar.gz/sha512/bcc2c6fa1fe5699f110a2b2ce5abcec7f4ebff94a2850d731f6d86aadeb7f4048b6f372db6724a91191c2ecc2853f5ac576233e0ff84ffef3de6c80d1250f1b6/haproxy-2.0.1.tar.gz\n解压\ntar -zxvf haproxy-2.0.1.tar.gz\n开始编译安装\ncd haproxy-2.0.1/ # 可指定安装目录PREFIX=/usr/local/haproxy，默认为/usr/local/sbin/ make TARGET=linux-glibc USE_OPENSSL=1 USE_SYSTEMD=1 USE_PCRE=1 USE_ZLIB=1 \u0026amp;\u0026amp; make install 注册到系统服务\nvim /usr/lib/systemd/system/haproxy.service\n[Unit] Description=HAProxy Load Balancer After=syslog.target network.target [Service] ExecStartPre=/usr/local/sbin/haproxy -f /etc/haproxy/haproxy.cfg -c -q ExecStart=/usr/local/sbin/haproxy -Ws -f /etc/haproxy/haproxy.cfg -p /run/haproxy.pid ExecReload=/bin/kill -USR2 $MAINPID [Install] WantedBy=multi-user.target 修改配置\nvim /etc/haproxy/haproxy.cfg\nglobal log 127.0.0.1 local0 log 127.0.0.1 local1 notice maxconn 4096 # 注 下面完成了工作进程数及亲缘性绑定，若想做的极致一点还可以通过在内核启动文件即 grub 文件启动参数来实现 内核进程绑定以及进程隔离 nbproc 3 # 配置进程数，不包含 status cpu-map 1 1 # 亲缘性绑定。前一个数字是进程号，后一个数字是CPU内核的号，注意将0号cpu留给内核 cpu-map\t2 2 cpu-map\t3 3 stats bind-process 3 #chroot /usr/share/haproxy #user haproxy #group haproxy daemon defaults mode http log global option httplog option dontlognull option http-server-close option forwardfor except 127.0.0.0/8 option redispatch retries 3 timeout http-request 10s timeout queue 1m timeout connect 10s timeout client 1m timeout server 1m timeout http-keep-alive 10s timeout check 10s frontend http_web bind *:2003 mode tcp #timeout client 1h #log global #option tcplog default_backend rgw #acl is_websocket hdr(Upgrade) -i WebSocket #acl is_websocket hdr_beg(Host) -i ws backend rgw mode tcp #timeout queue 1h #timeout server 1h #timeout connect 1h #log global balance roundrobin #hash-type consistent server ceph001 192.168.5.203:20003 check inter 3s fall 2 rise 2 server ceph002 192.168.5.204:20003 check inter 3s fall 2 rise 2 server ceph003 192.168.5.205:20003 check inter 3s fall 2 rise 2 启动\nsystemctl start haproxy \u0026amp;\u0026amp; systemctl enable haproxy\nkeepalived 下载安装包\nwget --no-check-certificate http://www.keepalived.org/software/keepalived-1.2.22.tar.gz\n解压\ntar -zxvf keepalived-1.2.22.tar.gz\n编译安装\ncd keepalived-1.2.22 ./configure --prefix=/opt/keepalived make \u0026amp;\u0026amp; make install cp /opt/keepalived/etc/rc.d/init.d/keepalived /etc/init.d/ cp /opt/keepalived/etc/sysconfig/keepalived /etc/sysconfig/ mkdir /etc/keepalived cp /opt/keepalived/etc/keepalived/keepalived.conf /etc/keepalived cp /opt/keepalived/sbin/keepalived /usr/sbin/ 修改配置\n全部节点采用backup且priority值一样，为了减少VIP来回切换。\nvim /etc/keepalived/keepalived.conf\n! Configuration File for keepalived global_defs { router_id LVS_DEVEL } vrrp_script check_haproxy { script \u0026#34;killall -0 haproxy\u0026#34; interval 2 weight -2 } vrrp_instance VI_1 { state BACKUP interface ens192 virtual_router_id 51 priority 100 advert_int 1 authentication { auth_type PASS auth_pass 35f18af7190d51c9f7f78f37300a0cbd } virtual_ipaddress { 192.168.5.202 } track_script { check_haproxy } } 启动\n/etc/init.d/keepalived start systemctl enable keepalived Reference 配置Ceph RGW (对象存储网关) 的高可用和负责均衡 HAproxy总结 haproxy2.0-编译安装-centos7.6 l4 l7 代理_Haproxy-4层和7层代理负载实战 调优 Haproxy 代理 Ceph ","description":"","id":24,"section":"posts","tags":["ceph"],"title":"ceph rgw高可用和负载均衡","uri":"https://starifly.github.io/posts/ceph-rgw-ha-and-load-balance/"},{"content":"查看ceph的实时运行状态 ceph -w 查看ceph存储空间 ceph df/ceph df detail 查看集群的详细配置 ceph daemon mon.ceph001 config show | more or ceph --admin-daemon /var/run/ceph/ceph-mon.ceph001.asok config show 查看集群版本信息 ceph versions 查看集群认证信息 ceph auth list 查看gc时间 radosgw-admin gc list --include-all | grep time mon命令 ceph mon stat#查看mon的状态信息 mds命令 ceph mds stat #查看msd状态 osd命令 ceph osd stat #查看osd状态 ceph osd df #查看osd磁盘 ceph osd dump #osd的映射信息 ceph osd perf #查看osd性能和延迟 ceph osd tree#查看osd目录树 rgw命令 radosgw-admin bucket list #查看所有桶 radosgw-admin bucket list --bucket=asdas #查看桶内对象 radosgw-admin bucket stats --bucket=asdas #查看桶信息 radosgw-admin bucket rm --bucket=s3test1 #删除一个桶 radosgw-admin bucket rm --bucket=s3test1 --purge-objects #默认只能删空的bucket，强制删除非空的bucket需要加上“--purge-objects ”选项 radosgw-admin bucket check --bucket=s3test1 #查看桶的索引信息 radosgw-admin object rm --object=1.jpg --bucket=s3test3 #删除桶中的对象 radosgw-admin object unlink --bucket=s3test3 —object=1.jpg #从桶索引里去除对象 radosgw-admin bucket limit check [--uid test] #查看Sharding Stat radosgw-admin bucket stats --bucket=bucketname|grep -w \u0026#39;num_objects\u0026#39; #查询桶的对象个数 创建realm： realm里所有的元數據名稱都是全局唯一的，無法創建同名的用戶（uid）和bucket， container； radosgw-admin realm create --rgw-realm=Giant --default 查看存在的realm： radosgw-admin realm list 查看bucket 的信息 radosgw-admin metadata get bucket:bucketname 获取bucket id 并根据 bucket id 从bucket.instace中获取真实shards，可以看到num_shards的值 radosgw-admin metadata get bucket.instance:api_image_test:4fb0079d-88d8-404e-9e3f-1375ba40a870.324279.2 查看index 下保留的shard数量（包含历史的分片） rados ls -p default.rgw.buckets.index 查看bucket : bucketname 当前的shard分片,首先找到 当前bucket id 找到bucket id后查询具体的shard编号，如下 rados ls -p default.rgw.buckets.index | grep \u0026#34;555c6dcf-7320-4b34-8922-5e005dedc130.164465.1\u0026#34; 统计当前使用的shard数 rados ls -p default.rgw.buckets.index | grep \u0026#34;555c6dcf-7320-4b34-8922-5e005dedc130.164465.1\u0026#34; | wc -l 查看当前shard 存在的index数据 rados -p default.rgw.buckets.index listomapkeys .dir.555c6dcf-7320-4b34-8922-5e005dedc130.164465.1.769 ","description":"","id":25,"section":"posts","tags":["ceph"],"title":"ceph 操作命令","uri":"https://starifly.github.io/posts/ceph-commands/"},{"content":"操作步骤 1. Monitor操作，将OSD的状态设置Out\n2. 登录对应OSD节点 ，停止OSD守护进程\n3. Crushmap中删除OSD\n4. 删除OSD的认证信息\n5. 登录对应OSD节点 ，取消挂载文件系统\n注意事项 停止OSD和删除Crushmao的信息，都将导致PG的重新收敛，所以最好每一个步骤都确保当前ceph状态是OK才进行下一步。\n######参考脚本\n#!/bin/sh ####judge state of ceph judge() { while true do sleep 5 ceph -s |grep HEALTH_OK if [[ $? -eq 0 ]] then break else sleep 60 fi done } ####rdel osd from host14 while true do for host in `seq 31 39` do echo \u0026#34;now.... ,del osd.$host from host14\u0026#34; \u0026gt;\u0026gt;stat.txt ceph osd out $host judge ssh 192.168.220.14 \u0026#34;/etc/init.d/ceph stop osd.$host\u0026#34; judge ceph osd crush remove osd.$host judge ceph auth del osd.$host judge ceph osd rm osd.$host judge ssh 192.168.220.14 \u0026#34;umount /var/lib/ceph/osd/ceph-$host\u0026#34; judge echo \u0026#34;del osd.$host from host14 suceess!!!!\u0026#34; \u0026gt;\u0026gt;stat.txt done break done ######del osd from host16 while true do for host in `seq 40 45` do echo \u0026#34;now.... ,del osd.$host from host16\u0026#34; \u0026gt;\u0026gt;stat.txt ceph osd out $host judge ssh 192.168.220.16 \u0026#34;/etc/init.d/ceph stop osd.$host\u0026#34; judge ceph osd crush remove osd.$host judge ceph auth del osd.$host judge ceph osd rm osd.$host judge ssh 192.168.220.16 \u0026#34;umount /var/lib/ceph/osd/ceph-$host\u0026#34; judge echo \u0026#34;del osd.$host from host16 suceess!!!!\u0026#34; \u0026gt;\u0026gt;stat.txt done break done Reference Ceph剔除主机，Crushmap遗留脏数据 ","description":"ceph剔除主机操作步骤","id":26,"section":"posts","tags":["ceph"],"title":"ceph剔除主机脚本","uri":"https://starifly.github.io/posts/ceph-remove-hosts-script/"},{"content":"注意：此文档用于 Ceph Nautilus 版本（包括社区版 Ceph 14.2.x 和红帽版 Redhat Ceph Storage 4.x）内的小版本升级，不能用于 Ceph 大版本升级（例如从 Ceph Luminous 升级到 Ceph Nautilus）。\nCeph 集群简介 Ceph Nautilus 集群（docker方式部署）包括的角色如下：\nmon：monitor 节点，用于集群选主。节点数量：3。 mgr：manager 节点，用于集群管理，包括监控，告警等。节点数量：3。 osd：存储节点，用于存放 ceph 集群所有数据。节点数量：3。 mds：cephfs 元数据节点，用于管理 cephfs 文件系统元数据。节点数量：3。 rgw：对象存储网关，用于对外提供 AWS S3 API 接口。节点数量：3。 升级步骤 根据官网升级文档，总结升级顺序如下：\n升级 mon 节点 ——\u0026gt; 升级 mgr 节点 ——\u0026gt; 升级 osd 节点 ——\u0026gt; 升级 mds 节点 ——\u0026gt; 升级 rgw 节点\n升级过程中每执行一步即时用 ceph -s 和 ceph versions 查看集群状态，确认状态到达期望值再进行下一步。\n升级前准备 1.设置集群 osd 状态为 noout，nodeep-scrub\nceph osd set noout ceph osd set nodeep-scrub 升级 mon 节点 203节点：\ndocker rm -f mon docker run -d --net=host --name=mon --restart=always -v /etc/localtime:/etc/localtime:ro -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -v /var/log/ceph:/var/log/ceph -e MON_IP=192.168.5.203 -e CEPH_PUBLIC_NETWORK=192.168.5.0/24 ceph/daemon:v4.0.22-stable-4.0-nautilus-centos-7-x86_64 mon 204节点：\ndocker rm -f mon docker run -d --net=host --name=mon --restart=always -v /etc/localtime:/etc/localtime:ro -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -v /var/log/ceph:/var/log/ceph -e MON_IP=192.168.5.204 -e CEPH_PUBLIC_NETWORK=192.168.5.0/24 ceph/daemon:v4.0.22-stable-4.0-nautilus-centos-7-x86_64 mon 205节点：\ndocker rm -f mon docker run -d --net=host --name=mon --restart=always -v /etc/localtime:/etc/localtime:ro -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -v /var/log/ceph:/var/log/ceph -e MON_IP=192.168.5.205 -e CEPH_PUBLIC_NETWORK=192.168.5.0/24 ceph/daemon:v4.0.22-stable-4.0-nautilus-centos-7-x86_64 mon 升级 mgr 节点 所有节点执行：\ndocker rm -f mgr docker run -d --net=host --name=mgr --restart=always -v /etc/localtime:/etc/localtime:ro -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -v /var/log/ceph:/var/log/ceph ceph/daemon:v4.0.22-stable-4.0-nautilus-centos-7-x86_64 mgr 确认 mgr 功能正常\nceph -s ... services: mon: 3 daemons, quorum ceph001,ceph002,ceph003 (age 5s) mgr: ceph001(active, since 89s) ... 升级 osd 节点 203节点：\ndocker rm -f osd docker run -d --name=osd --net=host --restart=always --privileged=true --pid=host -v /etc/localtime:/etc/localtime:ro -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -v /var/log/ceph:/var/log/ceph -v /dev/:/dev/ -v /run/udev/:/run/udev/ -e OSD_ID=0 ceph/daemon:v4.0.22-stable-4.0-nautilus-centos-7-x86_64 osd_ceph_volume_activate 204节点：\ndocker rm -f osd docker run -d --name=osd --net=host --restart=always --privileged=true --pid=host -v /etc/localtime:/etc/localtime:ro -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -v /var/log/ceph:/var/log/ceph -v /dev/:/dev/ -v /run/udev/:/run/udev/ -e OSD_ID=1 ceph/daemon:v4.0.22-stable-4.0-nautilus-centos-7-x86_64 osd_ceph_volume_activate 205节点：\ndocker rm -f osd docker run -d --name=osd --net=host --restart=always --privileged=true --pid=host -v /etc/localtime:/etc/localtime:ro -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -v /var/log/ceph:/var/log/ceph -v /dev/:/dev/ -v /run/udev/:/run/udev/ -e OSD_ID=2 ceph/daemon:v4.0.22-stable-4.0-nautilus-centos-7-x86_64 osd_ceph_volume_activate 可通过 ceph versions 观察升级进度\n升级 mds 节点 将 cephfs 的集群节点数量设置成 1\nceph status ceph fs set \u0026lt;fs_name\u0026gt; max_mds 1 等待 cephfs 集群只剩一个节点为 active\nceph status 将所有 standby 的 MDS 容器删掉\n204、205节点：\ndocker rm -f mds 确保只有一个 MDS 服务在线，并且为 rank 0\nceph status 升级 active（203节点） 节点的 mds 容器\ndocker rm -f mds docker run -d --net=host --name=mds --restart=always -v /etc/localtime:/etc/localtime:ro -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -v /var/log/ceph:/var/log/ceph -e CEPHFS_CREATE=1 -e MDS_NAME=ceoh001 ceph/daemon:v4.0.22-stable-4.0-nautilus-centos-7-x86_64 mds 启动升级 standby 节点的 mds 容器\n#204节点： docker run -d --net=host --name=mds --restart=always -v /etc/localtime:/etc/localtime:ro -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -v /var/log/ceph:/var/log/ceph -e CEPHFS_CREATE=1 -e MDS_NAME=ceoh002 ceph/daemon:v4.0.22-stable-4.0-nautilus-centos-7-x86_64 mds #205节点： docker run -d --net=host --name=mds --restart=always -v /etc/localtime:/etc/localtime:ro -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -v /var/log/ceph:/var/log/ceph -e CEPHFS_CREATE=1 -e MDS_NAME=ceoh003 ceph/daemon:v4.0.22-stable-4.0-nautilus-centos-7-x86_64 mds 将 cephfs 集群的 max_mds 数量还原\nceph fs set \u0026lt;fs_name\u0026gt; max_mds \u0026lt;original_max_mds\u0026gt; 升级 rgw 节点 所有节点执行：\ndocker rm -f rgw docker run -d --net=host --name=rgw --restart=always -v /etc/localtime:/etc/localtime:ro -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -v /var/log/ceph:/var/log/ceph ceph/daemon:v4.0.22-stable-4.0-nautilus-centos-7-x86_64 rgw 升级后操作 ceph osd unset noout ceph osd unset nodeep-scrub https://docs.ceph.com/en/latest/releases/nautilus/#upgrading-from-mimic-or-luminous\nCeph Nautilus 升级方案 - https://blog.csdn.net/zzboat0422/article/details/112787626?spm=1001.2014.3001.5501\n","description":"","id":27,"section":"posts","tags":["ceph"],"title":"ceph从14.2.20升级到14.2.22","uri":"https://starifly.github.io/posts/ceph-upgrade-from-14.2.20-to-14.2/"},{"content":"概念： 1、zone：可用区，有一个或多个对象网关实例组成。zone不可以跨集群，配置zone不同于其他典型配置，因为不需要在ceph.conf中配置。\n2、zonegroup：以前叫做“region”，有多个zone组成,一个zonegroup里面有一个master zone，在同一个zonegroup中的多个zone可以同步元数据和数据，提供灾难恢复能力。\n3、realm：代表一个唯一的命名空间，有一个或多个zonegroup组成。在同一个realm中的不同zonegroup只能同步元数据。在realm中有period的概念，表示zonegroup的配置状态，修改zonegroup，必须更新period。\n创建realm\nradosgw-admin realm create --rgw-realm=my-realm --default [root@ceph01 ~]# radosgw-admin realm list { \u0026#34;default_info\u0026#34;: \u0026#34;eff9b039-8c3c-4991-87f9-e9331b2c7824\u0026#34;, \u0026#34;realms\u0026#34;: [ \u0026#34;my-realm\u0026#34; ] } 创建master zonegroup,先删除默认的zonegroup\nradosgw-admin zonegroup delete --rgw-zonegroup=default\n创建一个为azonggroup的zonegroup\nradosgw-admin zonegroup create --rgw-zonegroup=azonegroup --endpoints=192.168.0.39:7480 --master --default\n创建master zone,先删除默认的zone\nadosgw-admin zone delete --rgw-zone=default\n创建一个为azone的zone\nradosgw-admin zone create --rgw-zonegroup=azonegroup --rgw-zone=azone --endpoints=192.168.0.39:7480 --default --master\n创建一个auser账户用于和其它 zone同步\nradosgw-admin user create --uid=\u0026quot;auser\u0026quot; --display-name=\u0026quot;auser\u0026quot; --system\n用创建system账户产生的access 和secret更新zone配置\nradosgw-admin zone modify --rgw-zone=azone --access-key=BKG10IM15N8EB0I7ZE7U --secret=Cvh60vBX5ciujqRaLw3bm6wMIGmLdlJ9FB4ukOG\n更新period\nradosgw-admin period update \u0026ndash;commit\n重启rgw实例\nReference ceph 多区域radosgw网关配置 动态设置bucket_index_max_shards参数 ","description":"本文介绍了在ceph中利用rgw创建新的region","id":28,"section":"posts","tags":["ceph","rgw","region"],"title":"ceph rgw创建新的region","uri":"https://starifly.github.io/posts/ceph-rgw-create-a-new-region/"},{"content":"背景说明 RGW 为每个 bucket 维护了一份索引，里面保存了 bucket 中全部对象的元数据。RGW 本身并没有足够有效的遍历对象的能力，所以在处理请求时，这些索引数据非常重要，比如遍历 bucket 中全部对象时。bucket 索引信息还有其他用处，比如为版本控制的对象维护日志、bucket 配额元数据和跨区同步的日志。bucket 索引不会影响对象的读操作，但写和修改确实会增加一些额外的操作。\n这隐含了两层意思：其一，在单个 bucket 索引对象上能存储的数据总量有限，默认情况下，每个 bucket 是只有一个索引对象的，所以每个 bucket 中能存储的对象数量就是有限的了。超大的索引对象会造成性能和可靠性的问题，极端情况下，可能因为缓慢的恢复操作，造成 OSD 进程挂掉。其二，这成了性能瓶颈，因为所有对同一 bucket 的写操作，都会对一个索引对象做修改和序列化操作。\n为此 ceph 引入bucket 分片功能来解决 bucket 中存储大量数据的问题，bucket 的索引数据可以存储在多个 RADOS 对象上了，这样 bucket 中存储对象的数量就可以随着索引数据的分片数量的增加而增加了。而且从 Luminous 开始最终引入了动态 bucket 分片，现在随着存储对象的增加，bucket 可以自动分片了。这项功能是默认打开的，将变量 rgw dynamic resharding 设置为 false（默认为 true），关闭自动分片；每个分片可存储的对象数量由该变量控制，rgw max objs per shard，默认是十万；自动分片线程扫描的间隔可以通过 rgw reshard thread interval 选项配置，默认为十分钟。虽然官方宣称自动分片是自动化的且不会阻塞业务的读写，但是实际上自动分片存在bug，会造成IO阻塞，所以建议最好关闭此项功能。\n解决方案 1.首先index pool建议上SSD，即rgw中的pool除data池外，其它都采用ssd。\n2.关闭动态resharding，不管怎么样不能影响到生产服务质量\nrgw dynamic resharding = false 3.预估单个bucket需要存放的对象数量，按照每个shard 十万数据，提前做好分片，这有个好处是，后期不需要再进行resharding。设置shard的两个方法\n方法1：rgw_override_bucket_index_max_shards rgw_override_bucket_index_max_shards ：\r配置在配置文件中，指定新建的bucket的shard数量，配置了这个参数，需要重启才能让参数生效。 方法2：multisite 中的 bucket_index_max_shards 这个参数需要配置在multisite zonegroup中的配置中,会覆盖rgw_override_bucket_index_max_shards 参数 获取zonegroup\n# radosgw-admin zonegroup --rgw-zonegroup=default get \u0026gt; /root/zonegroup.json { \u0026#34;id\u0026#34;: \u0026#34;3c5cc9c6-7e31-46cd-ac6d-652ce466c879\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;api_name\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;is_master\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;endpoints\u0026#34;: [], \u0026#34;hostnames\u0026#34;: [], \u0026#34;hostnames_s3website\u0026#34;: [], \u0026#34;master_zone\u0026#34;: \u0026#34;b70f0b56-08a0-472a-880f-6114f593c972\u0026#34;, \u0026#34;zones\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;b70f0b56-08a0-472a-880f-6114f593c972\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;endpoints\u0026#34;: [], \u0026#34;log_meta\u0026#34;: \u0026#34;false\u0026#34;, \u0026#34;log_data\u0026#34;: \u0026#34;false\u0026#34;, \u0026#34;bucket_index_max_shards\u0026#34;: 0, // 这个参数 \u0026#34;read_only\u0026#34;: \u0026#34;false\u0026#34;, \u0026#34;tier_type\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;sync_from_all\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;sync_from\u0026#34;: [] } ], \u0026#34;placement_targets\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;default-placement\u0026#34;, \u0026#34;tags\u0026#34;: [] } ], \u0026#34;default_placement\u0026#34;: \u0026#34;default-placement\u0026#34;, \u0026#34;realm_id\u0026#34;: \u0026#34;f43494f8-bfb6-4723-b169-6ba929cdca1c\u0026#34; } 修改 bucket_index_max_shards 参数\n重新导入zonegroup\n导入前先确认默认zonegroup有没有realm，如果没有realm，要先创建并修改zonegroup的realm\nradosgw-admin realm create --rgw-realm=my-realm --default radosgw-admin zonegroup modify --rgw-zonegroup=default --rgw-realm=my-realm # radosgw-admin zonegroup --rgw-zonegroup=default set \u0026lt; /root/zonegroup.json { \u0026#34;id\u0026#34;: \u0026#34;3c5cc9c6-7e31-46cd-ac6d-652ce466c879\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;api_name\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;is_master\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;endpoints\u0026#34;: [], \u0026#34;hostnames\u0026#34;: [], \u0026#34;hostnames_s3website\u0026#34;: [], \u0026#34;master_zone\u0026#34;: \u0026#34;b70f0b56-08a0-472a-880f-6114f593c972\u0026#34;, \u0026#34;zones\u0026#34;: [ { \u0026#34;id\u0026#34;: \u0026#34;b70f0b56-08a0-472a-880f-6114f593c972\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;endpoints\u0026#34;: [], \u0026#34;log_meta\u0026#34;: \u0026#34;false\u0026#34;, \u0026#34;log_data\u0026#34;: \u0026#34;false\u0026#34;, \u0026#34;bucket_index_max_shards\u0026#34;: 4, \u0026#34;read_only\u0026#34;: \u0026#34;false\u0026#34;, \u0026#34;tier_type\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;sync_from_all\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;sync_from\u0026#34;: [] } ], \u0026#34;placement_targets\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;default-placement\u0026#34;, \u0026#34;tags\u0026#34;: [] } ], \u0026#34;default_placement\u0026#34;: \u0026#34;default-placement\u0026#34;, \u0026#34;realm_id\u0026#34;: \u0026#34;f43494f8-bfb6-4723-b169-6ba929cdca1c\u0026#34; 提交配置，让配置生效，这一步不能少\n# radosgw-admin period update --commit 这个参数会对所有在这个zonegroup下面的zone其效果，而且不需要重启\n方法一手动配置了rgw_override_bucket_index_max_shards参数，使得bucket在一定程度上保证不会有索引分片方面的性能问题，那么问题来了，手动设置该值，是让集群的bucket在一开始就分好片，产生的大量分片为即将进行的数据存储做好了准备，但是，线上业务反映，每天写入的文件量远不及我们一开始的配置，在分片数设置为768的情况下，单个bucket可以容纳7千6百80万个文件，而线上的业务进来的文件每天只有百万级别，事先分好的片不但浪费掉了，而且在持续写入之后会有性能不友好的问题。所以最好采用方法二，在创建bucket的时候就可以设置分片值，这样能很大程度减少浪费问题。\n另外腾讯的cos还有阿里的oss对象存储理论上单桶可以存储无限个对象，据说是做了多桶联合，对外展示为一个桶，这个可能需要研发能力支撑，这里不做过多讨论。\nReference Ceph RGW bucket 自动分片介绍和存在的问题 动态设置bucket_index_max_shards参数 Rgw设置shard避免单bucket下数量过大导致性能下降的方法 ","description":"","id":29,"section":"posts","tags":["ceph","rgw","bucket","shard"],"title":"ceph rgw桶分片问题","uri":"https://starifly.github.io/posts/ceph-rgw-bucket-shard-problem/"},{"content":"在Ceph Luminous之前的版本，可以使用第三方的Prometheus exporter ceph_exporter。 Ceph Luminous 12.2.1后的mgr中自带了Prometheus插件，内置了 Prometheus ceph exporter，可以使用Ceph mgr内置的exporter作为Prometheus的target。\n本系统组件（都是部署在 docker 容器中）如下：\nceph-nautilus（14.2.20）版本 grafana的v8.1.3 (a61f38238c)版本 prometheus的v2.29.2版本 prometheus-webhook-dingtalk的v0.3.0版本 alertmanager的0.23.0版本 【启动 prometheus】\nceph mgr module enable prometheus docker run -d --name=prometheus --net=host --restart=always -v /etc/localtime:/etc/localtime:ro prom/prometheus:v2.29.2 # 进入 Prometheus 容器中，vi /etc/prometheus/prometheus.yml - job_name: \u0026#34;ceph\u0026#34; static_configs: - targets: [\u0026#34;localhost:9283\u0026#34;] 【启动 grafana】\ndocker run -d --name=grafana --net=host --restart=always -v /etc/localtime:/etc/localtime:ro grafana/grafana\n添加prometheus数据源，然后再添加dashboard（这里添加id为9550的dashboard）\n【启动钉钉插件】\n先要获取钉钉的 webhook，具体方法可以百度。\ndocker run -d --name=dingtalk --net=host --restart=always timonwong/prometheus-webhook-dingtalk:v0.3.0 --ding.profile=\u0026quot;\u0026lt;web-hook-name\u0026gt;=\u0026lt;dingtalk-webhook\u0026gt;\u0026quot;\n这里解释一下两个变量：\nweb-hook-name ：prometheus-webhook-dingtalk 支持多个钉钉 webhook，不同 webhook 就是靠名字对应到 URL 来做映射的。要支持多个钉钉 webhook，可以用多个 \u0026ndash;ding.profile 参数的方式支持，例如：sudo docker run -d \u0026ndash;restart always -p 8060:8060 timonwong/prometheus-webhook-dingtalk:v0.3.0 \u0026ndash;ding.profile=\u0026ldquo;webhook1=https://oapi.dingtalk.com/robot/send?access_token=token1\u0026rdquo; \u0026ndash;ding.profile=\u0026ldquo;webhook2=https://oapi.dingtalk.com/robot/send?access_token=token2\u0026rdquo;。而名字和 URL 的对应规则如下，ding.profile=\u0026ldquo;webhook1=\u0026hellip;\u0026hellip;\u0026quot;，对应的 API URL 为：http://localhost:8060/dingtalk/webhook1/send dingtalk-webhook：这个就是之前获取的钉钉 webhook。 【启动 alertmanager】\ndocker run -d --name=alertmanager --net=host --restart=always prom/alertmanager\n添加 webhook 告警，进入容器中，vi /etc/alertmanager/alertmanager.yml\nglobal: resolve_timeout: 5m route: group_by: [\u0026#39;alertname\u0026#39;] group_wait: 10s group_interval: 10s repeat_interval: 1h receiver: \u0026#39;web.hook\u0026#39; receivers: - name: \u0026#39;web.hook\u0026#39; webhook_configs: - url: \u0026#39;http://192.168.5.205:8060/dingtalk/webhook1/send\u0026#39; inhibit_rules: - source_match: severity: \u0026#39;critical\u0026#39; target_match: severity: \u0026#39;warning\u0026#39; equal: [\u0026#39;alertname\u0026#39;, \u0026#39;dev\u0026#39;, \u0026#39;instance\u0026#39;] 重启容器， docker restart alertmanager\n配置Prometheus，添加Alertmanager端点，进入 Prometheus 容器中，vi /etc/prometheus/prometheus.yml\nalerting: alertmanagers: - static_configs: - targets: [\u0026#34;192.168.1.10:9093\u0026#34;] 添加告警规则文件\n$ cat /etc/prometheus/prometheus.yml ...... rule_files: - /etc/prometheus/rules/ceph.yml 接下来轮到刚刚提到的告警规则文件了，下面这两个例子，第一个定义了osd状态为down的时候，发出告警消息，第二个定义在 ceph 存储空间使用率大于 80% 的情况下，发出告警消息。\n$ cat /etc/prometheus/rules/ceph.yml ...... groups: - name: ceph-rule rules: - alert: Ceph OSD Down expr: ceph_osd_up == 0 for: 2m labels: product: Ceph测试集群 annotations: Warn: \u0026#34;{{$labels.instance}}: OSD挂掉了\u0026#34; Description: \u0026#34;{{$labels.instance}}:{{ $labels.ceph_daemon }}当前状态为{{ $value }}\u0026#34; - alert: 集群空间使用率 expr: ceph_cluster_total_used_bytes / ceph_cluster_total_bytes * 100 \u0026gt; 80 for: 2m labels: product: Ceph测试集群 annotations: Warn: \u0026#34;{{$labels.instance}}:集群空间不足\u0026#34; Description: \u0026#34;{{$labels.instance}}:当前空间使用率为{{ $value }}\u0026#34; 重启容器， docker restart prometheus\n使用 Prometheus 监控 Ceph 使用prometheus+Grafana监控ceph 集群 https://docs.ceph.com/en/nautilus/mgr/prometheus/#mgr-prometheus https://docs.ceph.com/en/nautilus/mgr/dashboard/#enabling-the-embedding-of-grafana-dashboards ","description":"","id":30,"section":"posts","tags":["ceph","grafana","prometheus","dingding"],"title":"使用grafana+prometheus监控ceph集群并实现钉钉报警","uri":"https://starifly.github.io/posts/grafana-prometheus-monitor-ceph-cluster/"},{"content":"ceph 集群有三个节点，每台节点都用 docker 容器部署了 mon、osd、mgr、rgw、mds 服务，现在假设在其它机器备份了ceph集群的配置和认证信息（/etc/ceph和/var/lib/ceph目录），而当前三个节点全部出现系统故障导致集群瘫痪的情况下，我们怎么恢复 ceph 集群。\n恢复 mon 节点 当三个节点系统恢复正常后，首先我们要把备用配置拷贝至三节点对应目录，然后开始恢复三个 mon 节点。\n主节点：\ndocker run -d --net=host --name=mon --restart=always -v /etc/localtime:/etc/localtime:ro -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -v /var/log/ceph:/var/log/ceph -e MON_IP=192.168.5.203 -e CEPH_PUBLIC_NETWORK=192.168.5.0/24 ceph/daemon:v4.0.20-stable-4.0-nautilus-centos-7-x86_64 mon 204节点：\ndocker run -d --net=host --name=mon --restart=always -v /etc/localtime:/etc/localtime:ro -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -v /var/log/ceph:/var/log/ceph -e MON_IP=192.168.5.204 -e CEPH_PUBLIC_NETWORK=192.168.5.0/24 ceph/daemon:v4.0.20-stable-4.0-nautilus-centos-7-x86_64 mon 205节点：\ndocker run -d --net=host --name=mon --restart=always -v /etc/localtime:/etc/localtime:ro -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -v /var/log/ceph:/var/log/ceph -e MON_IP=192.168.5.205 -e CEPH_PUBLIC_NETWORK=192.168.5.0/24 ceph/daemon:v4.0.20-stable-4.0-nautilus-centos-7-x86_64 mon 恢复 osd 节点 当恢复了 mon 节点后，可以用命令 ceph -s 查询集群状态信息，发现 osd 还在集群中，所以首先要剔除 osd。\n# ceph -s …… services: mon: 3 daemons, quorum ceph001,ceph002 (age 15s), out of quorum: ceph003 mgr: ceph001(active, since 29h), standbys: ceph003 mds: cephfs:1 {0=ceoh001=up:active} 2 up:standby osd: 3 osds: 3 up (since 136y), 3 in (since 34h) rgw: 3 daemons active (ceph001, ceph002, ceph003) …… 剔除 osd\nceph osd crush rm osd.0 \u0026amp;\u0026amp; ceph osd down 0 \u0026amp;\u0026amp; ceph osd rm 0 ceph osd crush rm osd.1 \u0026amp;\u0026amp; ceph osd down 1 \u0026amp;\u0026amp; ceph osd rm 1 ceph osd crush rm osd.2 \u0026amp;\u0026amp; ceph osd down 2 \u0026amp;\u0026amp; ceph osd rm 2 重新部署恢复 osd\n# 只要激活即可 # 203节点 docker run -d --name=osd --net=host --restart=always --privileged=true --pid=host -v /etc/localtime:/etc/localtime:ro -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -v /var/log/ceph:/var/log/ceph -v /dev/:/dev/ -v /run/udev/:/run/udev/ -e OSD_ID=0 ceph/daemon:v4.0.20-stable-4.0-nautilus-centos-7-x86_64 osd_ceph_volume_activate # 204节点 docker run -d --name=osd --net=host --restart=always --privileged=true --pid=host -v /etc/localtime:/etc/localtime:ro -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -v /var/log/ceph:/var/log/ceph -v /dev/:/dev/ -v /run/udev/:/run/udev/ -e OSD_ID=1 ceph/daemon:v4.0.20-stable-4.0-nautilus-centos-7-x86_64 osd_ceph_volume_activate # 205节点 docker run -d --name=osd --net=host --restart=always --privileged=true --pid=host -v /etc/localtime:/etc/localtime:ro -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -v /var/log/ceph:/var/log/ceph -v /dev/:/dev/ -v /run/udev/:/run/udev/ -e OSD_ID=2 ceph/daemon:v4.0.20-stable-4.0-nautilus-centos-7-x86_64 osd_ceph_volume_activate 通过 ceph -s 命令观察集群状态，当 osd 和 pgs 都恢复正常后，即可进行下一步\n[root@ceph001 ~]# ceph -s cluster: id: d00c744a-17f6-4768-95de-1243202557f2 health: HEALTH_OK services: mon: 3 daemons, quorum ceph001,ceph002,ceph003 (age 8m) mgr: ceph001(active, since 3m), standbys: ceph003, ceph002 mds: cephfs:1 {0=ceoh001=up:active(laggy or crashed)} 2 up:standby osd: 3 osds: 3 up (since 4m), 3 in (since 34h) data: pools: 7 pools, 208 pgs objects: 262 objects, 14 MiB usage: 3.1 GiB used, 57 GiB / 60 GiB avail pgs: 208 active+clean 恢复 mgr 节点 三节点执行如下命令恢复：\ndocker run -d --net=host --name=mgr --restart=always -v /etc/localtime:/etc/localtime:ro -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -v /var/log/ceph:/var/log/ceph ceph/daemon:v4.0.20-stable-4.0-nautilus-centos-7-x86_64 mgr 恢复 rgw 节点 三节点执行如下命令恢复：\ndocker run -d --net=host --name=rgw --restart=always -v /etc/localtime:/etc/localtime:ro -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -v /var/log/ceph:/var/log/ceph ceph/daemon:v4.0.20-stable-4.0-nautilus-centos-7-x86_64 rgw 恢复 mds 节点 # 203节点 docker run -d --net=host --name=mds --restart=always -v /etc/localtime:/etc/localtime:ro -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -v /var/log/ceph:/var/log/ceph -e CEPHFS_CREATE=1 -e MDS_NAME=ceoh001 ceph/daemon:v4.0.20-stable-4.0-nautilus-centos-7-x86_64 mds # 204节点 docker run -d --net=host --name=mds --restart=always -v /etc/localtime:/etc/localtime:ro -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -v /var/log/ceph:/var/log/ceph -e CEPHFS_CREATE=1 -e MDS_NAME=ceoh002 ceph/daemon:v4.0.20-stable-4.0-nautilus-centos-7-x86_64 mds # 205节点 docker run -d --net=host --name=mds --restart=always -v /etc/localtime:/etc/localtime:ro -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -v /var/log/ceph:/var/log/ceph -e CEPHFS_CREATE=1 -e MDS_NAME=ceoh003 ceph/daemon:v4.0.20-stable-4.0-nautilus-centos-7-x86_64 mds Reference CEPH集群MON全部挂掉后恢复方法 ","description":"","id":31,"section":"posts","tags":["ceph"],"title":"ceph三节点故障恢复","uri":"https://starifly.github.io/posts/ceph-three-node-failure-recovery/"},{"content":"常见 OSD 故障处理\nReference 常见 OSD 故障处理 ","description":"","id":32,"section":"posts","tags":["ceph","osd"],"title":"ceph常见OSD故障处理","uri":"https://starifly.github.io/posts/ceph-osd-troubleshooting/"},{"content":"删除Ceph OSD节点\nReference 删除Ceph OSD节点 ","description":"","id":33,"section":"posts","tags":["ceph","osd"],"title":"删除osd的正确方式","uri":"https://starifly.github.io/posts/ceph-del-osd-node/"},{"content":"创建 RBD 服务器端操作 创建 pool [root@ceph-node1 ~/mycluster]#ceph osd pool create rbd 64 pool \u0026#39;rbd\u0026#39; created 创建客户端帐号 # 创建客户端用户 [root@ceph-node1 ~/mycluster]#ceph auth get-or-create client.rbd mon \u0026#39;allow r\u0026#39; osd \u0026#39;allow class-read object_prefix rbd_children,allow rwx pool=rbd\u0026#39; # 查看用户及权限 [root@ceph-node1 ~/mycluster]#ceph auth get client.rbd exported keyring for client.rbd [client.rbd] key = AQB6OAhfMN4jFhAAPmO17m5Z5gP5YC11JOJcTA== caps mon = \u0026#34;allow r\u0026#34; caps osd = \u0026#34;allow class-read object_prefix rbd_children,allow rwx pool=rbd\u0026#34; # 导出客户端keyring [root@ceph-node1 ~/mycluster]#ceph auth get client.rbd -o ./ceph.client.rbd.keyring exported keyring for client.rbd pool 启动 RBD [root@ceph-node1 ~/mycluster]#ceph osd pool application enable rbd rbd enabled application \u0026#39;rbd\u0026#39; on pool \u0026#39;rbd\u0026#39; 客户端操作 安装 ceph-common\n[root@ceph-client ~]#yum install ceph-common -y\n从服务端拷贝 ceph.conf 和 认证 keyring\n[root@ceph-node1 ~/mycluster]#scp ceph.conf ceph.client.rbd.keyring ceph-client:/etc/ceph/ [root@ceph-client ~]#ls /etc/ceph/ ceph.client.rbd.keyring ceph.conf rbdmap # 使用 创建的用户 rbd 查看集群状态 [root@ceph-client ~]#ceph -s --user rbd cluster: id: 45757634-b5ec-4172-957d-80c5c9f76d52 health: HEALTH_OK services: mon: 3 daemons, quorum ceph-node1,ceph-node2,ceph-node3 (age 65m) mgr: no daemons active osd: 0 osds: 0 up, 0 in data: pools: 0 pools, 0 pgs objects: 0 objects, 0 B usage: 0 B used, 0 B / 0 B avail pgs: 创建 image # 创建 image [root@ceph-client ~]#rbd create rbd1 -p rbd --size 1G --user rbd [root@ceph-client ~]#rbd create rbd/rbd2 --size 2G --user rbd # 查看创建的 image [root@ceph-client ~]#rbd ls -l --user rbd NAME SIZE PARENT FMT PROT LOCK rbd1 1 GiB 2 rbd2 2 GiB 2 # 通过json格式查看 [root@ceph-client ~]#rbd ls -p rbd -l --format json --user rbd --pretty-format [ { \u0026#34;image\u0026#34;: \u0026#34;rbd1\u0026#34;, \u0026#34;size\u0026#34;: 1073741824, \u0026#34;format\u0026#34;: 2 }, { \u0026#34;image\u0026#34;: \u0026#34;rbd2\u0026#34;, \u0026#34;size\u0026#34;: 2147483648, \u0026#34;format\u0026#34;: 2 } ] # 显示 image 的详细信息 [root@ceph-client ~]#rbd info rbd1 --user rbd rbd image \u0026#39;rbd1\u0026#39;: size 1 GiB in 256 objects order 22 (4 MiB objects) snapshot_count: 0 id: 112fe2290ad6 block_name_prefix: rbd_data.112fe2290ad6 format: 2 features: layering, exclusive-lock, object-map, fast-diff, deep-flatten op_features: flags: create_timestamp: Sat Jul 11 09:14:18 2020 access_timestamp: Sat Jul 11 09:14:18 2020 modify_timestamp: Sat Jul 11 09:14:18 2020 禁止 image 的特性 默认 image 的特性包括：\nfeatures: layering, exclusive-lock, object-map, fast-diff, deep-flatten\n作为 rbd 一般只需要 layering ，需要把其他的特性全部禁止掉。\n# 禁止 image 特性 [root@ceph-client ~]#rbd feature disable rbd/rbd1 exclusive-lock, object-map, fast-diff, deep-flatten --user rbd [root@ceph-client ~]#rbd feature disable rbd/rbd2 exclusive-lock, object-map, fast-diff, deep-flatten --user rbd # 查看详细信息 [root@ceph-client ~]#rbd info rbd/rbd1 --user rbd rbd image \u0026#39;rbd1\u0026#39;: size 1 GiB in 256 objects order 22 (4 MiB objects) snapshot_count: 0 id: 112fe2290ad6 block_name_prefix: rbd_data.112fe2290ad6 format: 2 features: layering op_features: flags: create_timestamp: Sat Jul 11 09:14:18 2020 access_timestamp: Sat Jul 11 09:14:18 2020 modify_timestamp: Sat Jul 11 09:14:18 2020 [root@ceph-client ~]#rbd info rbd/rbd2 --user rbd rbd image \u0026#39;rbd2\u0026#39;: size 2 GiB in 512 objects order 22 (4 MiB objects) snapshot_count: 0 id: 11342244e27f block_name_prefix: rbd_data.11342244e27f format: 2 features: layering op_features: flags: create_timestamp: Sat Jul 11 09:14:47 2020 access_timestamp: Sat Jul 11 09:14:47 2020 modify_timestamp: Sat Jul 11 09:14:47 2020 另外也可以通过配置或者创建 image 时禁止相关特性。\n修改ceph.conf文件，不用重启Monitor或者OSD，增加 rbd_default_features = 3 。\n创建 image 时禁止：\nrbd create rbd/rbd2 --size 2G --user rbd --image-format 2 --image-feature layering\n客户端挂载 Image [root@ceph-client ~]#lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sr0 11:0 1 4.4G 0 rom /mnt/centos7 vda 252:0 0 100G 0 disk ├─vda1 252:1 0 1G 0 part /boot └─vda2 252:2 0 99G 0 part ├─centos-root 253:0 0 50G 0 lvm / ├─centos-swap 253:1 0 7.9G 0 lvm [SWAP] └─centos-home 253:2 0 41.1G 0 lvm /home [root@ceph-client ~]#rbd ls -l --user rbd NAME SIZE PARENT FMT PROT LOCK rbd1 1 GiB 2 rbd2 2 GiB 2 # RBD 映射到客户端主机 [root@ceph-client ~]#rbd map rbd/rbd1 --user rbd /dev/rbd0 [root@ceph-client ~]#lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sr0 11:0 1 4.4G 0 rom /mnt/centos7 vda 252:0 0 100G 0 disk ├─vda1 252:1 0 1G 0 part /boot └─vda2 252:2 0 99G 0 part ├─centos-root 253:0 0 50G 0 lvm / ├─centos-swap 253:1 0 7.9G 0 lvm [SWAP] └─centos-home 253:2 0 41.1G 0 lvm /home rbd0 251:0 0 1G 0 disk 初始化文件系统\n# 格式化磁盘 [root@ceph-client ~]#mkfs.xfs /dev/rbd0 meta-data=/dev/rbd0 isize=512 agcount=8, agsize=32768 blks = sectsz=512 attr=2, projid32bit=1 = crc=1 finobt=0, sparse=0 data = bsize=4096 blocks=262144, imaxpct=25 = sunit=1024 swidth=1024 blks naming =version 2 bsize=4096 ascii-ci=0 ftype=1 log =internal log bsize=4096 blocks=2560, version=2 = sectsz=512 sunit=8 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 [root@ceph-client ~]#mkdir -pv /mnt/ceph-disk1 mkdir: created directory ‘/mnt/ceph-disk1’ # 挂载文件系统 [root@ceph-client ~]#mount /dev/rbd0 /mnt/ceph-disk1/ [root@ceph-client ~]#df -Th Filesystem Type Size Used Avail Use% Mounted on devtmpfs devtmpfs 3.9G 0 3.9G 0% /dev tmpfs tmpfs 3.9G 0 3.9G 0% /dev/shm tmpfs tmpfs 3.9G 8.6M 3.9G 1% /run tmpfs tmpfs 3.9G 0 3.9G 0% /sys/fs/cgroup /dev/mapper/centos-root xfs 50G 1.9G 49G 4% / /dev/vda1 xfs 1014M 149M 866M 15% /boot /dev/mapper/centos-home xfs 42G 33M 42G 1% /home /dev/sr0 iso9660 4.4G 4.4G 0 100% /mnt/centos7 tmpfs tmpfs 783M 0 783M 0% /run/user/0 /dev/rbd0 xfs 1014M 33M 982M 4% /mnt/ceph-disk1 客户端卸载磁盘 [root@ceph-client ~]#umount /dev/rbd0 # 查看本地 image 映射 [root@ceph-client ~]#rbd showmapped --user rbd id pool namespace image snap device 0 rbd rbd1 - /dev/rbd0 # 卸载 image [root@ceph-client ~]#rbd unmap rbd/rbd1 --user rbd [root@ceph-client ~]#lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sr0 11:0 1 4.4G 0 rom /mnt/centos7 vda 252:0 0 100G 0 disk ├─vda1 252:1 0 1G 0 part /boot └─vda2 252:2 0 99G 0 part ├─centos-root 253:0 0 50G 0 lvm / ├─centos-swap 253:1 0 7.9G 0 lvm [SWAP] └─centos-home 253:2 0 41.1G 0 lvm /home 扩展 image 大小 [root@ceph-client ~]#rbd resize -s 5G rbd/rbd1 --user rbd Resizing image: 100% complete...done. [root@ceph-client ~]#rbd ls -l --user rbd NAME SIZE PARENT FMT PROT LOCK rbd1 5 GiB 2 rbd2 2 GiB 2 删除 image [root@ceph-client ~]#rbd ls -l --user rbd NAME SIZE PARENT FMT PROT LOCK rbd1 5 GiB 2 rbd2 2 GiB 2 # 删除 rbd2 [root@ceph-client ~]#rbd rm rbd2 --user rbd Removing image: 100% complete...done. [root@ceph-client ~]#rbd ls -l --user rbd NAME SIZE PARENT FMT PROT LOCK rbd1 5 GiB 2 image 放进回收站 [root@ceph-client ~]#rbd ls -l --user rbd NAME SIZE PARENT FMT PROT LOCK rbd1 5 GiB 2 # 将 rbd1 放进回收站 [root@ceph-client ~]#rbd trash move rbd/rbd1 --user rbd [root@ceph-client ~]#rbd ls -l --user rbd # 查看回收站 [root@ceph-client ~]#rbd trash list -p rbd --user rbd 112fe2290ad6 rbd1 回收站恢复 image [root@ceph-client ~]#rbd trash list -p rbd --user rbd 112fe2290ad6 rbd1 # 恢复 rbd1 [root@ceph-client ~]#rbd trash restore -p rbd --image rbd1 --image-id 112fe2290ad6 --user rbd [root@ceph-client ~]#rbd ls -l --user rbd NAME SIZE PARENT FMT PROT LOCK rbd1 5 GiB 2 Reference Ceph - RBD操作 使用Docker快速部署Ceph集群 ceph的rbd使用和理解（全） ","description":"","id":34,"section":"posts","tags":["ceph","rbd"],"title":"Ceph rbd简单使用","uri":"https://starifly.github.io/posts/ceph-rbd/"},{"content":"Reference 附录2、ceph安装配置介绍与主机优化 [ ceph ] CEPH 部署完整版（CentOS 7 + luminous） 03 分布式存储ceph之crush规则配置 动态设置bucket_index_max_shards参数 实弹军演-基于Ceph对象存储的实战兵法 从传统运维到云运维演进历程之软件定义存储（一） 从传统运维到云运维演进历程之软件定义存储（三）上 从传统运维到云运维演进历程之软件定义存储（三）下 从传统运维到云运维演进历程之软件定义存储（五）上 从传统运维到云运维演进历程之软件定义存储（五）中 从传统运维到云运维演进历程之软件定义存储（五）下 Ceph-RGW多活机制 BlueStore-先进的用户态文件系统《一》 Ceph之对象存储网关RADOS Gateway(RGW) ","description":"","id":35,"section":"posts","tags":["ceph","crush"],"title":"Ceph 硬件选型、crush规则及region","uri":"https://starifly.github.io/posts/ceph-hardware-planning-crush-rules-and-region/"},{"content":"操作系统基础配置 三节点创建文件夹： mkdir -p /etc/ceph /var/lib/ceph /var/log/ceph\n配置定时任务 systemctl start ntpd \u0026amp;\u0026amp; systemctl enable ntpd 将时间每隔1小时自动校准同步 0 */1 * * * ntpdate ntp1.aliyun.com \u0026gt; /dev/null 2\u0026gt;\u0026amp;1; /sbin/hwclock -w 内核优化 #调整内核参数 [root@CENTOS7-1 ~]# cat \u0026gt;\u0026gt; /etc/sysctl.conf \u0026lt;\u0026lt; EOF \u0026gt; kernel.pid_max=4194303 \u0026gt; vm.swappiness = 0 \u0026gt; EOF [root@CENTOS7-1 ~]# sysctl -p # read_ahead, 通过数据预读并且记载到随机访问内存方式提高磁盘读操作，8192是比较理想的值 [root@CENTOS7-1 ~]# echo \u0026#34;8192\u0026#34; \u0026gt; /sys/block/sda/queue/read_ahead_kb # I/O Scheduler优化，如果SSD要用noop，SATA/SAS设备采用deadline。 [root@CENTOS7-1 ~]#echo \u0026#34;deadline\u0026#34; \u0026gt; /sys/block/sda/queue/scheduler [root@CENTOS7-1 ~]#echo \u0026#34;noop\u0026#34; \u0026gt; /sys/block/sda/queue/scheduler 关闭selinux # vi /etc/selinux/config文件， 将SELINUX设为disabled, 永久生效。 SELINUX=disabled # 临时生效： setenforce 0 修改主机名和hosts hostnamectl set-hostname ceph001 hostnamectl set-hostname ceph002 hostnamectl set-hostname ceph003 # vim /etc/hosts 192.168.5.203 ceph001 192.168.5.203 ceph002 192.168.5.203 ceph003 编辑别名 echo \u0026#39;alias ceph=\u0026#34;docker exec mon ceph\u0026#34;\u0026#39; \u0026gt;\u0026gt; /etc/profile source /etc/profile 准备磁盘 三节点各准备一块磁盘，不用分区。\n以下安装配置基于centos7.8、ceph 14.2.22版本\n启动mon 主节点：\ndocker run -d --net=host --name=mon --restart=always -v /etc/localtime:/etc/localtime:ro -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -v /var/log/ceph:/var/log/ceph -e MON_IP=192.168.5.203 -e CEPH_PUBLIC_NETWORK=192.168.5.0/24 ceph/daemon:v4.0.22-stable-4.0-nautilus-centos-7-x86_64 mon 启动成功之后生成配置数据，在ceph主配置文件中，修改内容：\nmon host = 192.168.5.203,192.168.5.204,192.168.5.205 cat \u0026gt;\u0026gt;/etc/ceph/ceph.conf \u0026lt;\u0026lt;EOF # 容忍更多的时钟误差 mon clock drift allowed = 2 mon clock drift warn backoff = 30 # 允许删除pool mon_allow_pool_delete = true # rbd 配置 rbd_default_features = 3 # 关闭动态分片，在zone中配置 rgw dynamic resharding = false [mgr] # 开启WEB仪表盘 mgr modules = dashboard [client.rgw.ceph001] # 设置rgw网关的web访问端口 rgw_frontends = \u0026#34;civetweb port=20003 num_threads=2048\u0026#34; EOF 复制配置到另外两个节点：\nscp -r /etc/ceph/ root@192.168.5.204:/etc scp -r /etc/ceph/ root@192.168.5.205:/etc scp -r /var/lib/ceph/bootstrap-* root@192.168.5.204:/var/lib/ceph/ scp -r /var/lib/ceph/bootstrap-* root@192.168.5.205:/var/lib/ceph/ 修改节点2和3配置文件：\nclient.rgw.ceph001 -\u0026gt; client.rgw.ceph002 client.rgw.ceph001 -\u0026gt; client.rgw.ceph003 204节点：\ndocker run -d --net=host --name=mon --restart=always -v /etc/localtime:/etc/localtime:ro -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -v /var/log/ceph:/var/log/ceph -e MON_IP=192.168.5.204 -e CEPH_PUBLIC_NETWORK=192.168.5.0/24 ceph/daemon:v4.0.22-stable-4.0-nautilus-centos-7-x86_64 mon 205节点：\ndocker run -d --net=host --name=mon --restart=always -v /etc/localtime:/etc/localtime:ro -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -v /var/log/ceph:/var/log/ceph -e MON_IP=192.168.5.205 -e CEPH_PUBLIC_NETWORK=192.168.5.0/24 ceph/daemon:v4.0.22-stable-4.0-nautilus-centos-7-x86_64 mon 启动OSD 首先需要在mon节点生成osd的密钥信息，不然直接启动会报错。命令如下：\ndocker exec -it mon ceph auth get client.bootstrap-osd -o /var/lib/ceph/bootstrap-osd/ceph.keyring # 复制密钥到另外两个节点： scp -r /var/lib/ceph/bootstrap-osd/ceph.keyring 192.168.5.204:/var/lib/ceph/bootstrap-osd/ scp -r /var/lib/ceph/bootstrap-osd/ceph.keyring 192.168.5.205:/var/lib/ceph/bootstrap-osd/ osd采用bluestore，所以磁盘事先不用分区和挂载文件系统。bluestore比filestore的效率更高，推荐采用。\n三个节点执行如下命令启动osd：\n如有必要先格式化磁盘：\ndocker run --privileged=true --rm --pid=host -v /etc/localtime:/etc/localtime:ro -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -v /var/log/ceph:/var/log/ceph -v /dev/:/dev/ -v /run/udev/:/run/udev/ --entrypoint /bin/bash ceph/daemon:v4.0.22-stable-4.0-nautilus-centos-7-x86_64 -c \u0026#39;ceph-volume lvm zap /dev/sdb\u0026#39; ceph集群无法初始化osd问题,有时候会遇到这样的报错。\n[node3-ceph][WARNIN] stderr: wipefs: error: /dev/sdb: probing initialization failed: Device or resource busy [node3-ceph][WARNIN] --\u0026gt; failed to wipefs device, will try again to workaround probable race condition 遇到这种报错时，只能上这台机器，手动进行dd命令清空磁盘并重启\ndd if=/dev/zero of=/dev/sdb bs=512K count=1 reboot 重启完成后，就能够清理磁盘成功了。\n部署bluestore并激活：\n203节点：\ndocker run --privileged=true --rm --pid=host -v /etc/localtime:/etc/localtime:ro -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -v /var/log/ceph:/var/log/ceph -v /dev/:/dev/ -v /run/udev/:/run/udev/ --entrypoint /bin/bash ceph/daemon:v4.0.22-stable-4.0-nautilus-centos-7-x86_64 -c \u0026#39;ceph-volume lvm prepare --bluestore --data /dev/sdb\u0026#39; docker run -d --name=osd --net=host --restart=always --privileged=true --pid=host -v /etc/localtime:/etc/localtime:ro -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -v /var/log/ceph:/var/log/ceph -v /dev/:/dev/ -v /run/udev/:/run/udev/ -e OSD_ID=0 ceph/daemon:v4.0.22-stable-4.0-nautilus-centos-7-x86_64 osd_ceph_volume_activate 204节点：\ndocker run --privileged=true --rm --pid=host -v /etc/localtime:/etc/localtime:ro -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -v /var/log/ceph:/var/log/ceph -v /dev/:/dev/ -v /run/udev/:/run/udev/ --entrypoint /bin/bash ceph/daemon:v4.0.22-stable-4.0-nautilus-centos-7-x86_64 -c \u0026#39;ceph-volume lvm prepare --bluestore --data /dev/sdb\u0026#39; docker run -d --name=osd --net=host --restart=always --privileged=true --pid=host -v /etc/localtime:/etc/localtime:ro -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -v /var/log/ceph:/var/log/ceph -v /dev/:/dev/ -v /run/udev/:/run/udev/ -e OSD_ID=1 ceph/daemon:v4.0.22-stable-4.0-nautilus-centos-7-x86_64 osd_ceph_volume_activate 205节点：\ndocker run --privileged=true --rm --pid=host -v /etc/localtime:/etc/localtime:ro -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -v /var/log/ceph:/var/log/ceph -v /dev/:/dev/ -v /run/udev/:/run/udev/ --entrypoint /bin/bash ceph/daemon:v4.0.22-stable-4.0-nautilus-centos-7-x86_64 -c \u0026#39;ceph-volume lvm prepare --bluestore --data /dev/sdb\u0026#39; docker run -d --name=osd --net=host --restart=always --privileged=true --pid=host -v /etc/localtime:/etc/localtime:ro -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -v /var/log/ceph:/var/log/ceph -v /dev/:/dev/ -v /run/udev/:/run/udev/ -e OSD_ID=2 ceph/daemon:v4.0.22-stable-4.0-nautilus-centos-7-x86_64 osd_ceph_volume_activate 启动mgr 三个节点执行如下命令启动mgr：\ndocker run -d --net=host --name=mgr --restart=always -v /etc/localtime:/etc/localtime:ro -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -v /var/log/ceph:/var/log/ceph ceph/daemon:v4.0.22-stable-4.0-nautilus-centos-7-x86_64 mgr 启动rgw 首先需要在mon节点生成rgw的密钥信息，不然直接启动会报错。命令如下：\ndocker exec -it mon ceph auth get client.bootstrap-rgw -o /var/lib/ceph/bootstrap-rgw/ceph.keyring # 复制密钥到另外两个节点： scp -r /var/lib/ceph/bootstrap-rgw/ceph.keyring 192.168.5.204:/var/lib/ceph/bootstrap-rgw/ scp -r /var/lib/ceph/bootstrap-rgw/ceph.keyring 192.168.5.205:/var/lib/ceph/bootstrap-rgw/ 三个节点执行如下命令启动rgw：\ndocker run -d --net=host --name=rgw --restart=always -v /etc/localtime:/etc/localtime:ro -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -v /var/log/ceph:/var/log/ceph ceph/daemon:v4.0.22-stable-4.0-nautilus-centos-7-x86_64 rgw 启动mds 三个节点执行如下命令启动mds，注意mds名称不能以数字开头：\ndocker run -d --net=host --name=mds --restart=always -v /etc/localtime:/etc/localtime:ro -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -v /var/log/ceph:/var/log/ceph -e CEPHFS_CREATE=1 -e MDS_NAME=ceoh001 ceph/daemon:v4.0.22-stable-4.0-nautilus-centos-7-x86_64 mds docker run -d --net=host --name=mds --restart=always -v /etc/localtime:/etc/localtime:ro -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -v /var/log/ceph:/var/log/ceph -e CEPHFS_CREATE=1 -e MDS_NAME=ceoh002 ceph/daemon:v4.0.22-stable-4.0-nautilus-centos-7-x86_64 mds docker run -d --net=host --name=mds --restart=always -v /etc/localtime:/etc/localtime:ro -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -v /var/log/ceph:/var/log/ceph -e CEPHFS_CREATE=1 -e MDS_NAME=ceoh003 ceph/daemon:v4.0.22-stable-4.0-nautilus-centos-7-x86_64 mds 安装Dashboard管理后台 在主节点执行\n开启dashboard功能 docker exec mgr ceph mgr module enable dashboard\n创建证书 docker exec mgr ceph dashboard create-self-signed-cert\n创建登陆用户与密码： docker exec mgr ceph dashboard set-login-credentials admin -i /root/passwd.txt\n配置外部访问IP docker exec mgr ceph config set mgr mgr/dashboard/server_addr 192.168.5.203\n重启各个节点上的Mgr DashBoard服务，主节点最后重启 docker restart mgr\n查看Mgr DashBoard服务信息 [root@CENTOS7-1 admin]# docker exec mgr ceph mgr services { \u0026#34;dashboard\u0026#34;: \u0026#34;http://192.168.5.203:18080/\u0026#34; }\tReference 用Docker搭建Ceph集群(octopus版本) Centos7系统Docker Ceph 集群的安装配置（中篇） Centos7系统下Ceph分布式存储集群的部署搭建配置（上篇） 使用Docker快速部署Ceph集群 ceph13.2使用docker部署 关于ceph的一些问题及解决 Ceph 手把手教你部署ceph集群 dockerhub ceph搭建及使用详解 Ceph架构简介及使用 ceph-volume部署filestore或bluestore Update Documentation: OSD_TYPE=disk does not work in nautilus #1406 ","description":"","id":36,"section":"posts","tags":["ceph","nautilus","bluestore"],"title":"Docker安装ceph nautilus","uri":"https://starifly.github.io/posts/docker-install-ceph-nautilus/"},{"content":"依赖 # centos yum install librados2-devel 源程序 #include \u0026lt;iostream\u0026gt; #include \u0026lt;string\u0026gt; #include \u0026lt;rados/librados.hpp\u0026gt; int main(int argc, const char **argv) { int ret = 0; /* Declare the cluster handle and required variables. */ librados::Rados cluster; char cluster_name[] = \u0026#34;ceph\u0026#34;; char user_name[] = \u0026#34;client.admin\u0026#34;; uint64_t flags = 0; /* Initialize the cluster handle with the \u0026#34;ceph\u0026#34; cluster name and \u0026#34;client.admin\u0026#34; user */ { ret = cluster.init2(user_name, cluster_name, flags); if (ret \u0026lt; 0) { std::cerr \u0026lt;\u0026lt; \u0026#34;Couldn\u0026#39;t initialize the cluster handle! error \u0026#34; \u0026lt;\u0026lt; ret \u0026lt;\u0026lt; std::endl; return EXIT_FAILURE; } else { std::cout \u0026lt;\u0026lt; \u0026#34;Created a cluster handle.\u0026#34; \u0026lt;\u0026lt; std::endl; } } /* Read a Ceph configuration file to configure the cluster handle. */ { ret = cluster.conf_read_file(\u0026#34;/etc/ceph/ceph.conf\u0026#34;); if (ret \u0026lt; 0) { std::cerr \u0026lt;\u0026lt; \u0026#34;Couldn\u0026#39;t read the Ceph configuration file! error \u0026#34; \u0026lt;\u0026lt; ret \u0026lt;\u0026lt; std::endl; return EXIT_FAILURE; } else { std::cout \u0026lt;\u0026lt; \u0026#34;Read the Ceph configuration file.\u0026#34; \u0026lt;\u0026lt; std::endl; } } /* Read command line arguments */ { ret = cluster.conf_parse_argv(argc, argv); if (ret \u0026lt; 0) { std::cerr \u0026lt;\u0026lt; \u0026#34;Couldn\u0026#39;t parse command line options! error \u0026#34; \u0026lt;\u0026lt; ret \u0026lt;\u0026lt; std::endl; return EXIT_FAILURE; } else { std::cout \u0026lt;\u0026lt; \u0026#34;Parsed command line options.\u0026#34; \u0026lt;\u0026lt; std::endl; } } /* Connect to the cluster */ { ret = cluster.connect(); if (ret \u0026lt; 0) { std::cerr \u0026lt;\u0026lt; \u0026#34;Couldn\u0026#39;t connect to cluster! error \u0026#34; \u0026lt;\u0026lt; ret \u0026lt;\u0026lt; std::endl; return EXIT_FAILURE; } else { std::cout \u0026lt;\u0026lt; \u0026#34;Connected to the cluster.\u0026#34; \u0026lt;\u0026lt; std::endl; } } librados::IoCtx io_ctx; const char *pool_name = \u0026#34;testpool\u0026#34;; { ret = cluster.ioctx_create(pool_name, io_ctx); if (ret \u0026lt; 0) { std::cerr \u0026lt;\u0026lt; \u0026#34;Couldn\u0026#39;t set up ioctx! error \u0026#34; \u0026lt;\u0026lt; ret \u0026lt;\u0026lt; std::endl; exit(EXIT_FAILURE); } else { std::cout \u0026lt;\u0026lt; \u0026#34;Created an ioctx for the pool.\u0026#34; \u0026lt;\u0026lt; std::endl; } } /* Write an object synchronously. */ { librados::bufferlist bl; bl.append(\u0026#34;Hello World!\u0026#34;); ret = io_ctx.write_full(\u0026#34;hw\u0026#34;, bl); if (ret \u0026lt; 0) { std::cerr \u0026lt;\u0026lt; \u0026#34;Couldn\u0026#39;t write object! error \u0026#34; \u0026lt;\u0026lt; ret \u0026lt;\u0026lt; std::endl; exit(EXIT_FAILURE); } else { std::cout \u0026lt;\u0026lt; \u0026#34;Wrote new object \u0026#39;hw\u0026#39; \u0026#34; \u0026lt;\u0026lt; std::endl; } } /* * Add an xattr to the object. */ { librados::bufferlist lang_bl; lang_bl.append(\u0026#34;en_US\u0026#34;); ret = io_ctx.setxattr(\u0026#34;hw\u0026#34;, \u0026#34;lang\u0026#34;, lang_bl); if (ret \u0026lt; 0) { std::cerr \u0026lt;\u0026lt; \u0026#34;failed to set xattr version entry! error \u0026#34; \u0026lt;\u0026lt; ret \u0026lt;\u0026lt; std::endl; exit(EXIT_FAILURE); } else { std::cout \u0026lt;\u0026lt; \u0026#34;Set the xattr \u0026#39;lang\u0026#39; on our object!\u0026#34; \u0026lt;\u0026lt; std::endl; } } /* * Read the object back asynchronously. */ { librados::bufferlist read_buf; int read_len = 4194304; //Create I/O Completion. librados::AioCompletion *read_completion = librados::Rados::aio_create_completion(); //Send read request. ret = io_ctx.aio_read(\u0026#34;hw\u0026#34;, read_completion, \u0026amp;read_buf, read_len, 0); if (ret \u0026lt; 0) { std::cerr \u0026lt;\u0026lt; \u0026#34;Couldn\u0026#39;t start read object! error \u0026#34; \u0026lt;\u0026lt; ret \u0026lt;\u0026lt; std::endl; exit(EXIT_FAILURE); } // Wait for the request to complete, and check that it succeeded. read_completion-\u0026gt;wait_for_complete(); ret = read_completion-\u0026gt;get_return_value(); if (ret \u0026lt; 0) { std::cerr \u0026lt;\u0026lt; \u0026#34;Couldn\u0026#39;t read object! error \u0026#34; \u0026lt;\u0026lt; ret \u0026lt;\u0026lt; std::endl; exit(EXIT_FAILURE); } else { std::cout \u0026lt;\u0026lt; \u0026#34;Read object hw asynchronously with contents.\\n\u0026#34; \u0026lt;\u0026lt; read_buf.c_str() \u0026lt;\u0026lt; std::endl; } } /* * Read the xattr. */ { librados::bufferlist lang_res; ret = io_ctx.getxattr(\u0026#34;hw\u0026#34;, \u0026#34;lang\u0026#34;, lang_res); if (ret \u0026lt; 0) { std::cerr \u0026lt;\u0026lt; \u0026#34;failed to get xattr version entry! error \u0026#34; \u0026lt;\u0026lt; ret \u0026lt;\u0026lt; std::endl; exit(EXIT_FAILURE); } else { std::cout \u0026lt;\u0026lt; \u0026#34;Got the xattr \u0026#39;lang\u0026#39; from object hw!\u0026#34; \u0026lt;\u0026lt; lang_res.c_str() \u0026lt;\u0026lt; std::endl; } } /* * Remove the xattr. */ { ret = io_ctx.rmxattr(\u0026#34;hw\u0026#34;, \u0026#34;lang\u0026#34;); if (ret \u0026lt; 0) { std::cerr \u0026lt;\u0026lt; \u0026#34;Failed to remove xattr! error \u0026#34; \u0026lt;\u0026lt; ret \u0026lt;\u0026lt; std::endl; exit(EXIT_FAILURE); } else { std::cout \u0026lt;\u0026lt; \u0026#34;Removed the xattr \u0026#39;lang\u0026#39; from our object!\u0026#34; \u0026lt;\u0026lt; std::endl; } } /* * Remove the object. */ { ret = io_ctx.remove(\u0026#34;hw\u0026#34;); if (ret \u0026lt; 0) { std::cerr \u0026lt;\u0026lt; \u0026#34;Couldn\u0026#39;t remove object! error \u0026#34; \u0026lt;\u0026lt; ret \u0026lt;\u0026lt; std::endl; exit(EXIT_FAILURE); } else { std::cout \u0026lt;\u0026lt; \u0026#34;Removed object \u0026#39;hw\u0026#39;.\u0026#34; \u0026lt;\u0026lt; std::endl; } } io_ctx.close(); cluster.shutdown(); return 0; } 编译 g++ -g -c cephclient.cc -o cephclient.o -std=c++11 g++ -g cephclient.o -lrados -o cephclient 运行 注意运行前要先创建“testpool”存储池\n[root@203 ~]# ./cephclient Created a cluster handle. Read the Ceph configuration file. Parsed command line options. Connected to the cluster. Created an ioctx for the pool. Wrote new object \u0026#39;hw\u0026#39; Set the xattr \u0026#39;lang\u0026#39; on our object! Read object hw asynchronously with contents. Hello World! Got the xattr \u0026#39;lang\u0026#39; from object hw!en_US Removed the xattr \u0026#39;lang\u0026#39; from our object! Removed object \u0026#39;hw\u0026#39;. Reference Introduction to librados librados/c++/N版 ","description":"","id":37,"section":"posts","tags":["ceph","librados"],"title":"ceph librados库c++使用","uri":"https://starifly.github.io/posts/ceph-librados-c++-use/"},{"content":"【前言】\nceph创建存储池需要pg数和pgp数的两个参数，在前面我们介绍了ceph的pg，那么pgp和pg有什么关系呢？\nPG (Placement Group)，pg是一个虚拟的概念，用于存放object，PGP(Placement Group for Placement purpose)，相当于是pg存放的一种osd排列组合。举个例子：假设集群有3个osd，即osd1，osd2，osd3，副本数为2，如果pgp=1，那么pg存放的osd的组合就有一种，可能是[osd1，osd2]，那么所有的pg主从副本都会存放到osd1和osd2上；如果pgp=2，那么其osd组合可能就两种，可能是[osd1,osd2]和[osd1,osd3]，pg的主从副本会落在[osd1,osd2]或者[osd1,osd3]中，和我们数学中的排列组合很像，所以pg是存放对象的归属组是一种虚拟概念，pgp就是pg对应的osd排列组合。一般情况下，存储池的pg和pgp的数量设置相等。\n【实践】\n上面大概讲了pg和pgp的关系，下面我们通过实际例子来更具体一步说明：\n我准备的测试环境是3个节点，每个节点3个osd，首先我们创建以个名为pool_1的存储池，包含6个pg：\n[root@node1 ~]# ceph osd pool create pool_1 6 6 pool \u0026#39;pool_1\u0026#39; created [root@node1 ~]# ceph osd pool set pool_1 size 2 #设置存储池副本数为2 set pool 2 size to 2 使用ceph pg dump pgs查看pg的分布：因为存储池为双副本，我们可以看到每个pg会分布在两个osd上，整个集群有9个osd，按照排列组合会有很多种，此时pgp=6，就会选择这些组合中的6种组合来供pg存放，我们可以看到最右侧的6中组合均不重复。\n[root@node1 ~]# ceph pg dump pgs |grep active |awk \u0026#39;{print $1,$19}\u0026#39; 2.5 0 [1,2] 2.4 0 [6,0] 2.3 0 [5,2] 2.2 0 [5,6] 2.1 0 [7,8] 2.0 0 [0,6] 接下来我们使用ceph自带的bench工具写入数据，来观察pg内的对象有没有移动：\nrados -p pool_1 bench 20 write --no-cleanu 再次查询结果如下：第2列为每个pg的对象数，第3列为pg所在的osd，我们可以看到存储创建好了pg设置固定了其osd的分布不会随着对象的增加而改变。\n[root@node1 ~]# ceph pg dump pgs |grep active |awk \u0026#39;{print $1,$19}\u0026#39; 2.5 178 [1,2] 2.4 162 [6,0] 2.3 368 [5,2] 2.2 308 [5,6] 2.1 176 [7,8] 2.0 166 [0,6] (1) 增加PG数\n接下来我们改变下pg数，将pg数扩大到12：\n[root@node1 ~]# ceph osd pool set pool_1 pg_num 12 set pool 2 pg_num to 12 再次查看存储池pg分布结果如下：\n[root@node1 ~]# ceph pg dump pgs |grep active |awk \u0026#39;{print $1,$2,$19}\u0026#39; 2.b 96 [5,2] 2.a 73 [5,6] 2.0 76 [0,6] 2.1 88 [7,8] 2.2 80 [5,6] 2.9 88 [7,8] 2.3 91 [5,2] 2.4 162 [6,0] 2.5 178 [1,2] 2.6 103 [5,6] 2.7 181 [5,2] 2.8 90 [0,6] 通过上面的测试结果我们可以看到，新增加的pg数还是基于pgp=6的排列组合，并没有出现新的排列组合，因为我们当前的存储池的pgp是6，那么双副本2个osd的组合就是6个，因为当前的pg是12，分布只能从6中组合里面选择，所以会有重复的组合。\n结论：增加PG会引起PG内的对象进行分裂，也就是说在OSD上新建了PG目录，然后进行部分对象迁移的操作。即PG改变会引起数据迁移。\n(2) 增加PGP\n我们将PGP从6调整为12：\n[root@node1 ~]# ceph osd pool set pool_1 pgp_num 12 set pool 2 pgp_num to 12 [root@node1 ~]# ceph pg dump pgs |grep active |awk \u0026#39;{print $1,$2,$19}\u0026#39; 2.b 96 [8,0] 2.a 73 [2,4] 2.0 76 [0,6] 2.1 88 [7,8] 2.2 80 [5,6] 2.9 88 [4,8] 2.3 91 [5,2] 2.4 162 [6,0] 2.5 178 [1,2] 2.6 155 [8,4] 2.7 181 [1,2] 2.8 90 [0,7] 可以看到pg内的对象数并没有发生改变，而pg所在的osd的对应关系发生了改变，可以看到最初pg=6 pgp=6的时候前6个pg的分布并没有发生变化，变化的都是后面增加的pg，也就是将重复的pg分布进行新分布，这里并不是随机打散，而是尽量做小改动的重新分布，这就是所谓的一致性哈希原理。\n结论：调整PGP不会引起PG内的对象的分裂，但是会引起PG的分布变动。\n【总结】\nPG是指定存储池存储对象的归属组有多少个，PGP是存储池PG的OSD分布组合个数\nPG的增加会引起PG内的数据进行迁移，迁移到不同的OSD上新生成的PG中\nPGP的增加会引起部分PG的分布变化，但是不会引起PG内对象的变动。\nReference Ceph中的PG和PGP的关系 ","description":"","id":38,"section":"posts","tags":["ceph","pg","pgp"],"title":"Ceph中的PG和PGP的关系","uri":"https://starifly.github.io/posts/ceph-pg-and-pgp/"},{"content":"本文基于centos7以docker方式安装ceph 12.2.13版本\n操作系统基础配置 三节点创建文件夹： mkdir -p /etc/ceph/ /var/lib/ceph/ /var/log/ceph/ chown -R 167:167 /var/log/ceph/ 配置定时任务 systemctl start ntpd \u0026amp;\u0026amp; systemctl enable ntpd # 将时间每隔1小时自动校准同步 0 */1 * * * ntpdate ntp1.aliyun.com \u0026gt; /dev/null 2\u0026gt;\u0026amp;1; /sbin/hwclock -w 内核优化 #调整内核参数 [root@CENTOS7-1 ~]# cat \u0026gt;\u0026gt; /etc/sysctl.conf \u0026lt;\u0026lt; EOF \u0026gt; kernel.pid_max=4194303 \u0026gt; vm.swappiness = 0 \u0026gt; EOF [root@CENTOS7-1 ~]# sysctl -p # read_ahead, 通过数据预读并且记载到随机访问内存方式提高磁盘读操作，8192是比较理想的值 [root@CENTOS7-1 ~]# echo \u0026#34;8192\u0026#34; \u0026gt; /sys/block/sda/queue/read_ahead_kb # I/O Scheduler优化，如果SSD要用noop，SATA/SAS设备采用deadline。 [root@CENTOS7-1 ~]#echo \u0026#34;deadline\u0026#34; \u0026gt; /sys/block/sda/queue/scheduler [root@CENTOS7-1 ~]#echo \u0026#34;noop\u0026#34; \u0026gt; /sys/block/sda/queue/scheduler 关闭selinux # vi /etc/selinux/config文件， 将SELINUX设为disabled, 永久生效。 SELINUX=disabled # 临时生效： setenforce 0 编辑别名 echo \u0026#39;alias ceph=\u0026#34;docker exec mon ceph\u0026#34;\u0026#39; \u0026gt;\u0026gt; /etc/profile echo \u0026#39;alias ceph-fuse=\u0026#34;docker exec mon ceph-fuse\u0026#34;\u0026#39; \u0026gt;\u0026gt; /etc/profile echo \u0026#39;alias ceph-mon=\u0026#34;docker exec mon ceph-mon\u0026#34;\u0026#39; \u0026gt;\u0026gt; /etc/profile echo \u0026#39;alias ceph-osd=\u0026#34;docker exec mon ceph-osd\u0026#34;\u0026#39; \u0026gt;\u0026gt; /etc/profile echo \u0026#39;alias radosgw=\u0026#34;docker exec mon radosgw\u0026#34;\u0026#39; \u0026gt;\u0026gt; /etc/profile echo \u0026#39;alias radosgw-admin=\u0026#34;docker exec mon radosgw-admin\u0026#34;\u0026#39; \u0026gt;\u0026gt; /etc/profile echo \u0026#39;alias rados=\u0026#34;docker exec mon rados\u0026#34;\u0026#39; \u0026gt;\u0026gt; /etc/profile source /etc/profile 启动mon 主节点启动mon：\ndocker run -d --net=host --name=mon1 --restart=always -v /etc/localtime:/etc/localtime:ro -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -v /var/log/ceph:/var/log/ceph -e MON_IP=192.168.5.203 -e CEPH_PUBLIC_NETWORK=192.168.5.0/24 ceph/daemon:latest-luminous mon 复制配置到另外两个节点：\nscp -r /etc/ceph/ root@192.168.5.204:/etc/ scp -r /etc/ceph/ root@192.168.5.205:/etc/ scp -r /var/lib/ceph/bootstrap-* root@192.168.5.204:/var/lib/ceph/ scp -r /var/lib/ceph/bootstrap-* root@192.168.5.205:/var/lib/ceph/ 启动成功之后生成配置数据，在ceph主配置文件中，追加如下内容：\n203节点\ncat \u0026gt;\u0026gt;/etc/ceph/ceph.conf \u0026lt;\u0026lt;EOF # 容忍更多的时钟误差 mon clock drift allowed = 2 mon clock drift warn backoff = 30 # 允许删除pool mon_allow_pool_delete = true [mgr] # 开启WEB仪表盘 mgr modules = dashboard [client.rgw.ceph1] # 设置rgw网关的web访问端口 rgw_frontends = \u0026#34;civetweb port=20003\u0026#34; EOF 204节点：\ncat \u0026gt;\u0026gt;/etc/ceph/ceph.conf \u0026lt;\u0026lt;EOF # 容忍更多的时钟误差 mon clock drift allowed = 2 mon clock drift warn backoff = 30 # 允许删除pool mon_allow_pool_delete = true [mgr] # 开启WEB仪表盘 mgr modules = dashboard [client.rgw.ceph2] # 设置rgw网关的web访问端口 rgw_frontends = \u0026#34;civetweb port=20003\u0026#34; EOF 205节点：\ncat \u0026gt;\u0026gt;/etc/ceph/ceph.conf \u0026lt;\u0026lt;EOF # 容忍更多的时钟误差 mon clock drift allowed = 2 mon clock drift warn backoff = 30 # 允许删除pool mon_allow_pool_delete = true [mgr] # 开启WEB仪表盘 mgr modules = dashboard [client.rgw.ceph3] # 设置rgw网关的web访问端口 rgw_frontends = \u0026#34;civetweb port=20003\u0026#34; EOF 204节点启动mon：\ndocker run -d --net=host --name=mon2 --restart=always -v /etc/localtime:/etc/localtime:ro -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -v /var/log/ceph:/var/log/ceph -e MON_IP=192.168.5.204 -e CEPH_PUBLIC_NETWORK=192.168.5.0/24 ceph/daemon:latest-luminous mon 205节点启动mon：\ndocker run -d --net=host --name=mon3 --restart=always -v /etc/localtime:/etc/localtime:ro -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -v /var/log/ceph:/var/log/ceph -e MON_IP=192.168.5.205 -e CEPH_PUBLIC_NETWORK=192.168.5.0/24 ceph/daemon:latest-luminous mon 启动OSD osd建议以bluestore运行，因为性能高于filestore，这里因为仅做测试，所以使用的是filestore，需要新的硬盘作为OSD存储设备， 关闭虚拟机， 增加一块硬盘(具体增加可参考网上的教程)，然后挂载磁盘。\nmkdir /cephdata mkfs.xfs -f /dev/sdc1 #设备名字根据实际修改 mount /dev/sdc1 /cephdata 增加开机挂载vim /etc/fstab /dev/sdc1 /cephdata/ xfs defaults 0 0 如果没有独立磁盘，怎么办？ 可以在Linux下面创建一个虚拟磁盘进行挂载。\nmkdir -p /usr/local/ceph-disk dd if=/dev/zero of=/usr/local/ceph-disk/ceph-disk-01 bs=1G count=2 将镜像文件虚拟成块设备： losetup -f /usr/local/ceph-disk/ceph-disk-01 格式化： #名称根据fdisk -l进行查询确认, 一般是/dev/loop0 mkfs.xfs -f /dev/loop0 挂载文件系统： mkdir -p /usr/local/ceph/data/osd/ mount /dev/loop0 /usr/local/ceph/data/osd/ 三个节点分别执行如下命令启动osd：\ndocker run -d --name=osd1 --net=host --restart=always --privileged=true --pid=host -v /etc/localtime:/etc/localtime:ro -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -v /var/log/ceph:/var/log/ceph -v /cephdata:/var/lib/ceph/osd -v /dev/:/dev/ -v /run/udev/:/run/udev/ -e OSD_TYPE=directory ceph/daemon:latest-luminous osd docker run -d --name=osd2 --net=host --restart=always --privileged=true --pid=host -v /etc/localtime:/etc/localtime:ro -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -v /var/log/ceph:/var/log/ceph -v /cephdata:/var/lib/ceph/osd -v /dev/:/dev/ -v /run/udev/:/run/udev/ -e OSD_TYPE=directory ceph/daemon:latest-luminous osd docker run -d --name=osd3 --net=host --restart=always --privileged=true --pid=host -v /etc/localtime:/etc/localtime:ro -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -v /var/log/ceph:/var/log/ceph -v /cephdata:/var/lib/ceph/osd -v /dev/:/dev/ -v /run/udev/:/run/udev/ -e OSD_TYPE=directory ceph/daemon:latest-luminous osd 如果直接指定磁盘分区，可以用如下命令启动osd： docker run -d --name=osd1 --net=host --restart=always --privileged=true --pid=host -v /etc/localtime:/etc/localtime:ro -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -v /var/log/ceph:/var/log/ceph -v /dev/:/dev/ -v /run/udev/:/run/udev/ -e OSD_DEVICE=/dev/sdc ceph/daemon:latest-luminous osd 启动mgr 三个节点执行如下命令启动mgr：\ndocker run -d --net=host --name=mgr1 --restart=always -v /etc/localtime:/etc/localtime:ro -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -v /var/log/ceph:/var/log/ceph ceph/daemon:latest-luminous mgr docker run -d --net=host --name=mgr2 --restart=always -v /etc/localtime:/etc/localtime:ro -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -v /var/log/ceph:/var/log/ceph ceph/daemon:latest-luminous mgr docker run -d --net=host --name=mgr3 --restart=always -v /etc/localtime:/etc/localtime:ro -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -v /var/log/ceph:/var/log/ceph ceph/daemon:latest-luminous mgr 启动rgw 三个节点执行如下命令启动rgw：\ndocker run -d --net=host --name=rgw1 --restart=always -v /etc/localtime:/etc/localtime:ro -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -v /var/log/ceph:/var/log/ceph ceph/daemon:latest-luminous rgw docker run -d --net=host --name=rgw2 --restart=always -v /etc/localtime:/etc/localtime:ro -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -v /var/log/ceph:/var/log/ceph ceph/daemon:latest-luminous rgw docker run -d --net=host --name=rgw3 --restart=always -v /etc/localtime:/etc/localtime:ro -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -v /var/log/ceph:/var/log/ceph ceph/daemon:latest-luminous rgw 启动mds MDS 只是为 CephFS 的使用者提供。目前 MDS 多实例仍然处于试验阶段（不提供商业支持），而且文件存储的性能要低于对象存储或者块存储。实际部署参考如下命令。\n三个节点执行如下命令启动mds，注意mds名称不能以数字开头：\ndocker run -d --net=host --name=mds1 --restart=always -v /etc/localtime:/etc/localtime:ro -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -v /var/log/ceph:/var/log/ceph -e CEPHFS_CREATE=1 -e MDS_NAME=m203 ceph/daemon:latest-luminous mds docker run -d --net=host --name=mds2 --restart=always -v /etc/localtime:/etc/localtime:ro -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -v /var/log/ceph:/var/log/ceph -e CEPHFS_CREATE=1 -e MDS_NAME=m204 ceph/daemon:latest-luminous mds docker run -d --net=host --name=mds3 --restart=always -v /etc/localtime:/etc/localtime:ro -v /etc/ceph:/etc/ceph -v /var/lib/ceph:/var/lib/ceph -v /var/log/ceph:/var/log/ceph -e CEPHFS_CREATE=1 -e MDS_NAME=m205 ceph/daemon:latest-luminous mds 安装Dashboard管理后台 在主节点执行\n开启dashboard功能 docker exec mgr1 ceph mgr module enable dashboard\n配置外部访问IP docker exec mgr1 ceph config-key put mgr/dashboard/server_addr 192.168.5.203\n配置外部访问端口 docker exec mgr1 ceph config-key put mgr/dashboard/server_port 18080\n重启各个节点上的Mgr DashBoard服务，主节点最后重启 docker restart mgr\n查看Mgr DashBoard服务信息 [root@CENTOS7-1 admin]# docker exec mgr1 ceph mgr services { \u0026#34;dashboard\u0026#34;: \u0026#34;http://0.0.0.0:18080/\u0026#34; }\tReference 用Docker搭建Ceph集群(luminous版本) Centos7系统Docker Ceph 集群的安装配置（中篇） Centos7系统下Ceph分布式存储集群的部署搭建配置（上篇） 使用Docker快速部署Ceph集群 ceph13.2使用docker部署 关于ceph的一些问题及解决 Ceph 手把手教你部署ceph集群 dockerhub ceph搭建及使用详解 Ceph架构简介及使用 ","description":"","id":39,"section":"posts","tags":["ceph","docker","luminous"],"title":"Docker安装ceph luminous","uri":"https://starifly.github.io/posts/docker-install-ceph-luminous/"},{"content":"之前一台 centos 虚拟机中毒，导致CPU飙升，这里记录下处理过程。\n查找可疑进程\n执行命令ll /proc/ID/exe，通过进程ID确定其执行路径，先删掉可疑应用，然后再利用chattr +ai /path再次生成，并 kill 掉可疑进程。\n删除定时任务及文件以及开机启动文件\n进入以下目录/etc/init.d/、/etc/rc.d/rc#.d/、/etc/cron.d、/usr/lib/systemd/system/、/etc/systemd/system/、/etc/profile.d，并清理可疑文件。\n确认文件/etc/crontab中是否有定时任务。\n删除可疑执行文件\n进入以下目录/bin，/sbin，/usr/libexec/，并清理可疑文件。\n修复ssh认证文件\n~/.ssh/authorized_keys删除或修复\n查看etc目录下文件\n查看/etc目录下的文件是否被篡改或增加，比如hosts文件。\nReference 记录一次linux病毒清除过程 记录一次清除Linux挖矿病毒的经历(sysupdate, networkservice进程) 11个步骤完美排查Linux机器是否已经被入侵 ","description":"","id":40,"section":"posts","tags":["linux","centos"],"title":"Linux中病毒后的处理过程","uri":"https://starifly.github.io/posts/draft/virus-process-in-linux-system/"},{"content":"\r友情链接\rCalvin Haynes's Blog\nCalvin Haynes的博客站\r","description":"飞鸟的友情链接","id":41,"section":"","tags":null,"title":"我的朋友们","uri":"https://starifly.github.io/friends/"},{"content":" 本文为转载文章， 原文地址 https://coolshell.cn/articles/10975.html\n（感谢网友 @我的上铺叫路遥 投稿）\n协程 (coroutine) 顾名思义就是“协作的例程”（co-operative routines）。跟具有操作系统概念的线程不一样，协程是在用户空间利用程序语言的语法语义就能实现逻辑上类似多任务的编程技巧。实际上协程的概念比线程还要早，按照 Knuth 的说法 “子例程是协程的特例”，一个子例程就是一次子函数调用，那么实际上协程就是类函数一样的程序组件，你可以在一个线程里面轻松创建数十万个协程，就像数十万次函数调用一样。只不过子例程只有一个调用入口起始点，返回之后就结束了，而协程入口既可以是起始点，又可以从上一个返回点继续执行，也就是说协程之间可以通过 yield 方式转移执行权，对称（symmetric）、平级地调用对方，而不是像例程那样上下级调用关系。当然 Knuth 的 “特例” 指的是协程也可以模拟例程那样实现上下级调用关系，这就叫非对称协程（asymmetric coroutines）。\n基于事件驱动模型 我们举一个例子来看看一种对称协程调用场景，大家最熟悉的 “生产者 - 消费者” 事件驱动模型，一个协程负责生产产品并将它们加入队列，另一个负责从队列中取出产品并使用它。为了提高效率，你想一次增加或删除多个产品。伪代码可以是这样的：\n# producer coroutine\rloop\rwhile queue is not full\rcreate some new items\radd the items to queue\ryield to consumer\r# consumer coroutine\rloop\rwhile queue is not empty\rremove some items from queue\ruse the items\ryield to producer\r大多数教材上拿这种模型作为多线程的例子，实际上多线程在此的应用还是显得有点 “重量级”，由于缺乏 yield 语义，线程之间不得不使用同步机制来避免产生全局资源的竟态，这就不可避免产生了休眠、调度、切换上下文一类的系统开销，而且线程调度还会产生时序上的不确定性。而对于协程来说，“挂起” 的概念只不过是转让代码执行权并调用另外的协程，待到转让的协程告一段落后重新得到调用并从挂起点“唤醒”，这种协程间的调用是逻辑上可控的，时序上确定的，可谓一切尽在掌握中。\n当今一些具备协程语义的语言，比较重量级的如 C#、erlang、golang，以及轻量级的 python、lua、javascript、ruby，还有函数式的 scala、scheme 等。相比之下，作为原生态语言的 C 反而处于尴尬的地位，原因在于 C 依赖于一种叫做栈帧的例程调用，例程内部的状态量和返回值都保留在堆栈上，这意味着生产者和消费者相互之间无法实现平级调用，当然你可以改写成把生产者作为主例程然后将产品作为传递参数调用消费者例程，这样的代码写起来费力不讨好而且看起来会很难受，特别当协程数目达到十万数量级，这种写法就过于僵化了。\n这就引出了协程的概念，**如果将每个协程的上下文（比如程序计数器）保存在其它地方而不是堆栈上，协程之间相互调用时，被调用的协程只要从堆栈以外的地方恢复上次出让点之前的上下文即可，这有点类似于 CPU 的上下文切换，**遗憾的是似乎只有更底层的汇编语言才能做到这一点。\n难道 C 语言只能用多线程吗？幸运的是，C 标准库给我们提供了两种协程调度原语：一种是 setjmp/longjmp，另一种是 ucontext 组件，它们内部（当然是用汇编语言）实现了协程的上下文切换，相较之下前者在应用上会产生相当的不确定性（比如不好封装，具体说明参考联机文档），所以后者应用更广泛一些，网上绝大多数 C 协程库也是基于 ucontext 组件实现的。\n“蝇量级” 的协程库 在此，我来介绍一种 “蝇量级” 的开源 C 协程库 protothreads。这是一个全部用 ANSI C 写成的库，之所以称为 “蝇量级” 的，就是说，实现已经不能再精简了，几乎就是原语级别。事实上 protothreads 整个库不需要链接加载，因为所有源码都是头文件，类似于 STL 这样不依赖任何第三方库，在任何平台上可移植；总共也就 5 个头文件，有效代码量不足 100 行；API 都是宏定义的，所以不存在调用开销；最后，每个协程的空间开销是 2 个字节（是的，你没有看错，就是一个 short 单位的“栈”！）当然这种精简是要以使用上的局限为代价的，接下来的分析会说明这一点。\n先来看看 protothreads 作者，\rAdam Dunkels，一位来自瑞典皇家理工学院的计算机天才帅哥。话说这哥们挺有意思的，写了好多轻量级的作品，都是 BSD 许可证。顺便说一句，轻量级开源软件全世界多如牛毛，可像这位哥们写得如此出名的并不多。比如嵌入式网络操作系统 Contiki，国人耳熟能详的 TCP/IP 协议栈 uIP 和 lwIP 也是出自其手。上述这些软件都是经过数十年企业级应用的考验，质量之高可想而知。\n很多人会好奇如此 “蝇量级” 的代码究竟是怎么实现的呢？在分析 protothreads 源码之前，我先来给大家补一补 C 语言的基础课;-^)简而言之，这利用了 C 语言特性上的一个“奇技淫巧”，而且这种技巧恐怕连许多具备十年以上经验的 C 程序员老手都不见得知晓。当然这里先要声明我不是推荐大家都这么用，实际上这是以破坏语言的代码规范为代价，在一些严肃的项目工程中需要谨慎对待，除非你想被炒鱿鱼。\nC 语言的 “yield 语义” 下面的教程来自于一位 ARM 工程师、天才黑客 Simon Tatham（开源 Telnet/SSH 客户端 PuTTY 和汇编器 NASM 的作者，吐槽一句，PuTTY 的源码号称是所有正式项目里最难 hack 的 C，你应该猜到作者是什么语言出身）的博文：\rCoroutines in C。中文译文在\r这里。\n我们知道 python 的 yield 语义功能类似于一种迭代生成器，函数会保留上次的调用状态，并在下次调用时会从上个返回点继续执行。用 C 语言来写就像这样：\nint function(void) {\rint i;\rfor (i = 0; i \u003c 10; i++)\rreturn i; /* won't work, but wouldn't it be nice */\r}\r连续对它调用 10 次，它能分别返回 0 到 9。该怎样实现呢？可以利用 goto 语句，如果我们在函数中加入一个状态变量，就可以这样实现：\nint function(void) {\rstatic int i, state = 0;\rswitch (state) {\rcase 0: goto LABEL0;\rcase 1: goto LABEL1;\r}\rLABEL0: /* start of function */\rfor (i = 0; i \u003c 10; i++) {\rstate = 1; /* so we will come back to LABEL1 */\rreturn i;\rLABEL1:; /* resume control straight after the return */\r}\r}\r这个方法是可行的。我们在所有需要 yield 的位置都加上标签：起始位置加一个，还有所有 return 语句之后都加一个。每个标签用数字编号，我们在状态变量中保存这个编号，这样就能在我们下次调用时告诉我们应该跳到哪个标签上。每次返回前，更新状态变量，指向到正确的标签；不论调用多少次，针对状态变量的 switch 语句都能找到我们要跳转到的位置。\n但这还是难看得很。最糟糕的部分是所有的标签都需要手工维护，还必须保证函数中的标签和开头 switch 语句中的一致。每次新增一个 return 语句，就必须想一个新的标签名并将其加到 switch 语句中；每次删除 return 语句时，同样也必须删除对应的标签。这使得维护代码的工作量增加了一倍。\n仔细想想，其实我们可以不用 switch 语句来决定要跳转到哪里去执行，而是直接利用 switch 语句本身来实现跳转：\nint function(void) {\rstatic int i, state = 0;\rswitch (state) {\rcase 0: /* start of function */\rfor (i = 0; i \u003c 10; i++) {\rstate = 1; /* so we will come back to \"case 1\" */\rreturn i;\rcase 1:; /* resume control straight after the return */\r}\r}\r}\r酷！没想到 switch-case 语句可以这样用，其实说白了 C 语言就是脱胎于汇编语言的，switch-case 跟 if-else 一样，无非就是汇编的条件跳转指令的另类实现而已（这也间接解释了为何汇编程序员经常揶揄 C 语言是 “大便一样的代码”）。我们还可以用 LINE 宏使其更加一般化：\nint function(void) {\rstatic int i, state = 0;\rswitch (state) {\rcase 0: /* start of function */\rfor (i = 0; i \u003c 10; i++) {\rstate = __LINE__ + 2; /* so we will come back to \"case __LINE__\" */\rreturn i;\rcase __LINE__:; /* resume control straight after the return */\r}\r}\r}\r这样一来我们可以用宏提炼出一种范式，封装成组件：\n#define Begin() static int state=0; switch(state) { case 0:\r#define Yield(x) do { state=__LINE__; return x; case __LINE__:; } while (0)\r#define End() }\rint function(void) {\rstatic int i;\rBegin();\rfor (i = 0; i \u003c 10; i++)\rYield(i);\rEnd();\r}\r怎么样，看起来像不像发明了一种全新的语言？实际上我们利用了 switch-case 的分支跳转特性，以及预编译的 LINE 宏，实现了一种隐式状态机，最终实现了 “yield 语义”。\n还有一个问题，当你欢天喜地地将这种鲜为人知的技巧运用到你的项目中，并成功地拿去向你的上司邀功问赏的时候，你的上司会怎样看待你的代码呢？你的宏定义中大括号没有匹配完整，在代码块中包含了未用到的 case，Begin 和 Yield 宏里面不完整的七拼八凑…… 你简直就是公司里不遵守编码规范的反面榜样！\n别着急，在原文中 Simon Tatham 大牛帮你找到一个坚定的反驳理由，我觉得对程序员来说简直是金玉良言。\n将编程规范用在这里是不对的。文章里给出的示例代码不是很长，也不很复杂，即便以状态机的方式改写还是能够看懂的。但是随着代码越来越长，改写的难度将越来越大，改写对直观性造成的损失也变得相当相当大。\n想一想，一个函数如果包含这样的小代码块：\ncase STATE1:\r/* perform some activity */\rif (condition) state = STATE2; else state = STATE3;\r对于看代码的人说，这和包含下面小代码块的函数没有多大区别：\nLABEL1:\r/* perform some activity */\rif (condition) goto LABEL2; else goto LABEL3;\r是的，这两个函数的结构在视觉上是一样的，而对于函数中实现的算法，两个函数都一样不利于查看。因为你使用协程的宏而炒你鱿鱼的人，一样会因为你写的函数是由小块的代码和 goto 语句组成而吼着炒了你。只是这次他们没有冤枉你，因为像那样设计的函数会严重扰乱算法的结构。\n**编程规范的目标就是为了代码清晰。**如果将一些重要的东西，像 switch、return 以及 case 语句，隐藏到起 “障眼” 作用的宏中，从编程规范的角度讲，可以说你扰乱了程序的语法结构，并且违背了代码清晰这一要求。但是我们这样做是为了突出程序的算法结构，而算法结构恰恰是看代码的人更想了解的。\n**任何编程规范，坚持牺牲算法清晰度来换取语法清晰度的，都应该重写。**如果你的上司因为使用了这一技巧而解雇你，那么在保安把你往外拖的时候要不断告诉他这一点。\n原文作者最后给出了一个 MIT 许可证的 coroutine.h 头文件。值得一提的是，正如文中所说，这种协程实现方法有个使用上的局限，就是协程调度状态的保存依赖于 static 变量，而不是堆栈上的局部变量，实际上也无法用局部变量（堆栈）来保存状态，这就使得代码不具备可重入性和多线程应用。后来作者补充了一种技巧，就是将局部变量包装成函数参数传入的一个虚构的上下文结构体指针，然后用动态分配的堆来 “模拟” 堆栈，解决了线程可重入问题。但这样一来反而有损代码清晰，比如所有局部变量都要写成对象成员的引用方式，特别是局部变量很多的时候很麻烦，再比如宏定义 malloc/free 的玩法过于托大，不易控制，搞不好还增加了被炒鱿鱼的风险（只不过这次是你活该）。\n我个人认为，既然协程本身是一种单线程的方案，那么我们应该假定应用环境是单线程的，不存在代码重入问题，所以我们可以大胆地使用 static 变量，维持代码的简洁和可读性。事实上我们也不应该在多线程环境下考虑使用这么简陋的协程，非要用的话，前面提到 glibc 的 ucontext 组件也是一种可行的替代方案，它提供了一种协程私有堆栈的上下文，当然这种用法在跨线程上也并非没有限制，请仔细阅读联机文档。\nProtothreads 的上下文 感谢 Simon Tatham 的淳淳教诲，接下来我们可以 hack 一下源码了。先来看看实现 protothreads 的数据结构， 实际上它就是协程的上下文结构体，用以保存状态变量，相信你很快就明白为何它的 “堆栈” 只有 2 个字节：\nstruct pt {\rlc_t lc;\r}\r里面只有一个 short 类型的变量，实际上它是用来保存上一次出让点的程序计数器。这也映证了协程比线程的灵活之处，就是协程可以是 stackless 的，如果需要实现的功能很单一，比如像生产者 - 消费者模型那样用来做事件通知，那么实际上协程需要保存的状态变量仅仅是一个程序计数器即可。像 python generator 也是 stackless 的，当然实现一个迭代生成器可能还需要保留上一个迭代值，前面 C 的例子是用 static 变量保存，你也可以设置成员变量添加到上下文结构体里面。如果你真的不确定用协程调度时需要保存多少状态变量，那还是用 ucontext 好了，它的上下文提供了堆栈和信号，但是由用户负责分配资源，详细使用方法见联机文档。。\ntypedef struct ucontext {\rstruct ucontext_t *uc_link;\rsigset_t uc_sigmask;\rstack_t uc_stack;\r...\r} ucontext_t;\rProtothreads 的原语和组件 有点扯远了，回到 protothreads，看看提供的协程 “原语”。有两种实现方法，在 ANSI C 下，就是传统的 switch-case 语句：\n#define LC_INIT（s） s = 0; // 源码中是有分号的，一个低级 bug，啊哈～\r#define LC_RESUME(s) switch (s) { case 0:\r#define LC_SET(s) s = __LINE__; case __LINE__:\r#define LC_END(s) }\r但这种 “原语” 有个难以察觉的缺陷：**就是你无法在 LC_RESUME 和 LC_END （或者包含它们的组件）之间的代码中使用 switch-case 语句，因为这会引起外围的 switch 跳转错误！**为此，protothreads 又实现了基于 GNU C 的调度 “原语”。在 GNU C 下还有一种语法糖叫做标签指针，就是在一个 label 前面加 \u0026amp;\u0026amp;（不是地址的地址，是 GNU 自定义的符号），可以用 void 指针类型保存，然后 goto 跳转：\ntypedef void * lc_t；\r#define LC_INIT(s) s = NULL\r#define LC_RESUME(s) \\\rdo { \\\rif (s != NULL) { \\\rgoto *s; \\\r}\r} while (0)\r#define LC_CONCAT2(s1, s2) s1##s2\r#define LC_CONCAT(s1, s2) LC_CONCAT2(s1, s2)\r#define LC_SET(s) \\\rdo { \\\rLC_CONCAT(LC_LABEL, __LINE__): \\\r（s） = \u0026\u0026LC_CONCAT(LC_LABEL, __LINE__); \\\r} while (0)\r好了，有了前面的基础知识，理解这些 “原语” 就是小菜一叠，下面看看如何建立“组件”，同时也是 protothreads API，我们先定义四个退出码作为协程的调度状态机：\n#define PT_WAITING 0\r#define PT_YIELDED 1\r#define PT_EXITED 2\r#define PT_ENDED 3\r下面这些 API 可直接在应用程序中调用：\n/* 初始化一个协程，也即初始化状态变量 */\r#define PT_INIT(pt) LC_INIT((pt)-\u003elc)\r/* 声明一个函数，返回值为 char 即退出码，表示函数体内使用了 proto thread，（个人觉得有些多此一举） */\r#define PT_THREAD(name_args) char name_args\r/* 协程入口点， PT_YIELD_FLAG=0表示出让，=1表示不出让，放在 switch 语句前面，下次调用的时候可以跳转到上次出让点继续执行 */\r#define PT_BEGIN(pt) { char PT_YIELD_FLAG = 1; LC_RESUME((pt)-\u003elc)\r/* 协程退出点，至此一个协程算是终止了，清空所有上下文和标志 */\r#define PT_END(pt) LC_END((pt)-\u003elc); PT_YIELD_FLAG = 0; \\\rPT_INIT(pt); return PT_ENDED; }\r/* 协程出让点，如果此时协程状态变量 lc 已经变为 __LINE__ 跳转过来的，那么 PT_YIELD_FLAG = 1，表示从出让点继续执行。 */\r#define PT_YIELD(pt) \\\rdo { \\\rPT_YIELD_FLAG = 0; \\\rLC_SET((pt)-\u003elc); \\\rif(PT_YIELD_FLAG == 0) { \\\rreturn PT_YIELDED; \\\r} \\\r} while(0)\r/* 附加出让条件 */\r#define PT_YIELD_UNTIL(pt, cond) \\\rdo { \\\rPT_YIELD_FLAG = 0; \\\rLC_SET((pt)-\u003elc); \\\rif((PT_YIELD_FLAG == 0) || !(cond)) { \\\rreturn PT_YIELDED; \\\r} \\\r} while(0)\r/* 协程阻塞点(blocking),本质上等同于 PT_YIELD_UNTIL，只不过退出码是 PT_WAITING，用来模拟信号量同步 */\r#define PT_WAIT_UNTIL(pt, condition) \\\rdo { \\\rLC_SET((pt)-\u003elc); \\\rif(!(condition)) { \\\rreturn PT_WAITING; \\\r} \\\r} while(0)\r/* 同 PT_WAIT_UNTIL 条件反转 */\r#define PT_WAIT_WHILE(pt, cond) PT_WAIT_UNTIL((pt), !(cond))\r/* 协程调度，调用协程 f 并检查它的退出码，直到协程终止返回 0，否则返回 1。 */\r#define PT_SCHEDULE(f) ((f) \u003c PT_EXITED)\r/* 这用于非对称协程，调用者是主协程，pt 是和子协程 thread （可以是多个）关联的上下文句柄，主协程阻塞自己调度子协程，直到所有子协程终止 */\r#define PT_WAIT_THREAD(pt, thread) PT_WAIT_WHILE((pt), PT_SCHEDULE(thread))\r/* 用于协程嵌套调度，child 是子协程的上下文句柄 */\r#define PT_SPAWN(pt, child, thread) \\\rdo { \\\rPT_INIT((child)); \\\rPT_WAIT_THREAD((pt), (thread)); \\\r} while(0)\r暂时介绍这么多，用户还可以根据自己的需求随意扩展组件，比如实现信号量，你会发现脱离了操作系统环境下的信号量竟是如此简单：\nstruct pt_sem {\runsigned int count;\r};\r#define PT_SEM_INIT(s, c) (s)-\u003ecount = c\r#define PT_SEM_WAIT(pt, s) \\\rdo { \\\rPT_WAIT_UNTIL(pt, (s)-\u003ecount \u003e 0); \\\r--(s)-\u003ecount; \\\r} while(0)\r#define PT_SEM_SIGNAL(pt, s) ++(s)-\u003ecount\r这些应该不需要我多说了吧，呵呵，让我们回到最初例举的生产者 - 消费者模型，看看 protothreads 表现怎样。\nProtothreads 实战 #include \"pt-sem.h\"\r#define NUM_ITEMS 32\r#define BUFSIZE 8\rstatic struct pt_sem mutex, full, empty;\rPT_THREAD(producer(struct pt *pt))\r{\rstatic int produced;\rPT_BEGIN(pt);\rfor (produced = 0; produced \u003c NUM_ITEMS; ++produced) {\rPT_SEM_WAIT(pt, \u0026full);\rPT_SEM_WAIT(pt, \u0026mutex);\radd_to_buffer(produce_item());\rPT_SEM_SIGNAL(pt, \u0026mutex);\rPT_SEM_SIGNAL(pt, \u0026empty);\r}\rPT_END(pt);\r}\rPT_THREAD(consumer(struct pt *pt))\r{\rstatic int consumed;\rPT_BEGIN(pt);\rfor (consumed = 0; consumed \u003c NUM_ITEMS; ++consumed) {\rPT_SEM_WAIT(pt, \u0026empty);\rPT_SEM_WAIT(pt, \u0026mutex);\rconsume_item(get_from_buffer());\rPT_SEM_SIGNAL(pt, \u0026mutex);\rPT_SEM_SIGNAL(pt, \u0026full);\r}\rPT_END(pt);\r}\rPT_THREAD(driver_thread(struct pt *pt))\r{\rstatic struct pt pt_producer, pt_consumer;\rPT_BEGIN(pt);\rPT_SEM_INIT(\u0026empty, 0);\rPT_SEM_INIT(\u0026full, BUFSIZE);\rPT_SEM_INIT(\u0026mutex, 1);\rPT_INIT(\u0026pt_producer);\rPT_INIT(\u0026pt_consumer);\rPT_WAIT_THREAD(pt, producer(\u0026pt_producer) \u0026 consumer(\u0026pt_consumer));\rPT_END(pt);\r}\r源码包中的 example-buffer.c 包含了可运行的完整示例，我就不全部贴了。整体框架就是一个 asymmetric coroutines，包括一个主协程 driver_thread 和两个子协程 producer 和 consumer ，其实不用多说大家也懂的，代码非常清晰直观。我们完全可以通过单线程实现一个简单的事件处理需求，你可以任意添加数十万个协程，几乎不会引起任何额外的系统开销和资源占用。唯一需要留意的地方就是没有一个局部变量，因为 protothreads 是 stackless 的，但这不是问题，首先我们已经假定运行环境是单线程的，其次在一个简化的需求下也用不了多少 “局部变量”。如果在协程出让时需要保存一些额外的状态量，像迭代生成器，只要数目和大小都是确定并且可控的话，自行扩展协程上下文结构体即可。\n当然这不是说 protothreads 是万能的，它只是贡献了一种模型，你要使用它首先就得学会适应它。下面列举一些 protothreads 的使用限制：\n由于协程是 stackless 的，尽量不要使用局部变量，除非该变量对于协程状态是无关紧要的，同理可推，协程所在的代码是不可重入的。\n如果协程使用 switch-case 原语封装的组件，那么禁止在实际应用中使用 switch-case 语句，除非用 GNU C 语法中的标签指针替代。\n一个协程内部可以调用其它例程，比如库函数或系统调用，但必须保证该例程是非阻塞的，否则所在线程内的所有协程都将被阻塞。毕竟线程才是执行的最小单位，协程不过是按 “时间片轮度” 的例程而已。\n官网上还例举了更多\r实例，都非常实用。另外，一个叫 Craig Graham 的工程师扩展了 pt.h，使得 protothreads 支持 sleep/wake/kill 等操作，文件在此 graham-pt.h。\n协程库 DIY 攻略 看到这里，手养的你是否想迫不及待地 DIY 一个协程组件呢？哪怕很多动态语言本身已经支持了协程语义，很多 C 程序员仍然倾向于自己实现组件，网上很多开源代码底层用的主要还是 glibc 的 ucontext 组件，毕竟提供堆栈的协程组件使用起来更加通用方便。你可以自己写一个调度器，然后模拟线程上下文，再然后…… 你就能搞出一个跨平台的 COS 了（笑）。GNU Pth 线程库就是这么实现的，其原作者德国人 Ralf S. Engelschall （又是个开源大牛，还写了 OpenSSL 等许多作品）就写了一篇\r论文教大家如何实现一个线程库。另外 protothreads 官网上也有一大堆\r推荐阅读。Have fun！\n","description":"协程 (coroutine) 顾名思义就是“协作的例程”（co-operative routines）。跟具有操作系统概念的线程不一样，协程是在用户空间利用程序语言的语法语义就能实现逻辑上类似多任务的编程技巧。","id":42,"section":"posts","tags":["c","coroutine"],"title":"一个“蝇量级” C 语言协程库","uri":"https://starifly.github.io/posts/a-c-coroutine/"},{"content":"git的分支管理十分强大，本文主要涉及git中分支的查看和最基本的分支管理操作，主要包括分支的创建、切换、合并、衍合以及分支的推送和拉取等。同时还介绍了如何删除本地分支以及远程分支。\n阅读前面的博文有助于理解本文：\n《\rGit基本操作》 《\rGit的撤销更改》 查看分支 //查看当前仓库的分支，分支前的*表示当前所处的分支 $ git branch //查看远程仓库的分支 $ git branch -r //查看所有分支 $ git branch -a //查看所有分支和该分支上最后的一次提交 $ git branch -v //查看已经合入当前分支的所有分支 $ git branch --merged //查看未被合入分支 $ git branch --no-merged //查看当前分支对应的远程分支（追踪关系） $ git branch -vv //可视化目前的分支状态 $ git log --oneline --graph --all 分支创建和切换 我们可以使用git branch \u0026lt;分支名\u0026gt;命令来创建一个分支，然后使用git checkout \u0026lt;分支名\u0026gt;来切换分支。\n如果觉得麻烦，我们可以使用一个简便的命令创建并切换到该分支上：git checkout -b \u0026lt;分支名\u0026gt;。\n也可以使用git checkout -命令来切换回上一个分支\n分支合并 假设当前仓库存在两个分支，我们先切换到master分支，然后通过git merge --no-ff \u0026lt;分支名\u0026gt;命令将分支合并到master分支上。\n其中--no-ff表示强行关闭fast-forward方式，fast-forward方式表示当条件允许时，git直接把HEAD指针指向合并分支的头，完成合并，这种方式合并速度快，但是在整个过程中没有创建commit，所以如果当我们删除掉这个分支时就再也找不回来了，因此在这里我们将之关闭。\n如果git merge合并分支时出现冲突，先重新编辑冲突文件，编辑完成之后，再执行git add和git commit即可。\n分支衍合 衍合（rebase）又称变基，其实也是分支合并的一种方式，既然也是合并，那它跟上文讲的合并（merge）有什么区别呢？\n假如我们现在有master和experiment两条分支：\n现在想把experiment分支合并到master分支上来，最容易的方法是 merge 命令。它会把两个分支的最新快照（C3 和 C4）以及二者最近的共同祖先（C2）进行三方比较合并，合并的结果是生成一个新的快照（并提交）。\n其实，还有一种方法：你可以提取在 （C4）中引入的补丁和修改，然后在（C3）的基础上应用一次。 在git中，这种操作就叫做变基。 你可以使用 rebase 命令将提交到某一分支上的所有修改都移至另一分支上，就好像“重新播放”一样。\n具体做法：\n1.检出experiment分支，对master进行衍合\n$ git checkout experiment $ git rebase master 它的原理是首先找到这两个分支（即当前分支 experiment、变基操作的目标基底分支 master）的最近共同祖先（C2），然后对比当前分支相对于该祖先的历次提交，提取相应的修改并存为临时文件，然后将当前分支指向目标基底（C3）, 最后以此将之前另存为临时文件的修改依序应用。\n可以看出，在衍合得过程中，experiment分支会丢弃现有的提交（C4），然后相应地新建一些内容一样但实际上不同的提交（C4’）。\n2.再回到master分支，进行一次快进合并\n$ git checkout master $ git merge experiment 在衍合过程中有可能会发生冲突，方案有两种：一种是通过git rebase --abort命令直接退回到之前的状态，另一种就是解决冲突继续合并。\n解决冲突后，要git add冲突的文件，标识冲突解决，然后再执行git rebase --continue，注意git add后不要执行git commit。\n分支的推送和拉取 git push与git pull是一对推送/拉取分支的git命令。\ngit push 使用本地的对应分支来更新对应的远程分支。\n$ git push \u0026lt;远程主机名\u0026gt; \u0026lt;本地分支名\u0026gt;:\u0026lt;远程分支名\u0026gt; 如果省略远程分支名，则表示将本地分支推送与之存在”追踪关系”的远程分支(通常两者同名)，如果该远程分支不存在，则会被新建。\n$ git push origin master 上面命令表示，将本地的master分支推送到origin主机的master分支。如果后者不存在，则会被新建。\n如果省略本地分支名，则表示删除指定的远程分支，因为这等同于推送一个空的本地分支到远程分支，这条命令是删除远程master分支。\n$ git push origin :\u0026lt;分支名\u0026gt; //等同于 git push origin --delete \u0026lt;分支名\u0026gt; 如果当前分支与远程分支之间存在追踪关系（即分支名相同），则本地分支和远程分支都可以省略。\n$ git push origin 上面命令表示，将当前分支推送到origin主机的对应分支。\n如果当前分支只有一个追踪分支，那么主机名都可以省略。\n$ git push 如果当前分支与多个主机存在追踪关系，则可以使用-u选项指定一个默认主机，这样后面就可以不加任何参数使用git push。\n$ git push -u origin master 上面命令将本地的master分支推送到origin主机，同时指定origin为默认主机，后面就可以不加任何参数使用git push了。\n不带任何参数的git push，默认只推送当前分支，这叫做simple方式。此外，还有一种matching方式，会推送所有有对应的远程分支的本地分支。Git 2.0版本之前，默认采用matching方法，现在改为默认采用simple方式。如果要修改这个设置，可以采用git config命令。\n$ git config --global push.default matching //或者 $ git config --global push.default simple 更详细的push.default属性，请参考《\rGit push与pull的默认行为》\n还有一种情况，就是不管是否存在对应的远程分支，将本地的所有分支都推送到远程主机，这时需要使用–all选项。\n$ git push --all origin 上面命令表示，将所有本地分支都推送到origin主机。\ngit pull 取回远程主机某个分支的更新，再与本地的指定分支合并。\ngit pull与git push操作的目的相同，但是操作的目标相反。命令格式如下：\ngit pull \u0026lt;远程主机名\u0026gt; \u0026lt;远程分支名\u0026gt;:\u0026lt;本地分支名\u0026gt; 例如：\ngit pull origin master:my-test 上面的命令是将origin厂库的master分支拉取并合并到本地的my-test分支上。\n如果省略本地分支，则将自动合并到当前所在分支上。如下：\ngit pull origin master 在某些场合，git会自动在本地分支与远程分支之间，建立一种追踪关系(tracking)。比如，在git clone的时候，所有本地分支默认与远程主机的同名分支，建立追踪关系，也就是说，本地的master分支自动”追踪”origin/master分支。\n以下也能自动建立追踪关系：\n检出时：git checkout -b master origin/master 推送时：git push -u origin \u0026lt;远程分支名\u0026gt;或git push --set-upstream origin \u0026lt;远程分支名\u0026gt; 注意，上述使用的前提：\n检出的本地分支必须和远程分支同名才能自动建立追踪关系 推送时，如果远程没有同名关联分支，则会推送失败。 git也允许手动建立追踪关系。\n$ git branch --set-upstream master origin/next 上面命令指定master分支追踪origin/next分支。\n如果当前分支与远程分支存在追踪关系，git pull就可以省略远程分支名。\n$ git pull origin 上面命令表示，本地的当前分支自动与对应的origin主机”追踪分支”(remote-tracking branch)进行合并。\n如果当前分支只有一个追踪分支，连远程主机名都可以省略。\n$ git pull 上面命令表示，当前分支自动与唯一一个追踪分支进行合并。\n如果合并需要采用rebase模式，可以使用--rebase选项。\n$ git pull --rebase \u0026lt;远程主机名\u0026gt; \u0026lt;远程分支名\u0026gt;:\u0026lt;本地分支名\u0026gt; 分支的删除 有时候当我们完成功能等分支的开发时，我们会把它合并到上一层分支，此时我们不再需要这个功能分支了，我们可以通过git branch -d \u0026lt;分支名\u0026gt;来删除它。\n实际上，对分支的删除只是删除的指向该commit号的指针，并不会删除其相关的提交号, 在日志中仍然可以找到之前的commit记录，也仍然可以在该commit上创建新的分支。\n如果你还想删除远程分支，要使用git push origin --delete \u0026lt;分支名\u0026gt;\nReference https://segmentfault.com/a/1190000011927868#articleHeader5 https://git-scm.com/book/zh/v2/Git-%E5%88%86%E6%94%AF-%E5%8F%98%E5%9F%BA https://www.yiibai.com/git/git_push.html#article-start https://www.yiibai.com/git/git_pull.html ","description":"","id":43,"section":"posts","tags":["git"],"title":"Git分支管理","uri":"https://starifly.github.io/posts/git-branch/"},{"content":"前文《\rGit基本操作》介绍了Git的基本概念和一些基本操作。\n本文将介绍Git四个阶段的撤销更改：\n工作区的代码撤销 add到暂存区的代码想撤销 提交到本地仓库的代码想撤销 推送到远程仓库的代码想撤销 工作区代码的撤销 如果我们在工作区修改了文件，状态会变为Modified，这时忽然发现写错了想回到一开始未修改状态，愚蠢的办法是打开文件一个一个的去恢复，其实可以通过git checkout .命令来一键撤销工作区的代码修改，\n当然也可以通过git checkout -- \u0026lt;filename\u0026gt;命令来指定撤销的文件。\nadd到暂存区的代码想撤销 如果add到了暂存区，你发现了错误想要撤销，可以执行以下两个步骤：\ngit reset HEAD or git reset 同以上\u0026rsquo;工作区代码撤销' git reset的参数，其值可以是hard、mixed及soft，mixed实现的是add后的回撤，上文中关于暂存未提交回撤中的git reset其实就是默认用的mixed参数，soft请听下文分解。\n三者的区别：\n模式 HEAD的位置 索引 工作树 soft 修改 不修改 不修改 mixed 修改 修改 不修改 hard 修改 修改 修改 提交到本地仓库的代码想撤销 如果执行了git commit命令把代码提交到了本地仓库，如果你后悔了，怎么办？不要着急，同样也可以撤销。\n有以下两种情形：\n回撤到指定版本号 可以利用git reset --hard \u0026lt;版本号\u0026gt;命令来实现版本回退，该命令中的版本号有几种不同的写法：\n1.可以使用HEAD^来描述版本，一个^表示前一个版本，两个^^表示前两个版本，以此类推。\n2.也可以使用数字来代替^，比如说前100个版本可以写作HEAD~100。\n3.也可以直接写版本号，表示跳转到某一个版本处（通过git log查看当前提交日志）。\n回撤到add之后未commit之前的状态 soft实现的是commit之后回撤到add之后未commit之前的状态，所以，像在commit之后发现少提交了一个文件等场景下，我们可以利用git reset --soft HEAD^命令来回撤。\npush到远程仓库的代码想撤销 有时，在git push之后，才发现还有一些代码需要进行很小的改动，这些改动在原则上不应该作为一次新的提交。\n这时，我们需要撤销这次推送(git push)与提交(git commit)，然后进行代码修改，再重新进行提交和推送。\n为了实现这个目的，需要进行三步操作:\n1.撤销提交信息 通过执行git reset --soft \u0026lt;版本号\u0026gt;重置至指定版本的提交，达到撤销提交的目的。\n参数soft指的是：保留当前工作区，以便重新提交。\n还可以选择参数hard，会撤销相应工作区的修改，而且有可能意外造成代码丢失，所以一定要谨慎使用。\n然后，通过git log确认是否成功撤销。\n2.撤销 通过git push origin master --force强制提交当前版本号，以达到撤销版本号的目的。\n当他人在仓库的远程副本的同一分支上进行改动后，会发生强制推送的风险。当你强制推送已重写的历史记录时，某些提交将会丢失。这是git push --force-with-lease出现的原因 - 如果远程分支已更新，它不会允许你执行强制推送，这将确保你不会丢弃他人的工作。\n必须添加参数force进行强制提交，否则会提交失败，并报错。\n报错原因：本地项目版本号低于远端仓库版本号。\n3.修改代码，重新提交和推送 //修改代码，添加修改 $ git add . //重新提交 $ git commit -m \u0026#34;message\u0026#34; //重新推送 $ git push origin master Reference Git中的各种后悔药 ","description":"","id":44,"section":"posts","tags":["git"],"title":"Git的撤销更改","uri":"https://starifly.github.io/posts/git-reset/"},{"content":"\n基本概念 4个区 如上图所示，git有4个区，和svn有很大的不同，git中引入了暂存区/缓存区(Stage/Index)的概念\n工作区(Workspace) 暂存区(Stage/Index) 本地仓库(Repository) 远程仓库(Remote) 在svn中我们都是直接将文件提交到版本仓库中去，而在git中，则多了一层关卡。\n工作区很好理解，就是我们能看到的工作目录，就是本地的文件夹。\n工作区最原始的状态是未修改状态(Origin)，如果修改了某些文件就会变成已修改(Modified)。\n我们要通过git add命令先将他们添加到暂存区中，这时就会产生已暂存(Staged)状态。\ngit commit命令则可以将暂存区中的文件提交到本地仓库中去，状态变为已提交(Committed)。\n最后通过git push推送到远程仓库，推送成功状态变为已推送(Pushed)。\n基本操作 仓库初始化 仓库的初始化有两种方式：\n一种是直接从远程仓库克隆 另一种则是直接从当前目录初始化 从当前目录初始化的方式很简单，直接执行如下命令:\n$ git init 执行完成后当前目录下会多出一个.git的隐藏文件夹，所有git需要的数据和资源都存放在该目录中。\n然后正常的add和commit之后，\n$ git remote add origin \u0026lt;url\u0026gt; $ git push -u origin master 查看仓库状态 可以通过git status命令来查看仓库中文件的状态。\ngit status -s : 文件状态的简写（M - 修改， A - 添加， D - 删除， R - 重命名，?? - 未追踪）。\n提交命令 一种是直接通过git commit -m \u0026quot;备注信息\u0026quot;命令 如果要写的备注很多，可以通过git commit来打开指定编辑器，然后在编辑器中输入备注信息。 在.gitconfig文件中 [core] 段中加上 editor=vim来修改git提交信息到编辑器为vim\n如图所示，在vim编辑器中按照既定的格式编辑内容，编辑完成后保存退出，此时文件就commit成功了。如果临时改变主意不想提交了，删除备注信息后保存退出，\n此时提交就会终止。\n提交成功之后，我们可以通过如下命令重新编辑上次提交的备注信息：\n$ git commit --amend 查看提交日志 我们可以使用 git log 命令来查看历史提交。git log 命令因为其后边参数的多样性，使用起来是比较灵活而强大的，并且比较实用。\n下面列车一些常用的命令：\n使用git log命令我们可以查看以往仓库中提交的日志 使用git log --abbrev-commit命令缩短版本号 使用git log --pretty=short命令查询简略信息 使用git log --stat命令查看简化版的diff日志信息 使用git log --name-only命令仅在提交信息后显示已修改的文件清单 使用git log --name-status命令显示新增、修改、删除的文件清单 使用git log --relative-date命令显示较短的相对时间 在git log后面加上文件名查看指定文件的提交日志，如果还想查看提交时文件的变化，加上-p参数即可。 使用git log -p -1来查看最近一次提交的差异 使用git log --graph命令可以让log以更直观的方式来展示 我们还可以使用git log --pretty来定义显示格式，如：git log --graph --pretty=format:\u0026quot;%h - %an, %ar : %s\u0026quot;， \u0026ndash;pretty=format:后边跟着的是格式化的字符串。\n其中 %h 表示简化版的Hash值， %an 表示作者名字（Author Name）, %ar 表示多久以前提交的，%s 则是提交信息。\n查看更改前后的差异 已修改，未暂存\n使用git diff命令我们可以查看工作区和暂存区的差异。\n已暂存，未提交\n如果我们执行git add .把修改提交到暂存区，然后再执行git diff，你会发现没有任何信息输出。这说明git diff这个命令只检查我们的工作区和暂存区之间的差异。\n如果我们想看到暂存区和本地仓库之间的差异，就需要加一个参数git diff --cached。\n或者通过命令git diff HEAD来查看（实际上是工作区与当前分支最近一次commit之间的差异）。\n已提交，未推送\n现在，如果通过命令git commit把修改从暂存区提交到本地仓库，然后再执行git diff \u0026ndash;cached，没有差异，执行git diff master origin/master，可以看到差异\n删除文件 当我们需要删除工作区和暂存区上的文件，可以执行git rm \u0026lt;filename\u0026gt;命令，该命令等同于以下两个命令：\n//直接在文件管理器中把文件删了 $ rm file //提交到工作区 $ git add file //或者 git rm file 当我们希望某个文件不被版本控制，但是本地又需要使用，可以使用git rm --cached \u0026lt;filename\u0026gt;。\n重命名 可以通过git mv \u0026lt;SourceFile\u0026gt; \u0026lt;RenameFile\u0026gt;命令将SourceFile重命名为RenameFile，该命令等同于以下三个命令：\n$ mv SourceFile RenameFile $ git rm SourceFile $ git add RenameFile git add -N 你可以用git add -N \u0026lt;filename\u0026gt;（“通知”）来告诉Git你想把新添加的文件包含在提交中在你第一次实际提交之前，使用该命令之后执行git diff命令就可以查看到更改。\ngit add -p 交互式提交，询问你是否愿意将它提交，跳过，或者推迟决定（还有其他一些更强大的选项，你可以通过在运行这命令后选择？来查看）。git add -p是一个神奇的工具来生产结构良好的提交。\ngit checkout -p 与git add -p类似，git checkout命令将使用 --patch 或 -p 选项，这会使 git 在本地工作副本中展示每个“大块”的改动，并允许丢弃对应改动 —— 简单地说就是恢复本地工作副本到你改变之前的状态。\n压缩提交历史 git rebase -i命令可以实现提交历史的压缩。比如我们在开发某一个功能时，提交了很多次，当所有功能都写完时，想将这些提交压缩为一个，就可以使用该命令。\n如：通过执行git rebase -i HEAD~2命令来压缩前两个版本的提交历史，会自动打开一个vim编辑器，压缩之后，最新一次的提交日志就没了，但是数据还在。\n基于时间修改的指南 这个功能在某些时候会变得十分有用，比如当你处理最新出现的 bug，自言自语道：“这个功能明明昨天还是好好的，到底又改了些什么”，不用盯着满屏的 git 日志的输出试图弄清楚什么时候更改了提交，您只需运行git diff HEAD@{yesterday}，会看到从昨天以来的所有修改，这也适用于较长的时间段（例如 git diff HEAD@{'2 months ago'}） ，以及一个确切的日期（例如git diff HEAD@{'2010-01-01 12:00:00'}）。\n您还可以将这些基于日期的修改参数与使用修正参数的任何 Git 子命令一起使用。在 gitrevisions 手册页中有关于具体使用哪种格式的详细信息。\n全知的 reflog 你是不是试过在 rebase 时干掉过某次提交，后来又发现你需要保留这次提交的一些东西？你可能觉得这些提交的东西已经永远找不回来了，只能从头再来了。其实不然，但如果你在本地工作副本中提交了，提交就会进入到 \u0026ldquo;引用日志\u0026rdquo; ，你仍然可以访问到。\n运行 git reflog 将在本地工作副本中显示当前分支的所有活动的列表，并为您提供每个提交的 SHA1 值。一旦发现你 rebase 时放弃的那个提交，你可以运行 git checkout 来检出该次提交，复制好你需要的信息，然后再运行 git checkout HEAD 返回到分支最新的提交去。\nReference https://mp.weixin.qq.com/s/S1T4wy3srmLvXgIjvpVEwg https://opensource.com/article/18/4/git-tips Extended Git关联远程仓库 Git工作区储藏兼谈分支管理中的一个小问题 Git标签管理 git生成ssh key 避免每次push都要输入账号密码 你可能不知道的关于 Git stash 的技巧 ","description":"","id":45,"section":"posts","tags":["git"],"title":"Git基本操作","uri":"https://starifly.github.io/posts/git-base/"},{"content":"git add -A、git add .和git add -u在功能上看似相近，但是有细微的差别，而且功能会随着git版本的不同而不同。\n区别 Git Version 1.x:\nGit Version 2.x:\n总结 git add -A就不用介绍了，它在任何情况下都是包括所有的变动。 git add .在git版本是2.x的时候等同于git add -A，而当版本是1.x的时候不包括被删除的文件。 git add -u仅监控已经被add的文件（即tracked file），不会提交新文件（untracked file）。 希望本文对你有所帮助。\n","description":"","id":46,"section":"posts","tags":["git"],"title":"git add -A 和 git add . 的区别","uri":"https://starifly.github.io/posts/git-add-difference/"},{"content":"之前\r《通过github和caddy实现hugo的自动部署》，使用的是 caddy 的 http.git 插件。最近发现，那方法不管用了，不知道原因。\n所以决定使用传统的 webhook 。原理是：在 vps 运行 webhook 监听程序，github 收到 push 事件后，\n就通知该监听程序，由该监听程序执行相应的命令。现在记录一下。\n安装 webhook sudo apt install webhook 也可以到该\r项目的github地址手动\r下载。\n编写 webhook 的配置文件 我的itlaws.cn的markdown源码放在/var/www/itlaws.cn\nmkdir /var/www/itlaws.cn/webhook touch itlaws.cn-config.json touch itlaws.cn-deploy.sh chmod +x itlaws.cn-deploy.sh itlaws.cn-config.json内容为：\n[ { \u0026#34;id\u0026#34;: \u0026#34;itlaws.cn-deploy\u0026#34;, \u0026#34;execute-command\u0026#34;: \u0026#34;/var/www/itlaws.cn/webhook/itlaws.cn-deploy.sh\u0026#34;, \u0026#34;command-working-directory\u0026#34;: \u0026#34;/var/www/itlaws.cn\u0026#34; } ] 其中：\nid 是监听程序名称，自定义。 execute-command 是监听程序接收到 github 的通知后执行的命令，自定义。 command-working-directory 指定目录运行上述命令。 更多的参数见官方的\rHook definition。\nitlaws.cn-deploy.sh内容为：\n#!/bin/bash cd /var/www/itlaws.cn \u0026amp;\u0026amp; git pull --recurse-submodules \u0026amp;\u0026amp; rm -rf public \u0026amp;\u0026amp; hugo 测试监听程序 webhook -hooks /var/www/itlaws.cn/webhook/itlaws.cn-config.json -verbose -port=9000 其中：\n-hooks 后面要接配置文件；\n-verbose表示输出日志；\n-port表示监听端口，自定义。\n更多的内容见\rWebhook parameters。\n上述命令将在9000端口运行webhook监听程序，即：\nhttp://itlaws.cn:9000/hooks/itlaws.cn-deploy 在 github 端测试，正常。该监听程序目前是 http ，而不是 https 。\n通过 supervisor 运行监听程序 supervisor是用于启动某种服务并监控该服务，在该服务异常退出时重启该服务。\nsudo apt install supervisor cd /etc/supervisor/conf.d/ touch webhook.conf webhook.conf的内容为：\n[program:webhook] command=/var/www/itlaws.cn/webhook/webhook -hooks /var/www/itlaws.cn/webhook/itlaws.cn-config.json -verbose -port=9000 -secure -cert ssl证书的路径 -key ssl密钥路径 user=www-data directory=/var/www/itlaws.cn/webhook autorestart=true redirect_stderr=true 其中，-secure表示启用 https，-cert 和-key分别是 https 的证书和密钥的路径，从而把监听服务从 http 改为 https：\nhttps://itlaws.cn:9000/hooks/itlaws.cn-deploy 好了，大功告成。\n原文链接\n","description":"","id":47,"section":"posts","tags":["CI","webhook","github","hugo"],"title":"使用Webhook和Github实现hugo的自动部署","uri":"https://starifly.github.io/posts/webhook-github-autodeploy/"},{"content":"之前我写了一个脚本用来自动部署我的 Hugo 博客，今天闲来无事完善了一下这个脚本，使这个脚本更加通用一些。\n脚本路径：\rhttps://github.com/coderzh/coderzh-hugo-blog/blob/master/deploy.py\n原理 deploy.py 会自动执行 hugo 命令生成静态站点，然后将生成的文件拷贝到上层的一个目录里，然后，在那个目录里将文件 push 到你指定的 Git Repository 里。\n使用方法 将 deploy.py 放到你的 Hugo 站点目录。（和 config.yaml 等文件放一起）\n编辑 deploy.py 文件，修改你要部署到的 Git Repository：\nGIT_REPO = [ # [别名, 分支名, Git Repo 路径] [\u0026#39;origin\u0026#39;, \u0026#39;gh-pages\u0026#39;, \u0026#39;git@github.com:coderzh/hugo-blog-deployed.git\u0026#39;], [\u0026#39;gitcafe\u0026#39;, \u0026#39;gh-pages\u0026#39;, \u0026#39;git@gitcafe.com:coderzh/coderzh-hugo-blog.git\u0026#39;], ] # 部署到哪里，相对上一级目录。比如下面的配置，会部署到 ../gh-pages 目录里 DEPLOY_DIR = \u0026#39;gh-pages\u0026#39; 如果你的网站需要指定皮肤，需要在 config 文件中指定 theme 。因为我的脚本在生成静态文件时并不会指定皮肤。\ntheme: \u0026#34;rapid\u0026#34; 第一次执行，使用 first 参数，它会做一些初始化的操作。并使用 -t 表示只是测试一下，并不会真的 push 。\npython deploy.py first -t 中间可能需要输入密码，如果是自动化部署，可在 Git Repo 里添加一个没有密码的 SSH Key 。\n如果一切正常，切换到 DEPLOY_DIR 目录，git log 看看 commit 记录是否正常。如果一切也如你所愿。则可以把 -t 参数去掉重新执行一遍，执行真的 push 操作：\npython deploy.py first 执行完成后，应该已经将生成的静态页面自动 push 到了你指定的 GIT_REPO 里。\n之后如需再次手工部署，只需要使用 manual 参数，速度会快很多：\npython deploy.py manual 如果你想通过 webhook 来自动部署，使用 auto 参数，这样在执行 deploy.py 时，会使用 Git 自动更新你当前的 Hugo 站点目录 ，然后部署：\npython deploy.py auto That\u0026rsquo;s all, 祝你好运！\n原文链接\n","description":"","id":48,"section":"posts","tags":["CI","hugo"],"title":"Hugo 自动化部署脚本","uri":"https://starifly.github.io/posts/hugo-deploy-script/"},{"content":"Vim Tips.\nGeneral 操作 说明 i 光标前 a 光标后 I 行首 A 行尾 x 删除光标所在字符 D 删除从当前光标到本行末尾的字符 r 替换光标处的字符 U 取消当前行中所有的改动 = 自动缩进 . (点)会重做最后一个命令 V 按行选择 Jump 操作 说明 0 跳到第一列 ^ 跳到本行第一个非空字符 $ 跳到本行末尾 g_ 跳到本行最后一个非空字符 w 跳到下一个单词的开头 e 跳到这个单词的末尾 W 跳到下一个由空格分隔的单词的开头 E 跳到这个由空格分隔的单词的末尾 | 跳到当前光标的下一个相同单词的地方 | 跳到当前光标的上一个相同单词的地方 `. | 返回最后一次编辑位置\n[[ | 到函数头\n[{ | 到块开始位置\n]} | 到块结束位置\ngf | 打开以光标所在字符串为文件名的文件\ngd | 跳转到局部变量的定义处\ngD | 跳转到全局变量的定义处，从当前文件开头开始搜索\ng] | 展示匹配tags的文件\nE | INSERT 模式下移动光标到行尾\nB | INSERT 模式下光标左移一位\nctrl+w-f | 同gf，不过会水平分割一个窗口\nctrl-^ | 跳转到上一个编辑的文件\nPattern 操作 说明 :/ pattern 单词前加空格，精确匹配 :/^pattern 搜索仅在行首出现 :/pattern$ 搜索仅在行末出现 Buffer 操作 说明 :5,10bd 删除编号5到10的buffer :ba 把所有缓冲区在当前页中 :sb 3 分屏打开编号为3的buffer :vertical sb 3 同上，垂直分屏 File 操作 说明 :1,10 w outfile 1到10行内容写到outfile :1,10 w \u0026raquo; outfile 1到10行内容追加到outfile :r infile 插入文件内容 :23r infile 插入文件23行的内容 :e#或ctrl+^ 回到刚才编辑的文件9（貌似只有:e#有效） :f或ctrl+g 显示文件是否被修改 :f filename 改变编辑的文件名 :e! 强制回到文件原始状态 Object 操作 说明 dip 删除所有临近的空白行 cab 等同于ca) yiB 等同于yi} cw 替换从光标到单词结束 dib 删除括号内的内容 Modify 操作 说明 c[n]l 改写光标后n个字母 c[n]h 改写光标前n个字母 [n]cc 修改当前n行 Replace 操作 说明 :s/old/new 用new替换行中首次出现的old :s/old/new/g 用new替换行中所有出现的old :#,# s/old/new/g 用new替换从第＃行到第＃行中出现的old :% s/old/new/g 用new替换整篇中出现的old 如果替换的范围较大时，在所有的命令尾加一个c命令，强制每个替换需要用户进行确认，例如:s/old/new/c 或s/old/new/gc 相对屏幕移动 通过c-f向下翻页，c-b向上翻页；c-e逐行下滚，c-y逐行上滚。这在几乎所有Unix软件中都是好使的，比如man和less。 H可以移动到屏幕的首行，L到屏幕尾行，M到屏幕中间。 Others 30i+ — 插入30个+组成的分割线 ctrl+v 下移 I // Esc zt可以置顶当前行，通常用来查看完整的下文，比如函数、类的定义。 zz将当前行移到屏幕中部，zb移到底部。 :packadd termdebug，然后Termdebug 可执行程序名字可在vim里启用GDB1。 需要vim版本8.1以上\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","description":"","id":49,"section":"posts","tags":["vim","tips"],"title":"Vim Tips","uri":"https://starifly.github.io/posts/vim-tips/"},{"content":"现在移动平台非常火热，我们在使用C/C++开发一些移动应用的时候，经常可能需要将一些第三方c/c++库编译成对应的iOS/Android/WP/Tizen上面的静态库。\n之前我介绍过\r如何使用CMake来编译跨平台库。\n那种方法有一个局限性，它只针对一些提供了CMake支持或者源码本身不需要configure的库来说，使用会非常方便，但是如果库本身没有提供CMake或者\n在编译之前需要configure的库，使用起来就会很麻烦了。最好的做法，其实是重用它本身提供的编译系统，然后提供交叉编译所需要的一些参数即可。\n问题描述 对于提供了configure/make/make install编译系统的库来说，交叉编译只需要指定\u0026ndash;build, \u0026ndash;host，然后提供CC,CXX,AR,LD,RANLIB,STRIP等编译所需要的变量，最后可能还需要指定arch，CFLAGS，CXXFLAGS和LDFLAGS即可。\n而对于提供了CMake支持的库来说，我们只需要为CMake提供一个交叉编译所需要的toolchain文件即可。一般来说，一个CMake的toolchain文件如下所示:\n# # CMake Toolchain file for crosscompiling on ARM. # # This can be used when running cmake in the following way: # cd build/ # cmake .. -DCMAKE_TOOLCHAIN_FILE=../cross-arm-linux-gnueabihf.cmake # set(CROSS_PATH /opt/gcc-linaro-arm-linux-gnueabihf-4.7-2013.02-01-20130221_linux) # Target operating system name. set(CMAKE_SYSTEM_NAME Linux) # Name of C compiler. set(CMAKE_C_COMPILER \u0026#34;${CROSS_PATH}/bin/arm-linux-gnueabihf-gcc\u0026#34;) set(CMAKE_CXX_COMPILER \u0026#34;${CROSS_PATH}/bin/arm-linux-gnueabihf-g++\u0026#34;) # Where to look for the target environment. (More paths can be added here) set(CMAKE_FIND_ROOT_PATH \u0026#34;${CROSS_PATH}\u0026#34;) # Adjust the default behavior of the FIND_XXX() commands: # search programs in the host environment only. set(CMAKE_FIND_ROOT_PATH_MODE_PROGRAM NEVER) # Search headers and libraries in the target environment only. set(CMAKE_FIND_ROOT_PATH_MODE_LIBRARY ONLY) 这里重要的是指定编译器，sysroot和编译时的CFLAGS，CXXFLAGS等参数。具体就不在这里展开了。\n为什么我要研究这些东西呢？因为cocos2d-x现在依赖20多个第三方库，如果全部采用源码的形式，那开发编译的时间会变得非常长。为了减少编译等待时间，我们一般会把第三方库事先编译好，形成一堆.a文件。然后在程序运行的时候，指定链接参数给链接器就ok了。\n但是，目前我们这些库都是临时编译的。当时编译的脚本没有保留下来了，也就是不清楚编译时候所指定的参数。\n另外，编译各个平台下面的静态库，这本身也不是一件简单的事。如果没有相关经验，可能会无从下手。所以，我们为了解决这个问题，现在新建了一个仓库\rcocos2d-x-3rd-party-libs-src. 这个仓库会把cocos2d-x所需要编译的第三方库的编译脚本全部保存在这里，如果哪一天，我们需要升级第三方库，或者发现现有的第三方库有问题，也可以从编译脚本找问题。\n交叉编译一般步骤 一般来讲，我们编译一个第三方库有以下几个步骤。\n下载第三方库的源码（一般下载稳定版本）\n阅读它自带的Install或者Readme文件，看看如何在本机上编译这个库\n如果在本机上不能顺利编译的，可能需要自己给第三方库写patch，这样做的好处是，下次别人下载这个库，只要应用你的patch就可以顺利编译了。\n指定交叉编译器，sysroot, CFLAGS, CXXFLAG等参数\n./configure, make, make install就搞定了。\n一套成形的脚本看起来，大概是\r这样子的.\n解决办法 如果我们为每一个库都编写这样一套脚本，那会显得非常浪费。因为基本上每个平台的脚本都大同小异，如果可以把共用的部分抽取出来就最好了。\n要抽取出公共的部分其实是有难度的，但是世界上做这件事情的人绝对不只你一个。本着不重复造轮子的原则，我们直接把\rVLC\n所使用的编译系统拿来改造一下就ok啦。\n这套系统主要组成如下\n它有一个bootstrap脚本，主要用来配置一些交叉编译的平台参数，如果是Android，则需要指定NDK路径，ANDROID ABI和ANDROID API的版本，是否使用neon等。\n一堆rules.mk文件，每一个第三库都会编译一个rules.mk文件，用来指定交叉编译时的参数\n一个SHA512SUMS文件，主要用来对下载的第三库进行md5的验证。\n一些patch，这部分是可选的。当下载的第三方库在编译时遇到困难时可能需要自己编译一些patch.编写patch非常简单，使用git diff \u0026gt; xxx.patch即可。\n具体的文档介绍，请看\rREADME文件\n结论 编译链接这件事情，其实我之前是没有整得非常清楚的。尤其是configure, make, makefile, cmake这些东西。通过这次折腾这堆编译脚本，我学到了很多东西，对编译链接理解地更加深入了。\n另外，我觉得在做任何一件比较棘手的事情之前，先想想是否别人也会遇到同样的问题。他们是怎么解决的，站在巨人的肩膀上会看得更远。\n还有，Don\u0026rsquo;t repeat yourself!\nReference Introduction to making makefile\nGNU make reference\nVLC github\n阅读原文\n","description":"","id":50,"section":"posts","tags":[],"title":"一套交叉编译脚本","uri":"https://starifly.github.io/posts/a-promising-cross-compile-framework/"},{"content":"在开始介绍如何使用CMake编译跨平台的静态库之前，先讲讲我在没有使用CMake之前所趟过的坑。因为很多开源的程序，比如png，都是自带编译脚本的。我们可以使用下列脚本来进行编译：\n./configure --prefix=/xxx/xx --enable-static=YES make make install 相信手动在类Unix系统上面编译过开源程序的同学对上面的命令肯定非常熟悉。但是，如果不配置编译器和一些编译、链接参数，这样的操作，最后编译出来的静态库只能在本系统上面被链接使用。比如你在mac上面运行上面的命令，编译出来的静态库就只能给mac程序链接使用。如果在Linux上面运行上述命令，则也只能给Linux上面的程序所链接使用。如果我们想要在Mac上面编译出ios和android的静态库，就必须要用到交叉编译。\n要进行交叉编译，一般来说要指定目标编译平台的编译器，通常是指定一个CC环境变量，根据编译的是c库还是c++库，要分别指定C_flags和CXX_flag，当然还需要指定c/c++和系统sdk的头文件包含路径。总之，非常之繁琐，大家可以看一下我之前把png编译到ios和mac上面的静态库所使用的\r脚本。\n为什么要使用CMake 为什么我们不使用autoconf？为什么我们不使用QMake,JAM,ANT呢？具体原因大家可以参考我在本文最后的参考链接里面的\r《Mastering CMake》一书的第一章。我自己使用CMake的感受就是：我原来编写bash，配置configure参数，读各个开源库的INSTALL文件(因为不同库的configure参数有差别)，配置各种编译flag，头文件包含等。最后3天时间，折腾了png和jepg两个库的编译。当然，中间我还写了android和linux的编译脚本。而换用CMake以后，我2天时间编译完了Box2D,spine和Chipmunk的编译。并且配置脚本相当简单，添加新的库，基本上只是拷贝脚本，修改一两个参数即可。有了CMake，编译跨平台静态库和生成跨平台可执行程序So Easy！\n编写CMakeLists.txt 编写一个静态库的CMake配置文件过程如下：（这里我以Box2D为例）\n指定头文件和源文件 include_directories( ${CMAKE_CURRENT_SOURCE_DIR} ) file(GLOB_RECURSE box2d_source_files \u0026#34;${CMAKE_CURRENT_SOURCE_DIR}/Box2D/*.cpp\u0026#34;) 我的CMakeLists.txt和Box2D的文件结构关系如下图所示：\n这里的${CMAKE_CURRENT_SOURCE_DIR}表示CMakeLists.txt所在的目录。而GLOB_RECURSE可以递归地去搜索Box2D目录下面所有的.cpp文件来参与静态库的编译。而include_directories和file指令，显而易见，它们是用来指定静态库的头文件和实现文件。\n指定库的名字 add_library(Box2D STATIC ${box2d_source_files}) 这里add_library表示最终编译为一个库，static表示是静态库，如果想编译动态库，可以修改为shared.\n至此，一个静态库的配置就完成了。当然，因为这个库没有包括其它外部的头文件，所以会比较简单。但这也远比一个Makefile要简单N倍。\n编译linux静态库（含64位和32位） 编译linux的静态库是非常简单的，只需要安装好cmake以后，运行以下命令即可:\ncmake . make 注意，如果是64位的系统，那么这样只能生成64位的静态库。想要编译出32位的静态库，则必须要先安装32位系统的编译工具链。\nsudo apt-get install libx32gcc-4.8-dev sudo apt-get install libc6-dev-i386 sudo apt-get install lib32stdc++6 sudo apt-get install g++-multilib 然后，只需要指定cxx_flags为-m32即可，对应的CMake的写法为：\nset(CMAKE_CXX_FLAGS \u0026#34;${CMAKE_CXX_FLAGS} -m32\u0026#34;) 最后用cmake生成makefile并make即可生成32位的静态库\n编译ios静态库 编译ios库，最好先用cmake生成xcode工程。但是cmake默认的写法\ncmake -GXcode . 只能生成mac平台的xcode工程，而不能生成ios平台的xcode工程。不过我们可以借助\rios-cmake开源项目。\n下载iOS_32.cmake这个toolchain文件，然后使用下列命令来生成ios工程:\ncmake -DCMAKE_TOOLCHAIN_FILE=../toolchain/iOS_32.cmake -DCMAKE_IOS_DEVELOPER_ROOT=/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/ -GXcode .. 有了ios工程以后，我们就可以调用xcodebuild来生成静态库了:\nxcodebuild -project Project.xcodeproj -alltargets -sdk iphonesimulator7.1 -configuration Release 这条命令会生成一个胖包（armv7、armv7s）。但是默认只会生成32位的胖包。因此，我修改了iOS_32.cmake，让它可以生成64位的静态库。这个文件为\riOS_64.cmake.\n所有的ios静态库（i386,x86_64,armv7,armv7s,arm64）生成完以后，可以用lipo来生成一个胖包，命令如下：\nlipo lib/i386/libBox2D.a lib/x86_64/libBox2D.a lib/armv7/libBox2D.a lib/arm64/libBox2D.a -create -output libBox2D.a 编译mac静态库 这个比较简单，直接Xcode -GXcode，然后用xcodebuild命令即可。\n编译Andoird静态库 编译android库我们同样可以引入一个toolchain文件，这里我是从\randroid-cmake里面下载的。\n在使用这个toolchain文件之前，我们先要使用ndk自带的make-standalone-toolchain.sh脚本来生成对应平台的toolchain.这个脚本位于你的NDK的路径下面的buil/tools目录下。\n比如要生成arm平台的toolchain，我们可以使用下列命令:\nsh $ANDROID_NDK/build/tools/make-standalone-toolchain.sh --platform=android-$ANDROID_API_LEVEL --install-dir=./android-toolchain --system=darwin-x86_64 --ndk-dir=/Users/guanghui/AndroidDev/android-ndk-r9d/ --toolchain=arm-linux-androideabi-4.8 这里的$ANDROID_NDK 为你的NDK的安装路径。这段命令可以生成arm的toolchain，最终可以编译出armeabi和armeabi-v7a静态库。\n如果想生成x86的toolchain，指需要使用下列命令:\nsh $ANDROID_NDK/build/tools/make-standalone-toolchain.sh --platform=android-$ANDROID_API_LEVEL --install-dir=./android-toolchain-x86 --system=darwin-x86_64 --ndk-dir=/Users/guanghui/AndroidDev/android-ndk-r9d/ --toolchain=x86-4.8 export PATH=$PATH:./android-toolchain/bin export ANDROID_STANDALONE_TOOLCHAIN=./android-toolchain cmake -DCMAKE_TOOLCHAIN_FILE=../android.toolchain.cmake -DANDROID_ABI=\u0026#34;armeabi\u0026#34; .. 编译Win32，wp8和winrt静态库 这里直接使用cmake-gui生成对应的VS工程，然后再手动编译即可。\n关于Box2D完整的跨平台编译脚本可以参考\r我的Github\nReference cmake by example\nmastering cmake pdf\nMeta-configuration of C/C++ projects with CMake\nSetting up Android standalone toolchain for CMake\ncmake and the Android NDK\nandroid-cmake\nCMake for Andrioid\nios-cmake google code\n阅读原文\n","description":"","id":51,"section":"posts","tags":["cmake"],"title":"使用CMake编译跨平台静态库","uri":"https://starifly.github.io/posts/how-to-use-cmake-to-compile-static-library/"},{"content":" Learning How to Learn\n关于编程的练习方法——读《刻意练习》有感\n《如何阅读一本书》阅读笔记\n工作流\n笔记：有关开源项目\n有一种焦虑叫：什么都想学，但什么都学不会\n你该如何打破自己停滞不前的状态？\n","description":"","id":52,"section":"posts","tags":["learning"],"title":"Learning Something","uri":"https://starifly.github.io/posts/learning-something/"},{"content":"这是转载文章。\r为什么要使用 HTTPS 协议呢？ 在浏览器和网站之间提供更安全的通讯 HTTPS 比 HTTP 的速度更快 能提高搜索引擎的优化排名 HTTPS 协议的网站在地址栏前会有绿色锁的图标，感觉有点逼格，就冲着这点，果断 HTTPS 走起。\nGithub Pages 本身支持 HTTPS，但仅支持 github.io 域名。如果绑定了自己的域名，就不支持 HTTPS 了。幸运的是，CloudFlare 提供免费的启用 HTTPS 的解决方案。\n接下来就开始进入主题。\n步骤一：配置 Github Pages 这里就默认大家已经有 Github Pages 仓库了。如果不太清楚怎么创建的，可以先搜索一下教程。\n在 Github Pages 仓库的根目录下新建一个 CNAME 文件，文件内容就是我们要绑定的域名，这一步可以直接在设置面板里配置，保存好之后回到仓库主页就能看到刚才添加的 CNAME 文件\n步骤二：注册 CloudFlare 账号，添加我们的域名 注册成功后在返回的页面中添加域名，点击扫描 DNS 记录，等待大约一分钟之后继续下一步。\n注意：输入的域名不需要带 www 前缀。例如：zhouhao.me\n步骤三：添加域名解析 CNAME 被称为规范名字，能将域名解析到另一个域名。本例中，我们将自己的域名重定向到 Github Pages URL。\n然后下一步选择免费的 CloudFlare Plan。\n接下来，修改域名服务器。因为我用的是万网的 DNS，所以现在要在万网的域名控制台将 DNS 服务器修改至 CloudFlare 提供的域名服务器。\n步骤四：设置 SSL 为 Flexible 步骤五：添加路由重定向规则 使用通配符将路由重定向到 HTTPS 的链接。本例中将路由设置为：http://zhouhao.me/，协议设置为：Always Use HTTPS\n总结 至此，Github Pages 绑定的自定义域名启用 HTTPS 协议就完成了，打开自己的 Github Pages 主页就能看到启用 HTTPS 的域名。\n参考资料 https://zhouhao.me/2017/07/21/using-https-with-custom-domain-name-on-github-pages/ ","description":"","id":53,"section":"posts","tags":["github","blog","https"],"title":"为 Github Pages 自定义域名博客开启 HTTPS","uri":"https://starifly.github.io/posts/using-https-with-custom-domain-name-on-github-pages/"},{"content":"这是一篇非常有价值，里面的每一个都非常值得大家实践。\nAlfred workflow 的详细说明，我就不再赘述了，大家可以直接看最后的参考资料，我把我在用的 Alfred workflow 列一下，以供参考：\n插件名 插件快捷键 Baidu Weather tq Chrome Bookmarks ,b Dash dash Douban book/music/movie Evernote en/ennew/typenote Github repos gh Github Trending all Kill Process kill MacVim vi/vim/mvim Network Tools flush/ping/nslookup/traceroute Open with Visual Studio Code code QRcode Creator qr Search 复制内容，然后按快捷键：option+w Slack slack StackOverflow .so Sublime Text subl/subl* V2EX v2ex new WeChat Plugin wx xxx 选中人，然后直接在Alfred输入框中输入内容即可发送 What\u0026rsquo;s my ip ip YouTube yt/youtube xxx 有道翻译加强版 yd 知乎 zh/zhdaily Github Search: alfred workflow：\nhttps://github.com/franzheidl/alfred-workflows https://github.com/hzlzh/Alfred-Workflows https://github.com/deanishe/alfred-workflow https://github.com/willfarrell/alfred-workflows https://github.com/zenorocha/alfred-workflows 参考资料 Alfred神器使用手册 Alfred Workflow 列表 效率启动器 Aflred 中最强大的 Workflow 功能，都有哪些进阶玩法？ Alfred Workflow 微信插件 macOS Windows上的效率神器/\rhttps://github.com/Wox-launcher/Wox Everything-Windows上的绝对顶级搜索工具 ","description":"","id":54,"section":"posts","tags":["alfred","workflow"],"title":"无比强大的 Alfred Workflow","uri":"https://starifly.github.io/posts/alfred-workflow/"},{"content":"这是一篇有关环境变量加载的入门介绍（不太确定）。\r/etc/profile: 此文件为系统的每个用户设置环境信息，当用户第一次登录时，该文件被执行。并从 /etc/profile.d 目录的配置文件中收集 shell 的设置。如果你有对 /etc/profile 有修改的话必须得 source 一下 你的修改才会生效，此修改对每个用户都生效。\n/etc/bashrc: 为每一个运行 bash shell 的用户执行此文件。当 bash shell 被打开时，该文件被读取。如果你想对所有的使用 bash 的用户修改某个配置并在以后打开的 bash 都生效的话可以修改这个文件，修改这个文件不用重启，重新打开一个 bash 即可生效。\n~/.bash_profile: 每个用户都可使用该文件输入专用于自己使用的 shell 信息，当用户登录时，该文件仅仅执行一次!默认情况下,他设置一些环境变量，执行用户的 .bashrc 文件。\n此文件类似于 /etc/profile，也是需要需要 source 才会生效，/etc/profile 对所有用户生效，~/.bash_profile 只对当前用户生效。\n~/.bashrc: 该文件包含专用于你的 bash shell 的 bash 信息，当登录时以及每次打开新的 shell 时，该文件被读取。（每个用户都有一个 .bashrc 文件，在用户目录下）\n此文件类似于 /etc/bashrc，不需要重启就可以生效，重新打开一个 bash 即可生效，/etc/bashrc 对所有用户新打开的 bash 都生效，但 ~/.bashrc 只对当前用户新打开的 bash 生效。\n~/.bash_logout: 当每次退出系统(退出 bash shell)时，执行该文件。\n总结 /etc/profile 中设定的变量(全局)的可以作用于任何用户，而 ~/.bashrc 中设定的变量(局部)只能继承 /etc/profile 中的变量，他们是\u0026quot;父子\u0026quot;关系。\n~/.bash_profile 是交互式、login 方式进入 bash 运行的；\n~/.bashrc 是交互式 non-login 方式进入 bash 运行的；\n通常二者设置大致相同，所以通常前者会调用后者。\n参考资料 Linux 之 /etc/profile、~/.bash_profile 等几个文件的执行过程 ","description":"","id":55,"section":"posts","tags":["linux","profile","bashrc"],"title":"Linux 中 profile，bashrc，bash_profile等的区别","uri":"https://starifly.github.io/posts/profile-bashrc-bash_profile-in-linux/"},{"content":"\r一分钟告诉你究竟DevOps是什么鬼？ 历史回顾\n为了能够更好的理解什么是DevOps，我们很有必要对当时还只有程序员(此前还没有派生出开发者，前台工程师，后台工程师之类)这个称号存在的历史进行一下回顾。\n如编程之道中所言：\n老一辈的程序员是神秘且深奥的。我们没法揣摩他们的想法，我们所能做的只是描述一下他们的表象。\n清醒的像一只游过水面的狐狸\n警惕的像一位战场上的将军\n友善的像一位招待客人的女主人\n单纯的像一块未经雕琢的木头\n深邃的像一潭幽深洞穴中漆黑的池水\n程序员开发了机器语言，机器语言又产生了汇编语言，汇编语言产生了编译器，如今的语言已经多不胜数。每一种语言都有其各自的谦卑用途。每一种语言都表达出软件的阴和阳。每一种语言都在此道之中有其一席之地。\n遥想当年，软件程序员的大部分办公司那时还被称作实验室，程序员那时还叫做科学家。为了开发出一套优秀的软件，程序员们必须深入了解他们需要的应用相关的所有问题。他们必须清楚知道这个软件应用在什么场合，这个软件是必须在什么系统上运行。本质上说，程序员对所要开发的软件的所有环节都有透彻的了解，从规格说明书编写、到软件开发、到测试、到部署、再到技术支持。\n过了不久，人类(客户)贪婪的特性就开始表现出来，他们开始不断的进行更多的索求。更快的速度，更多的功能，更多的用户，更多的所有所有。\n作为一类谦虚、谦卑、且平静的生物，我们的老一辈程序员们将很难在这种爆发性的过度的需求索取中幸存。最好的取胜办法就是往不同的方向进化成不同的新物种。很快，程序员这个称号就开始绝迹于江湖，而那些叫做开发者、软件工程师、网络管理员、数据库开发者、网页开发者、系统架构师、测试工程师等等更多的新物种就开始诞生。快速进化和快速适应外界的挑战成为了他们的DNA的一部分。这些新的种族可以在几个星期内就完成进化。网页开发者很快就能进化成后台开发者，前台开发者，PHP开发者，Ruby开发者，Angular开发者…多得让人侧目。\n很快他们就都忘却了他们都是起源于程序员这个共同的祖先的事实，忘却了曾经有过这么一个单纯且平静的，想要让这个世界变得更好的科学家。然后他们开始不断的剑拔弩张，都声称自己才是“程序员”的纯血统继承人。\n随着时间的转移，各门各派开始独占山头，很少进行交流互动，只有在迫不得已的时刻才会进行沟通。他们开始不再为同源的遥远的同宗兄弟们的成功而欢呼雀跃，甚至再也不会时把的遥寄张明信片进行嘘寒问暖。\n但是在深夜仰望星空的时候，他们还是会发现他们的心底深处的程序员基因还是会不停的闪烁着，期盼着这闪烁的火花能照亮整个银河系并带来和平。\n在这场自私且以自我为中心的欲征服世界的赛跑旅程里，程序员的子孙们早把他们真正的工作目标置之脑后-为客户解决问题。面对一拖再拖的项目交付日期，昂贵的开发代价，甚至最终失败的项目，客户们开始对这种情况深恶痛绝。\n偶尔，也会有一个闪亮的明星站出来，灵机一动的提供一种办法来尝试结束这种混乱并带来和平。所以瀑布开发流程就应运而生了。这是一个非常了不起的创意，因为它利用了不同团队的开发者们只在必须的时候才进行沟通的这个事实。当一个团队完成了他们的工作的时候，它就会和下游的团队进行交流并把任务进行往下传，如此一级接一级的传递下去，永不回首。\n这种方式在一段时间内发挥了效用，但很快，一如既往，贪婪的人们(客户)又开始提出更多的诉求。他们希望能够更多地参加到整个软件的开发流程中来，不时的提出他们的建议，甚至在很晚的时候还提出改需求这种丧心病狂的事情来。\n结果就是如大家有目共睹的事实一样，软件项目非常容易失败这个说法已经作为一个行业标准被人们所接受。数据表明超过50%的项目最终都是以失败告终的。更可悲的是，在当时看来，人们对这种情况是束手无策。\n值得庆幸的是，每一个时代总会有那么几个思想开放的英雄如漆黑中的萤火虫般冒出来。他们知道这些不同团队的开发者们必须要找到一个可以协同工作、进行交流、并且能够弹性的向客户保证对方将会拿到最优的解决方案的方式。这种尝试最早可以追溯到1957年，伟大的约翰·冯·诺依曼和同行们的努力。但是我们最终却是等到2001年才收获到革命的果实，当时行业的十多个精英创造出了如今闻名世界的“敏捷宣言”。\n敏捷宣言基于以下十二条原则：\n我们的首要任务是通过尽早地、持续地交付可评价的软件来使客户满意。\n乐于接受需求变更，即使是在开发后期也应如此。敏捷过程能够驾驭变化，从而为客户赢得竞争优势。\n频繁交付可使用的软件，交付间隔越短越好，可以从几个星期到几个月。\n在整个项目开发期间，业务人员和开发人员必须朝夕工作在一起。\n围绕那些有推动力的人们来构建项目。给予他们所需的环境和支持，并且信任他们能够把工作完成好。\n与开发团队以及在开发团队内部最快速、有效的传递信息的方法就是，面对面的交谈。\n可使用的软件是进度的主要衡量指标。\n敏捷过程提倡可持续发展。出资人、开发人员以及使用者应该总是共同维持稳定的开发速度。\n为了增强敏捷能力，应持续关注技术上的杰出成果和良好的设计。\n简洁——最大化不必要工作量的艺术——是至关重要的。\n最好的架构、需求和设计都源自自我组织的团队。\n团队应该定期反思如何能变得更有战斗力，然后相应地转变并调整其行为。\n敏捷宣言是为银河系带来和平以及维护各自的平衡所迈出的很重要的第一步。在很长的时间里，相比此前基于流程和机械化的方式，这是第一次基于文化和“人性”来将不同的关键项目关系人连接在一起的方式。人们开始互相交流，进行基本的碰头会议，并开始不断的交流意见和看法。他们开始意识到他们是有着很多比想象中还多的共同点的，客户也开始成为他们之中的一员，而不再是像以往一样只是往项目砸钱然后开始求神拜佛祈求一切顺利如愿。\n尽管前面还是有不少的障碍需要克服，但是未来已经光明了许多。敏捷意味着开放和拥抱(需求)改变。但是，如果改变过多的话，人们就很难专注到最终的目标和交付上来。此时精益软件开发就开始破土而出了。\n因为对精益软件开发的着迷以及为了达成放逐和驱赶风险的目的，一些程序员的子孙们就开始探首窗外，开始向软件之外的行业进行取经。他们从一家主要的汽车生产商身上找到了救赎。丰田生产系统在精益上面的成就是不可思议的，同时它们的精益生产的经验也是很容易应用到软件开发上来的。\n精益有以下7个原则：\n杜绝浪费\n内建质量\n创建知识(放大学习)\n延迟决策(尽量延迟决定)\n快速交付\n尊重人员(团队授权)\n全局优化\n将这些放到敏捷上去的话，精益原则就能让人们在从精神上关注做正确的事情，同时还能够让整个开发流程拥有足够的弹性。\n一旦敏捷和精益软件开发被软件开发团队采纳，那么下一步就是把这一套原则应用到IT团队上来。把IT也纳入到整体战略上，然后我们就来到了DevOps跟前了！\n进入DevOps – 高速公路的三条车道\n老一派的软件开发团队成员会包含业务分析员，系统架构师，前端开发者，后端开发者，测试员，等等。优化如敏捷和精益原则等的软件开发流程的关注点就在这些地方。比如，软件一旦达到”可以生产“的程度，就会发到系统工程师、发布工程师、DBA、网络工程师，安全专家这些“运维人员”的手上。这里该如何将横在Dev(开发)和Ops(运维)之间的鸿沟给填平，这就是DevOps的主要关注点了。\nDevOps是在整个IT价值流中实施精益原则的结果。IT价值流将开发延伸至生产，将由程序员这个遥远的祖宗所繁衍的所有子孙给联合在一起。\n这是来自Gene Kim的对DevOps的最好的解析，如果你还没有看过他的《凤凰项目》这本书的话，我建议你真的该好好花时间看看。\n你不应该重新招聘DevOps工程师，且DevOps也不应该是一个IT的新部门。DevOps是一种文化，一种理念，且是和IT糅合成一整体的。世间没有任何工具可以把你的IT变成一个DevOps组织，也没有任何自动化方式可以指引你该如何为你的客户提供最大化的效益。\nDevOps通常作为下面这三个方式而为人所熟知，而在我眼里我是把它们看成是一条高速公路上的三条车道。你从第一条车道开始，然后加速进入到第二条车道，最终在第三车道上高速行驶。\n车道1 – 系统级别的整体效率考量是最主要的关注点，这超过对系统中任何一个单独个体元素的考虑\n车道2 – 确保能提供持续不断的反馈循环，且这些反馈不被忽视。\n车道3 – 持续的学习和吸取经验，不停的进步，快速的失败。\n车道1 – 获取速度\n要采纳DevOps的原则，理解整个运作系统的重要性并对工作事项进行合适的优先级排序是组织首先要学的事情。在整个价值流中不能允许任何人产生瓶颈并降低整个工作流程。\n确保工作流程的不可中断是身处流程中的所有成员的终极目标。无论一个成员或者团队的角色是什么，他们都必须力图对整个系统进行深入的理解。这种思维方式对质量会有着直接的影响，因为缺陷永远不会被下放到“下游“中，这样做的话将会导致瓶颈的产生。\n确保整个工作流程不会被瓶颈堵塞住还不够。一个高产的组织应该时常考虑该如何提升整个工作流程。有很多方法论可以做到这一点，你不妨去看下“约束理论”，“六西格玛”，精益，或者丰田生产系统。\nDevOps原则不关心你身处哪个团队，你是否是系统架构师，DBA，QA，或者是网络管理员。相同的规则覆盖所有的成员，每个成员都应该遵循两个简单的原则：\n保持系统运作流程不可中断\n随时提升和优化工作流程\n车道2 – 换挡加速\n不可中断的系统流程是定向的，且预期是从开发流向运维。在一个理想的世界中，这就意味着快速的开发出高质量的软件，部署，并为客户提供价值。\n但是，DevOps并非乌托邦式的理想国。如果单向的交付方式是可行的话，我们的瀑布模式早就能胜任了。评估可交付产品和整个流程中的交流对确保质量是至关重要的。这里首个必须实现的”面向上游”的交流通道是从Ops到Dev。\n我们独自意淫是件非常容易的事情，但是获取别人的反馈和提供反馈给别人才是探究事实真相的正确方法。下游的每一步(反馈)都必须紧跟着有一个上游的确定。\n你如何建立反馈循环机制并不重要。你可以邀请开发人员加入技术支持团队的会议，或者将网络管理员放到Sprint计划会议中去。一旦你的反馈机制就绪，反馈能够被接收并被处理，你就已经可以说是走到了DevOps高速车道上来了。\n车道3 – 飞速前进\nDevOps这条快速车道并不适合意志脆弱的人。为了进入这条车道，你的组织必须要足够的成熟。这里充满了冒险和对失败教训的学习，不断的尝试，并认同屡败屡战和不断的实践是走向成功这条康庄大道的前提条件。在这里你应该会经常听到”套路“这个词，这是有原因的。不断的训练和重复所以能培养出大师，是因为其让复杂的动作常规化。\n但是在你要将这些复杂的动作连接起来之前，你很有必要先去掌握好每一个单独步骤。\n“适合大师的动作并不适合新手，脱胎换骨之前你必须先要明白道的真谛。“\nDevOps的第三个方式/快速车道包括每天分配时间来持续的进行试验，时常的奖励敢于冒险的团队，并将缺陷特意引入到运作系统上来以增加系统的抗击打能力。\n为了确保你的组织能够消化好这些方法，你必须在每个团队之间建立好频繁的反馈循环，同时需要确保所有的瓶颈都能够及时的被清理掉，并确保整个系统的运作流程是不可中断的。\n实施好这些措施可以让你的组织时刻保持警惕，并能够快速且高效的应对挑战。\n概要 – DevOps清单\n下面是一张你可以用来检验你的组织对DevOps的应用情况的清单。当然你也可以在文章评论后面给出你的观点。\n开发团队和运维团队之间没有障碍。两者皆是DevOps统一流程的一部分。\n从一个团队流到另一个团队的工作都能够得到高质量的验证\n工作没有堆积，所有的瓶颈都已经被处理好。\n开发团队没有占用运维团队的时间，因为部署和维护都是处于同一个时间盒里面的。\n开发团队不会在周五下午5点后把代码交付进行部署，剩下运维团队周末加班加点来给他们擦屁股\n开发环境标准化，运维人员可以很容易將之扩展并进行部署\n开发团队可以找到合适的方式交付新版本，且运维团队可以轻易的进行部署。\n每个团队之间的通信线路都很明确\n所有的团队成员都有时间去为改善系统进行试验和实践\n常规性的引入(或者模拟)缺陷到系统中来并得到处理。每次学习到的经验都应该文档化下来并分享给相关人员。事故处理成为日常工作的一部分，且处理方式是已知的\n总结\n使用现代化的DevOps工具，如Chef、Docker、Ansible、Packer、Troposphere、Consul、Jenkins、SonarQube、AWS等，并不代表你就在正确的应用DevOps的原则。DevOps是一种思维方式。我们所有人都是该系统流程的一部分，我们一起分享共同的时光和交付价值。每个参加到这个软件交付流程上来的成员都能够加速或减缓整个系统的运作速度。系统出现的一个缺陷，以及错误配置的团队之间的“防火墙”，都可能会使得整个系统瘫痪，\n所有的人都是DevOps的一部分，一旦你的组织明白了这一点，能够帮你管理好这些的工具和技术栈就自然而然的会出现在你眼前了。\n是否可以通俗地认为：DevOps 是一个完整的面向IT运维的工作流，以 IT 自动化以及持续集成（CI）、持续部署（CD）为基础，来优化程式开发、测试、系统运维等环节的工作流。\nReference What The Hell Is DevOps? 阅读原文\n","description":"","id":56,"section":"posts","tags":["DevOps"],"title":"一分钟告诉你究竟DevOps是什么鬼？","uri":"https://starifly.github.io/posts/what-the-hell-is-devops/"},{"content":"本文的主要内容如标题所示，通过webhook将Hugo自动部署至GitHub Pages和GitCafe Pages。如果你正好有这个需求，看这篇文章正好，可以节省你不少时间。如果不是，了解一下也无妨。\n首先，必须解释一下，为什么需要自动部署，以及为什么需要需要同时部署到GitHub Pages和GitCafe Pages。\n为什么要自动部署 使用Hugo生成的静态页面是在public文件夹里，部署的时候需要把public文件夹里的内容push到GitHub的gh-pages分支里。每次写完文章，除了push markdown格式的文章，还需要单独push生成的public文件夹里的东西，步骤稍显麻烦。\n之前参照了官方的做法，使用subtree来push public，步骤简化不少。然而，这还是不够简单。因为每次修改文章之后，必须依赖一个脚本才能正确提交和部署。假如你在手机里浏览时，发现一个错别字，顺手就在GitHub的Web界面就把错别字改了，然而这样并不会重新生成静态页面和部署。有些不方便。\nHugo生成静态页面和部署的过程应该让机器自动来完成。写作应该是一个相对单纯的事情，使用Hugo的人应该更专注于写作。\n为什么需要同时部署到GitHub Pages和GitCafe Pages 大公司很喜欢的一个词：容灾。GitHub出现不可访问的事情在国内也是常有的，而GitCafe作为国内的代码托管厂商，是否是一个稳定的存在也不好说。所以，将网站同时部署到这两个上面。通过DnsPod里CNAME设置线路“国内”和“国外”，不仅起到了任何一个挂掉，另一个可以继续工作的目的，还起到了CDN就近访问的作用。\n使用GitCafe还有另外一个原因。GitHub Pages拒绝了一切百度的爬虫，所以，百度无法索引到GitHub Pages的网页。对于国内的搜索市场来说，百度的份额还是比较大的，虽然我认为看我的博客的人都不应该使用百度，但现实总是残酷的。如果希望网站被百度收录，就必须放到GitHub以外的地方。GitCafe就是一种比较好的选择。\nhttp://gitcafe.com\n如果，你只是希望使用官方的Hugo自动化部署到GitHub Pages，下面的内容你可以不用看了。你可以直接使用Wercker的服务来自动部署。\n文档见：\rhttp://gohugo.io/tutorials/automated-deployments/\n由于Wercker还不支持GitCafe的部署，以及我需要使用特定的修改版本的Hugo来生成静态网页，并且希望这些步骤比较可控，所以，还是自己来折腾整个过程吧。\nwebhook webhook是GitHub上提供的Git的一种Hook机制，当代码发生变化时，比如代码被Push到GitHub的Repo时，GitHub会自动请求一个你指定的网页，并且把变更相关的参数都传递过来。入口在Repo的Settings - webhooks \u0026amp; services\n说明文档：\rhttps://developer.github.com/webhooks/\n借助webhook的机制，我们就可实现当有新的文章Push之后，自动通知远程的一台机器执行一个脚本，脚本的内容就是生成静态页面和Push部署到最终的服务器。\nwebhook的Server接收webhook通知，然后执行一个脚本。这样的需求太普遍了，以至于完全不需要自己来实现。在GitHub里搜webhook可以搜出来很多。我主要挑选了Go语言的版本。主要有两个：\nhttps://github.com/qiniu/webhook https://github.com/adnanh/webhook 第一个是七牛写的，代码很简单，用法也很简单。开始打算用七牛的版本。最后调试的时候发现json解析失败，完全不可用啊！有点坑爹。于是换成了第二个，这个Repo有200多个Star。还是靠谱很多，最后部署，调试，非常顺利。\n用法也很简单，首先安装webhook：\n$ go get github.com/adnanh/webhook 写一个配置文件hooks.json，里面指定需要执行的脚本：\n[\r{\r\u0026#34;id\u0026#34;: \u0026#34;redeploy-webhook\u0026#34;,\r\u0026#34;execute-command\u0026#34;: \u0026#34;/var/scripts/redeploy.sh\u0026#34;,\r\u0026#34;command-working-directory\u0026#34;: \u0026#34;/var/webhook\u0026#34;\r}\r] 指定端口启动：\n$ /path/to/webhook -hooks hooks.json -port=9876 -verbose 然后它将接受webhook地址：（把它填到GitHub里的webhook里）\nhttp://yourserver:9876/hooks/redeploy-webhook 自动部署 大致的流程如上图。上图的DigitalOcean是一台VPS服务器，我用了很长时间了，速度和稳定性都不错。需要的同学使用这个链接购买，可以获得10美元的优惠：\rhttps://www.digitalocean.com/?refcode=e131e2bba197\n整个流程中，复杂度主要是在DigitalOcean的VPS上部署服务和脚本。\n部署的脚本可以在我的GitHub上看到：\rhttps://github.com/coderzh/coderzh-hugo-blog\n需要的同学可以参考下，代码见：\rdeploy.py\ndeploy.py放到你的主工程，也就是你写markdown的Repo下。比如：/var/coderzh-hugo-blog/下\nadnanh-webhook的配置文件：\n[\r{\r\u0026#34;id\u0026#34;: \u0026#34;hugo-deploy\u0026#34;,\r\u0026#34;execute-command\u0026#34;: \u0026#34;/var/webhook/hugo-deploy.sh\u0026#34;\r}\r] hugo-deploy.sh里执行deploy.py：\n#!/bin/bash\rpython /var/coderzh-hugo-blog/deploy.py --auto 剩下的是怎么在DigitalOcean的VPS上把这套东西部署起来。我使用nginx + supervisor搭建webhook的Server。\n关于nginx和supervisor可以参考之前的一篇文章：\rhttp://blog.coderzh.com/2014/05/19/digitalocean/\nsupervisor的配置如下：\n[program:webhook]\rcommand=/root/gocode/bin/webhook -hooks /var/webhook/hooks.json -verbose -port=9876\ruser=root\rdirectory=/var/webhook\rautorestart=true\rredirect_stderr=true\renvironment=HOME=\u0026#34;/root\u0026#34;,USER=\u0026#34;root\u0026#34; 关于VPS上SSH Key的设置，见：\rhttps://help.github.com/articles/generating-ssh-keys/ 为了自动部署方便，可以不设置密码。\n当然，还有个大前提，在VPS上安装最新版本的golang。推荐使用gvm来安装。（记得安装1.5之前必须先把1.4先装上）\ngolang 安装：\nbash \u0026lt; \u0026lt;(curl -s -S -L https://raw.githubusercontent.com/moovweb/gvm/master/binscripts/gvm-installer)\rsource ~/.bashrc\rgvm version\rgvm install go1.4\rgvm install go1.5.1\rgvm use go1.5.1 --default\rgo version 最后，Good Luck！\n","description":"","id":57,"section":"posts","tags":["CI","webhook","github","gitcafe"],"title":"通过webhook将Hugo自动部署至GitHub Pages和GitCafe Pages","uri":"https://starifly.github.io/posts/use-webhook-automated-deploy-hugo/"},{"content":"为什么要在这个时候探索flv.js做直播呢？原因在于各大浏览器厂商已经默认禁用Flash，之前常见的Flash直播方案需要用户同意使用Flash后才可以正常使用直播功能，这样的用户体验很致命。\n在介绍flv.js之前先介绍下常见的直播协议以及给出我对它们的延迟与性能所做的测试得出的数据。\n如果你看的很吃力可以先了解下音视频技术的一些\r基础概念。\n常见直播协议 RTMP: 底层基于TCP，在浏览器端依赖Flash。 HTTP-FLV: 基于HTTP流式IO传输FLV，依赖浏览器支持播放FLV。 WebSocket-FLV: 基于WebSocket传输FLV，依赖浏览器支持播放FLV。WebSocket建立在HTTP之上，建立WebSocket连接前还要先建立HTTP连接。 HLS: Http Live Streaming，苹果提出基于HTTP的流媒体传输协议。HTML5可以直接打开播放。 RTP: 基于UDP，延迟1秒，浏览器不支持。 常见直播协议延迟与性能数据以下数据只做对比参考 传输协议 播放器 延迟 内存 CPU RTMP Flash 1s 430M 11% HTTP-FLV Video 1s 310M 4.4% HLS Video 20s 205M 3% 在支持浏览器的协议里，延迟排序是：\nRTMP = HTTP-FLV = WebSocket-FLV \u0026lt; HLS\n而性能排序恰好相反：\nRTMP \u0026gt; HTTP-FLV = WebSocket-FLV \u0026gt; HLS\n也就是说延迟小的性能不好。\n可以看出在浏览器里做直播，使用HTTP-FLV协议是不错的，性能优于RTMP+Flash，延迟可以做到和RTMP+Flash一样甚至更好。\nflv.js 简介 flv.js是来自Bilibli的开源项目。它解析FLV文件喂给原生HTML5 Video标签播放音视频数据，使浏览器在不借助Flash的情况下播放FLV成为可能。\nflv.js 优势 由于浏览器对原生Video标签采用了硬件加速，性能很好，支持高清。 同时支持录播和直播 去掉对Flash的依赖 flv.js 限制 FLV里所包含的视频编码必须是H.264，音频编码必须是AAC或MP3， IE11和Edge浏览器不支持MP3音频编码，所以FLV里采用的编码最好是H.264+AAC，这个让音视频服务兼容不是问题。 对于录播，依赖 原生HTML5 Video标签 和 Media Source Extensions API 对于直播，依赖录播所需要的播放技术，同时依赖 HTTP FLV 或者 WebSocket 中的一种协议来传输FLV。其中HTTP FLV需通过流式IO去拉取数据，支持流式IO的有\rfetch或者\rstream flv.min.js 文件大小 164Kb，gzip后 35.5Kb，flash播放器gzip后差不多也是这么大。 由于依赖Media Source Extensions，目前所有iOS和Android4.4.4以下里的浏览器都不支持，也就是说目前对于移动端flv.js基本是不能用的。 flv.js依赖的浏览器特性兼容列表 HTML5 Video Media Source Extensions WebSocket HTTP FLV: fetch 或 stream flv.js 原理 flv.js只做了一件事，在获取到FLV格式的音视频数据后通过原生的JS去解码FLV数据，再通过\rMedia Source Extensions API 喂给原生HTML5 Video标签。(HTML5 原生仅支持播放 mp4/webm 格式，不支持 FLV)\nflv.js 为什么要绕一圈，从服务器获取FLV再解码转换后再喂给Video标签呢？原因如下：\n兼容目前的直播方案：目前大多数直播方案的音视频服务都是采用FLV容器格式传输音视频数据。 FLV容器格式相比于MP4格式更加简单，解析起来更快更方便。 flv.js兼容方案 由于目前flv.js兼容性还不是很好，要用在产品中必要要兼顾到不支持flv.js的浏览器。兼容方案如下：\nPC端 优先使用 HTTP-FLV，因为它延迟小，性能也不差1080P都很流畅。 不支持 flv.js 就使用 Flash播放器播 RTMP 流。Flash兼容性很好，但是性能差默认被很多浏览器禁用。 不想用Flash兼容也可以用HLS，但是PC端只有Safari支持HLS 移动端 优先使用 HTTP-FLV，因为它延迟小，支持HTTP-FLV的设备性能运行 flv.js 足够了。 不支持 flv.js 就使用 HLS，但是 HLS延迟非常大。 HLS 也不支持就没法直播了，因为移动端都不支持Flash。 flv.js实战 说了这么多介绍与原理，接下来教大家如何用flv.js搭建一个完整的直播系统。\n我已经搭建好了\r一个demo可以供大家体验。\n搭建音视频服务 主播推流到音视频服务，音视频服务再转发给所有连接的客户端。为了让你快速搭建服务推荐我用go语言实现的\rlivego，因为它可以运行在任何操作系统上，对Golang感兴趣？请看\rGolang 中文学习资料汇总。\n下载livego，注意选对你的操作系统和位数。 解压，执行livego，服务就启动好了。它会启动RTMP(1935端口)服务用于主播推流，以及HTTP-FLV(7001端口)服务用于播放。 实现播放页 在react体系里使用react flv.js 组件\rreflv 快速实现。\n先安装npm i reflv，再写代码：\nimport React, { PureComponent } from \u0026#39;react\u0026#39;; import Reflv from \u0026#39;reflv\u0026#39;; export class HttpFlv extends PureComponent { render() { return ( \u0026lt;Reflv url={`http://localhost:7001/live/test.flv`} type=\u0026#34;flv\u0026#34; isLive cors /\u0026gt; ) } } 让以上代码在浏览器里运行。这是你还看不到直播，是因为还没有主播推流。\n你可以使用\rOBS来推流，注意要配置好OBS：\n也可以使用\rffmpeg来推流，推流命令ffmpeg -f avfoundation -i \u0026quot;0\u0026quot; -vcodec h264 -acodec aac -f flv rtmp://localhost/live/test\nflv.js延迟优化 按照上面的教程运行起来的直播延迟大概有3秒，经过优化可以到1秒。在教你怎么优化前先要介绍下直播运行流程：\n主播端在采集到一段时间的音视频原数据后，因为音视频原数据庞大需要先压缩数据：\n通过H264视频编码压缩数据数据 通过PCM音频编码压缩音频AAC数据 压缩完后再通过FLV容器格式封装压缩后的数据，封装成一个FLV TAG\n再把FLV TAG通过RTMP协议推流到音视频服务器，音视频服务器再从RTMP协议里解析出FLV TAG。\n音视频服务器再通过HTTP协议通过和浏览器建立的长链接流式把FLV TAG传给浏览器。\nflv.js 获取FLV TAG后解析出压缩后的音视频数据喂给Video播放。\n知道流程后我们就知道从哪入手优化了：\n主播端采集时收集了一段时间的音视频原数据，它专业的叫法是\rGOP。缩短这个收集时间(也就是减少GOP长度)可以优化延迟，但这样做的坏处是导致视频压缩率不高，传输效率低。 关闭音视频服务器的I桢缓存可以优化延迟，坏处是用户看到直播首屏的时间变大。 减少音视频服务器的buffer可以优化延迟，坏处是音视频服务器处理效率降低。 减少浏览器端flv.js的buffer可以优化延迟，坏处是浏览器端处理效率降低。 浏览器端开启flv.js的Worker，多线程运行flv.js提升解析速度可以优化延迟，这样做的flv.js配置代码是： { enableWorker: true, enableStashBuffer: false, stashInitialSize: 128,// 减少首桢显示等待时长 } 这里是\r优化后的完整代码\n阅读原文\n","description":"","id":58,"section":"posts","tags":["音视频"],"title":"使用flv.js做直播","uri":"https://starifly.github.io/posts/live-streaming-with-flv/"},{"content":"入门与概念 [总结]视音频编解码技术零基础学习方法 入门启发：音视频的简单理解 视频编码基础\u0026ndash;帧的类型 容器格式 视音频编解码学习工程：FLV封装格式分析器 传输协议 RTMP协议规范 带你吃透RTMP RTMP服务器的延迟，多级边缘不影响延迟，gop为最大因素 编解码","description":"","id":59,"section":"posts","tags":["音视频"],"title":"音视频技术参考资料","uri":"https://starifly.github.io/posts/audio-video-tech-references/"},{"content":"在我写 Makefile 的头 10 年里，我养成了一个非常不好的习惯\n\u0026ndash; 完全严格使用 GNU Make 的扩展名。过去我并不知道， GNU Make 与 POSIX 所保证的可移植特性之间的区别与联系。通常情况，它并不十分重要，但是当在非 Linux 系统上进行构建时，比如在各种 BSD 系统上，就会变成一件麻烦事儿。我不得不指定安装 GNU Make，然后在心里记住不要使用系统自带的 make ，而是使用 gmake 这样的工具来调用它。\n我已经对 make 官方规范 十分熟悉，并且在过去的一年，我都在严格要求自己编写可移植的 Makefile。现在，我的构建不仅可以在各种类 unix 的系统之间进行移植，而且 Makefile 看起来更清晰与健壮。许多常见的 make 扩展名 \u0026ndash; 尤其是条件判断 \u0026ndash; 会导致不够健壮的却又复杂的 Makefile, 因此最好避免这些情况。能够确信你的构建系统能够各司其职，正常工作是非常重要的。\n本指南不仅适用于之前从来没有写过 Makefile 的 make 初学者，同样适用于想要学习如何写出可移植 Makefile 的资深开发者。 但不管怎样，为了能够理解文中的示例，你必须首先对命令行（编译器，链接器，目标文件等等）构建程序的常规步骤十分熟悉。我不会建议使用任何花哨的技巧，也不会提供任何标准的初学者模板。当项目不大的时候，Makefile 应该是相当的简单，并且随着项目的成长，以一种可预见，清晰的方式不断丰富。\n我不会覆盖 make 的每一个特性。如果想要学习所有完整的内容，你需要自行阅读它的规范。本指南将会详细讨论一些重要特性和约定俗成的规定。遵守已有的约定是非常重要的，这样使用你的 Makefile 的其他人，才能知道它能够完成和如何完成一些基本的任务。\n如果你的系统是 Debian, 或是基于 Debian 的系统，比如 Ubuntu，bmake 和 freebsd-buildutils 包将会分别提供 bmake 和 fmake 程序。这些可供选择的 make 实现，对于测试 Makefile 的可移植性十分有用，尤其是当你不小心使用了 GNU Make 的特性。虽然每个实现都实现了与 GNU Make 完全相同的一些扩展，但是它会捕获一些常见的错误。\n什么是 Makefile? make 的核心就是一个或多个依赖树（dependency tree），这些依赖树是由 *规则（rule）*构造而来。树中的每个节点叫做“目标（target）”。构建（build）的最后产物（可执行程序，文档等等）位于树根。Makefile 指定了依赖树的内容，并且提供了 Shell 命令来从目标的 先决条件（prerequisite） 生成目标。\ndependency tree:\n在上面的图示中，“.c” 结尾的文件是事先写好的源文件，而不是由命令生成的文件，所以它们没有先决条件。在依赖树中，指定一条或多条边的语法非常简单：\ntarget [target...]: [prerequisite...]\r从技术层面来讲，虽然多个目标可以通过一个单一规则指定，但是这种做法并不常见。典型地，每个目标会被它自己的构建规则来进行指定。比如，指定上述图示中的依赖：\ngame: graphics.o physics.o input.o\rgraphics.o: graphics.c\rphysics.o: physics.c\rinput.o: input.c\r这些规则的先后顺序并不重要。在采取任何实际的动作之前，整个的 Makefile 都会被解析，所以树的节点和边可以被以任意顺序指定。只有一个意外：在 Makefile 中，第一个非特殊的目标会被认为是 默认目标（default target）。当调用 make 但是没有并没有指定一个目标时，这个默认目标就会被自动选择。它应该是看起来比较显然的一些东西，这样即使一个用户盲目地运行 make，也会得到一个有用的结果。\n一个目标可以被指定多次。任何新的先决条件，都会被附加到已有的先决条件中。比如，下面的 Makefile 与上面的是一样的，不过实际上通常并不会这么写：\ngame: graphics.o\rgame: physics.o\rgame: input.o\rgraphics.o: graphics.c\rphysics.o: physics.c\rinput.o: input.c\r有 6 个特殊目标（special target）用来改变 make 自身的行为。所有特殊目标都有大写的名字，并且开始于一个周期。符合这个模式的名字被 make 保留使用。根据标准，为了获得可靠的 POSIX 行为，Makefile 的第一个非注释行必须是 .POSIX. 因为这是一个特殊的目标，所以它不能作为默认目标，故而 game 仍将作为默认目标：\n.POSIX:\rgame: graphics.o physics.o input.o\rgraphics.o: graphics.c\rphysics.o: physics.c\rinput.o: input.c\r在实际应用中，即使是一个简单的程序，也会有头文件。对于包含头文件的源文件，在依赖树也应该有指向源文件的边。如果头文件改变了，那么包含它的目标也应该被重新构建。\n.POSIX:\rgame: graphics.o physics.o input.o\rgraphics.o: graphics.c graphics.h\rphysics.o: physics.c physics.h\rinput.o: input.c input.h graphics.h physics.h\rAdding commands to rules 虽然我们已经构造了一个依赖树，但是还没有告诉 make 如何真正地从目标的先决条件中构建出目标。规则也需要指定 Shell 命令，这些 Shell 命令会被用于从先决条件中生成目标。\n如果你打算创建示例中的源文件，并调用 make, 你会发现它实际上已经知道了它该如何构建目标文件。这是因为 make 的初始配置已经有了一些 推断规则（inference rule），这部分将会在后面讨论。现在，我们会在开头加上 .SUFFIXES 这个特殊目标，擦除所有的内置推断规则。\n在一个规则中，命令会随即跟在目标或先决条件那一行的后面。每个命令行必须以一个 tab 字符开头。如果你的编辑器不能进行相关配置的话，可能会非常麻烦。并且当你想要从拷贝本文的示例时，可能会遇到一些问题。\n每个命令在属于自己的 Shell 中运行（译者：意思是每个 Shell 命令都是一个单独的进程），所以要注意：在使用像 cd 这样的命令时，它不会对后面的行造成影响。\n要做的最简单的事情，就是就像在 Shell 输入一样逐字地输入同样的命令：\n.POSIX:\r.SUFFIXES:\rgame: graphics.o physics.o input.o\rcc -o game graphics.o physics.o input.o\rgraphics.o: graphics.c graphics.h\rcc -c graphics.c\rphysics.o: physics.c physics.h\rcc -c physics.c\rinput.o: input.c input.h graphics.h physics.h\rcc -c input.c\rInvoking make and choosing targets 当调用 make 时，它会从依赖树中接受零个或多个目标， 如果目标*过时（out-of-date）*了，然后构建这些目标 \u0026ndash; 比如，运行目标规则中的命令。如果目标比其中的任一个先决条件要旧，那么这个目标就是过时了。\n# build the \u0026quot;game\u0026quot; binary (default target)\r$ make\r# build just the object files\r$ make graphics.o physics.o input.o\r这会导致依赖树产生连锁效应，也就是说，一个目标的重建可能会导致它所涉及的更早期目标的重新构建，直到所有涉及的目标都是最新状态。因为树的不同分支可以被独立地进行更新，所以有很多并行化的空间。很多 make 的实现都支持通过 -j 选项进行并行构建。虽然这并非标准，但是在 Makefile 的一个非常棒的特性就是，它不需要任何特殊的东西就能正确地工作。\nmake 的 -k （\u0026ldquo;keep going\u0026rdquo;）选项，功能与并行构建类似，是标准的。它会告诉 make 在遇到第一个错误时不要停下，而是继续更新不受该错误影响的目标。这对于 Vim’s quickfix list 和 Emacs’ compilation buffer 的填充非常好。\n默认构建多个目标是十分常见的情况。如果第一个规则选择了默认目标，我们该如何解决需要多个默认目标的问题呢？传统方式是使用伪目标（phony target）. 之所以用“伪”这个词，是因为它们没有相关文件与之关联，所以伪目标永远都不会是最新状态。习惯上，使用伪目标 all 作为默认目标。\n我会用 game 作为新的 all 目标的一个先决条件。更多实际目标，可以作为必要条件加入到默认目标中。这个 Makefile 的使用者也可以使用 make all 来构建整个项目。\n另一个常见的伪目标是 clean，它会移除所有 make 创建的文件。用户可以使用 make clean 来删除所有构建生成的中间文件。\n.POSIX:\r.SUFFIXES:\rall: game\rgame: graphics.o physics.o input.o\rcc -o game graphics.o physics.o input.o\rgraphics.o: graphics.c graphics.h\rcc -c graphics.c\rphysics.o: physics.c physics.h\rcc -c physics.c\rinput.o: input.c input.h graphics.h physics.h\rcc -c input.c\rclean:\rrm -f game graphics.o physics.o input.o\rCustomize the build with macros 到目前为止，Makefile 是编译器硬编码为 cc, 也没有使用任何的编译器标志（warning，optimization，hardening 等等）。虽然用户能够很容易控制所有这些事情，但是现在他们也不得不去编辑整个 Makefile 来这么做。可能用户同时安装了 gcc 和 clang，并且想要选择一个或另一个不改变已安装的作为 cc.\n为了解决这一点，make 有*宏（macro）*的概念，当宏被引用时就会被展开为字符串。传统上，使用叫做 CC 的宏表示 C 编译器，CFLAGS 表示传递给 C 编译器的标志，LDFLAGS 表示当 C 编译器链接时的标志，LDLIBS 表示库链接时的标志。Makefile 应该在需要时提供默认值。\n一个宏通过 $(...) 进行展开。引用一个尚未定义的宏是有效（也是常见）的，未定义的宏会被展开为一个空字符串。这就是下面的 LDFLAGS 情况。\n宏的值可以包含其他宏，每当宏被展开时，它们会被递归展开。一些 make 的实现允许被展开为自身的宏的名字也是一个宏，这是\r图灵完备的, 但是这个行为并非是标准行为。\n.POSIX:\r.SUFFIXES:\rCC = cc\rCFLAGS = -W -O\rLDLIBS = -lm\rall: game\rgame: graphics.o physics.o input.o\r$(CC) $(LDFLAGS) -o game graphics.o physics.o input.o $(LDLIBS)\rgraphics.o: graphics.c graphics.h\r$(CC) -c $(CFLAGS) graphics.c\rphysics.o: physics.c physics.h\r$(CC) -c $(CFLAGS) physics.c\rinput.o: input.c input.h graphics.h physics.h\r$(CC) -c $(CFLAGS) input.c\rclean:\rrm -f game graphics.o physics.o input.o\r通过 name=value 的形式，可以用命令行参数的方式对覆盖已有的宏定义。这是 make 其中一个非常强大，但是尚未被认识到的特性。\n$ make CC=clang CFLAGS='-O3 -march=native'\r如果用户不想在每次调用时指定这些宏，他们可以（小心）使用 make 的 -e 标志从环境中覆盖宏定义。\n$ export CC=clang\r$ export CFLAGS=-O3\r$ make -e all\r除了简单赋值（=）, 一些 make 的实现有一些其他特殊的宏赋值操作符。这些并不是必要的，所以不用担心它们。\nInference rules so that you can stop repeating yourself 在三个不同的目标文件之间会有重复操作。如果有某种方式能够在这种模式通信不是更好吗？幸运的是，我们有 推断规则（inference rule）。它说的是某个特定扩展名的目标，有另一个特定扩展名的先决条件，该目标通过某种确定的方式构建。用一个例子来说明更好一些。\n在一个推断规则中，目标隐式表明了扩展名是什么。$\u0026lt; 宏展开为先决条件，这对使得推断规则变得更加通用十分重要。不幸的是，这个宏在目标规则中并不存在，这些都是有用的。\n举个例子，下面是一个推断规则，它描述了如果从一个 C 源文件构建一个 .o 的目标文件。这个特殊的规则是 make 预先定义的，所以你不必自己去定义。为了完整性，我会包含这个：\n.c.o:\r$(CC) $(CFLAGS) -c $\u0026lt;\r在它们生效之前，这些扩展名必须被加到 .SUFFIXES。有了这个，生成目标文件规则的命令就可以被省略了。\n.POSIX:\r.SUFFIXES:\rCC = cc\rCFLAGS = -W -O\rLDLIBS = -lm\rall: game\rgame: graphics.o physics.o input.o\r$(CC) $(LDFLAGS) -o game graphics.o physics.o input.o $(LDLIBS)\rgraphics.o: graphics.c graphics.h\rphysics.o: physics.c physics.h\rinput.o: input.c input.h graphics.h physics.h\rclean:\rrm -f game graphics.o physics.o input.o\r.SUFFIXES: .c .o\r.c.o:\r$(CC) $(CFLAGS) -c $\u0026lt;\r第一个空的 .SUFFIXES 会清空后缀列表(suffix list). 第二个 .SUFFIXES 将 .c 和 .o 加到现在是空的后缀列表中。\nOther target conventions 用户通常会希望有一个 install 目标，它会安装构建好的程序，库，man 手册等等。按照惯例，这个目标应该使用 PREFIX 和 DESTDIR 宏。\nPREFIX 宏默认应该为 /usr/local, 因为它是一个可以覆盖的宏，用户可以选择覆盖它将程序安装到其他地方，\r比如安装到他们的用户目录。用户应该同时为构建和安装覆盖该值，因为 prefix 可能需要会需要构建到二进制中（比如，-DPREFIX=$(PREFIX)）.\nDESTDIR 是一个用于 staged build(分段式构建) 的宏，为了打包的需要，它会安装到一个伪根目录。与 PREFIX 不同，它实际上不会从这个目录下运行。\n.POSIX:\rCC = cc\rCFLAGS = -W -O\rLDLIBS = -lm\rPREFIX = /usr/local\rall: game\rinstall: game\rmkdir -p $(DESTDIR)$(PREFIX)/bin\rmkdir -p $(DESTDIR)$(PREFIX)/share/man/man1\rcp -f game $(DESTDIR)$(PREFIX)/bin\rgzip \u0026lt; game.1 \u0026gt; $(DESTDIR)$(PREFIX)/share/man/man1/game.1.gz\rgame: graphics.o physics.o input.o\r$(CC) $(LDFLAGS) -o game graphics.o physics.o input.o $(LDLIBS)\rgraphics.o: graphics.c graphics.h\rphysics.o: physics.c physics.h\rinput.o: input.c input.h graphics.h physics.h\rclean:\rrm -f game graphics.o physics.o input.o\r你可能也想要提供一个 uninstall 的伪目标来卸载程序。\nmake PREFIX=$HOME/.local install\r其他常见的目标有 “mostlyclean”（与 clean 类似，但是不会删除构建缓慢的目标），\u0026ldquo;distclean\u0026rdquo; (与 “clean” 删除的更多)，“test” （运行测试组件），“dist”（创建一个包）。\nComplexity and growing pains make 的一大缺点是当项目不断成长时，会变得越来越麻烦。\nRecursive Makefiles 当你的项目被分为几个子目录，你可能会试图在每个子目录下放一个 Makefile ，然后递归调用。\n不要使用递归的 Makefile。它会在几个分离的 make 实例之间打破依赖树，并且常常会产生脆弱的构建。使用递归的 Makefile 毫无益处。好的选择是在项目的根目录放置一个 Makefile, 在那里进行调用。你可能需要告诉你的编辑器如何做到这一点。\n当涉及子目录下的文件时，在名字中包含子目录即可。所有 make 关心的内容都会跟之前一样正常工作，包括推断规则。\nsrc/graphics.o: src/graphics.c\rsrc/physics.o: src/physics.c\rsrc/input.o: src/input.c\rOut-of-source builds 将你的目标文件从源文件中分离出来是一个不错的想法。当谈到 make 时，总是喜忧参半。\n喜的是 make 能做。你可以为目标和先决条件设置任何你喜欢的文件名。\nobj/input.o: src/input.c\r忧的是，推断规则对于源文件之外的构建并不兼容。如果推断规则不存在，那么你就需要对每个规则重复同样的命令。对于大型项目，这太繁琐了，所以你可能想要有一些“配置”脚本，即使这些脚本是手写的，来为你生成这些重复的命令。实际上，这就是 CMake 所涉及的所有事情，再加上依赖管理。\nDependency management 项目规模越来越大的另一个问题是，在所有的源文件上跟踪所有改变过的依赖。除非你先 make clean，否则缺失一个依赖，就意味着构建可能失败.\n如果你打算用一个脚本来生成 Makefile 冗长的部分，GCC 和 Clang 都提供了一个生成所有 Makefile 依赖的特性（-MM, -MT），至少对 C 和 C++ 如此。有很多教程讲述了如何在构建时同时生成依赖，但是它很脆弱和缓慢。最好是在一次性完成，在 Makefile 中写好依赖，以便于 make 能够如期工作。如果依赖改变了，那么重新构建你的 Makefile.\n举个例子，下面是在源文件之外的构建，它一个调用 gcc 的依赖生成器的例子，而不是虚构的 input.c :\n$ gcc $CFLAGS -MM -MT '$(BUILD)/input.o' input.c\r$(BUILD)/input.o: input.c input.h graphics.h physics.h\r注意，输出的是 Makefile 的规则格式。\n不幸的是，这个特性去除了目标的路径头，所以，在实际中，使用它往往会它本来的要更复杂（比如，比要求使用 -MT）.\nMicrosoft’s Nmake 微软有一个叫做 Nmake 的 make 实现，\r它与 Visual Studio 一起发行。它几乎是一个兼容 POSIX 的 make, 但是在一些地方对与标准不同。他们的 cl.exe 编译器使用 .obj 作为目标文件扩展名， .exe 作为二进制扩展名，这两个扩展名与 unix 系统都不同，所以它有一些不同的内置推断规则。Windows 同样也缺少一个 bash 和标准的 unix 工具，所以所有的命令都会有所不同。\n在 Windows 上，并没有 rm -f 这样的替代品，所以在写 claen 目标时只能说好运了。del /f 并不能达到同样的效果。\n所以，尽管它与 POSIX make 已经很接近，但是想要写一个 Makefile 能够同时被 POSIX make 和 Nmake 同时使用，是不太实际的。需要有两个不同的 Makfile.\nMay your Makefiles be portable 有一个值得信赖，能够在任何地方工作的可移植 Makefile 是非常棒的一件事情。\rCode to the standards，然后你就不再需要特性测试或其他一些特殊处理了。\n本文译自：\rA Tutorial on Portable Makefiles\n附录：\n伪目标惯例意义all所有目标的目标，一般为编译所有的目标，对同时编译多个程序极为有用clean删除由make创建的文件install安装已编译好的程序，主要任务是完成目标执行文件的拷贝print列出改变过的源文件tar打包备份源程序，形成tar文件dist创建压缩文件，一般将tar文件压缩成Z文件或gz文件TAGS更新所有的目标，以备完整地重编译使用check和test一般用来测试makefile的流程\n附录来自清华的 MOOC 学堂在线课程 \u0026laquo;基于 Linux 的 C++ \u0026raquo;，第 12.12 - 12.14 节有讲 Makefile，初学者推荐看一下。\n","description":"","id":60,"section":"posts","tags":["makefile"],"title":"可移植的 Makefile 教程","uri":"https://starifly.github.io/posts/portable-makefile-tutorial/"},{"content":"作为一个有高尚的情操的程序员，应该学会在写非练手项目的时候构建流程不使用与本机路径相关的依赖，这样把代码往 CI 上部署、在公司与个人电脑间传输、或者给别人用的时候都方便的多。\n如果有 gradle, pip, gem, maven, sbt 这类基于包的构建工具的话，这根本就不是问题。别人写的依赖可以在远端调用，自己写的依赖也可以本地调用。\n如果有 npm, go get 这类基于版本控制的构建工具的话，这当作也只有一个小问题，就是项目维护者脑抽了一下，可能就会误伤在此期间构建程序的程序员。\n不过至少他们都能自主寻找包并下载安装，只要包是正确的就能成功安装，安装的配置是由包的开发者提供的。\nCMake 作为全宇宙 用户量 / 构建工具本身的质量 最高的构建工具，要做到这一点是很困难的 —— CMake 只有在『没有需要导入的依赖』或者『所有依赖都是系统自带的（比如在 Windows 上调用 DirectX）』情况下能做到『无痛使用』。\n一旦需要导入一些外部依赖（比如 glfw3，比如 jni），就需要用到 find_package 这个函数，which 只有满足这两个条件的时候才能成功执行：\nCMake 能找到 [package名]Config.cmake 或者 [package名]-config.cmake 中的一个文件 CMake 能找到 Find[package名].cmake 这个文件，并且这个文件里的代码能 find 到这个 package 之前在写 JNI 代码的时候需要使用 CMake 寻找 jni.h 等一系列文件，于是只需要这么一小段代码就可以把他们加进 CMake 编译时的 classpath:\nfind_package(Java REQUIRED) find_package(JNI REQUIRED) 然后我们可以在找到之后输出一下找到的 JDK 的路径：\nif (JNI_FOUND) message(STATUS \u0026#34;JNI_INCLUDE_DIRS=${JNI_INCLUDE_DIRS}\u0026#34;) message(STATUS \u0026#34;JNI_LIBRARIES=${JNI_LIBRARIES}\u0026#34;) endif () 但如果我们要调用 libclang 的时候:\nfind_package(LibClang REQUIRED) 它就会说找不到 LibClangConfig.cmake, LibClang-config.cmake, FindLibClang.cmake 中的一个。\n首先确保已经安装 clang:\n$ sudo apt install clang-dev 在这个时候我已经上 Google/StackOverflow 找了一遍了，由于 clang 这个名词的特殊性（不仅是个包，还是个 C++ 编译器），搜索 clang + cmake 都是教你怎么配置 CMake 使 CMake 调用 clang 而不是 gcc 的，搜索 libclang + cmake 得到的很多结果需要配置环境变量，把 clang/llvm 的安装路径指定好。\n而我的 clang 明明就在 PATH 里，凭什么需要另外配置呢。\n那么我就来自己想办法。\n前两者应该是安装包的时候就自带了的，既然 clang 没有，那么只能去找个 FindLibClang.cmake 啦。\n搜索 FindLibClang.cmake ，找到了一个 Emacs 插件 rtags 的一个 CMake 模块（注意这个代码是 GPLv3 的哦）：\nif (NOT LIBCLANG_ROOT_DIR) set(LIBCLANG_ROOT_DIR $ENV{LIBCLANG_ROOT_DIR}) endif () if (NOT LIBCLANG_LLVM_CONFIG_EXECUTABLE) set(LIBCLANG_LLVM_CONFIG_EXECUTABLE $ENV{LIBCLANG_LLVM_CONFIG_EXECUTABLE}) if (NOT LIBCLANG_LLVM_CONFIG_EXECUTABLE) if (APPLE) foreach(major RANGE 9 3) foreach(minor RANGE 9 0) foreach(patch RANGE 9 0) message(STATUS \u0026#34;trying llvm-config llvm-config${major}${minor} in /usr/local/Cellar/llvm/${major}.${minor}.${patch}/bin\u0026#34;) find_program(LIBCLANG_LLVM_CONFIG_EXECUTABLE NAMES llvm-config llvm-config${major}${minor} llvm-config-${major}${minor} llvm-config-${major} llvm-config${major} PATHS /usr/local/Cellar/llvm/${major}.${minor}.${patch}/bin) if (LIBCLANG_LLVM_CONFIG_EXECUTABLE) break() endif () endforeach () if (LIBCLANG_LLVM_CONFIG_EXECUTABLE) break() endif () endforeach () if (LIBCLANG_LLVM_CONFIG_EXECUTABLE) break() endif () endforeach () else () set(llvm_config_names llvm-config) foreach(major RANGE 9 3) list(APPEND llvm_config_names \u0026#34;llvm-config${major}\u0026#34; \u0026#34;llvm-config-${major}\u0026#34;) foreach(minor RANGE 9 0) list(APPEND llvm_config_names \u0026#34;llvm-config${major}${minor}\u0026#34; \u0026#34;llvm-config-${major}.${minor}\u0026#34; \u0026#34;llvm-config-mp-${major}.${minor}\u0026#34;) endforeach () endforeach () find_program(LIBCLANG_LLVM_CONFIG_EXECUTABLE NAMES ${llvm_config_names}) endif () endif () if (LIBCLANG_LLVM_CONFIG_EXECUTABLE) message(STATUS \u0026#34;llvm-config executable found: ${LIBCLANG_LLVM_CONFIG_EXECUTABLE}\u0026#34;) endif () endif () if (NOT LIBCLANG_CXXFLAGS) if (NOT LIBCLANG_LLVM_CONFIG_EXECUTABLE) message(FATAL_ERROR \u0026#34;Could NOT find llvm-config executable and LIBCLANG_CXXFLAGS is not set \u0026#34;) endif () execute_process(COMMAND ${LIBCLANG_LLVM_CONFIG_EXECUTABLE} --cxxflags OUTPUT_VARIABLE LIBCLANG_CXXFLAGS OUTPUT_STRIP_TRAILING_WHITESPACE) if (NOT LIBCLANG_CXXFLAGS) find_path(LIBCLANG_CXXFLAGS_HACK_CMAKECACHE_DOT_TEXT_BULLSHIT clang-c/Index.h HINTS ${LIBCLANG_ROOT_DIR}/include NO_DEFAULT_PATH) if (NOT EXISTS ${LIBCLANG_CXXFLAGS_HACK_CMAKECACHE_DOT_TEXT_BULLSHIT}) find_path(LIBCLANG_CXXFLAGS clang-c/Index.h) if (NOT EXISTS ${LIBCLANG_CXXFLAGS}) message(FATAL_ERROR \u0026#34;Could NOT find clang include path. You can fix this by setting LIBCLANG_CXXFLAGS in your shell or as a cmake variable.\u0026#34;) endif () else () set(LIBCLANG_CXXFLAGS ${LIBCLANG_CXXFLAGS_HACK_CMAKECACHE_DOT_TEXT_BULLSHIT}) endif () set(LIBCLANG_CXXFLAGS \u0026#34;-I${LIBCLANG_CXXFLAGS}\u0026#34;) endif () string(REGEX MATCHALL \u0026#34;-(D__?[a-zA-Z_]*|I([^\\\u0026#34; ]+|\\\u0026#34;[^\\\u0026#34;]+\\\u0026#34;))\u0026#34; LIBCLANG_CXXFLAGS \u0026#34;${LIBCLANG_CXXFLAGS}\u0026#34;) string(REGEX REPLACE \u0026#34;;\u0026#34; \u0026#34; \u0026#34; LIBCLANG_CXXFLAGS \u0026#34;${LIBCLANG_CXXFLAGS}\u0026#34;) set(LIBCLANG_CXXFLAGS ${LIBCLANG_CXXFLAGS} CACHE STRING \u0026#34;The LLVM C++ compiler flags needed to compile LLVM based applications.\u0026#34;) unset(LIBCLANG_CXXFLAGS_HACK_CMAKECACHE_DOT_TEXT_BULLSHIT CACHE) endif () if (NOT EXISTS ${LIBCLANG_LIBDIR}) if (NOT LIBCLANG_LLVM_CONFIG_EXECUTABLE) message(FATAL_ERROR \u0026#34;Could NOT find llvm-config executable and LIBCLANG_LIBDIR is not set \u0026#34;) endif () execute_process(COMMAND ${LIBCLANG_LLVM_CONFIG_EXECUTABLE} --libdir OUTPUT_VARIABLE LIBCLANG_LIBDIR OUTPUT_STRIP_TRAILING_WHITESPACE) if (NOT EXISTS ${LIBCLANG_LIBDIR}) message(FATAL_ERROR \u0026#34;Could NOT find clang libdir. You can fix this by setting LIBCLANG_LIBDIR in your shell or as a cmake variable.\u0026#34;) endif () set(LIBCLANG_LIBDIR ${LIBCLANG_LIBDIR} CACHE STRING \u0026#34;Path to the clang library.\u0026#34;) endif () if (NOT LIBCLANG_LIBRARIES) find_library(LIBCLANG_LIB_HACK_CMAKECACHE_DOT_TEXT_BULLSHIT NAMES clang libclang HINTS ${LIBCLANG_LIBDIR} ${LIBCLANG_ROOT_DIR}/lib NO_DEFAULT_PATH) if (LIBCLANG_LIB_HACK_CMAKECACHE_DOT_TEXT_BULLSHIT) set(LIBCLANG_LIBRARIES \u0026#34;${LIBCLANG_LIB_HACK_CMAKECACHE_DOT_TEXT_BULLSHIT}\u0026#34;) else () find_library(LIBCLANG_LIBRARIES NAMES clang libclang) if (NOT EXISTS ${LIBCLANG_LIBRARIES}) set (LIBCLANG_LIBRARIES \u0026#34;-L${LIBCLANG_LIBDIR}\u0026#34; \u0026#34;-lclang\u0026#34; \u0026#34;-Wl,-rpath,${LIBCLANG_LIBDIR}\u0026#34;) endif () endif () unset(LIBCLANG_LIB_HACK_CMAKECACHE_DOT_TEXT_BULLSHIT CACHE) endif () set(LIBCLANG_LIBRARY ${LIBCLANG_LIBRARIES} CACHE FILEPATH \u0026#34;Path to the libclang library\u0026#34;) if (LIBCLANG_LLVM_CONFIG_EXECUTABLE) execute_process(COMMAND ${LIBCLANG_LLVM_CONFIG_EXECUTABLE} --version OUTPUT_VARIABLE LIBCLANG_VERSION_STRING OUTPUT_STRIP_TRAILING_WHITESPACE) else () set(LIBCLANG_VERSION_STRING \u0026#34;Unknown\u0026#34;) endif () message(\u0026#34;-- Using Clang version ${LIBCLANG_VERSION_STRING} from ${LIBCLANG_LIBDIR} with CXXFLAGS ${LIBCLANG_CXXFLAGS}\u0026#34;) # Handly the QUIETLY and REQUIRED arguments and set LIBCLANG_FOUND to TRUE if all listed variables are TRUE include(FindPackageHandleStandardArgs) find_package_handle_standard_args(LibClang DEFAULT_MSG LIBCLANG_LIBRARY LIBCLANG_CXXFLAGS LIBCLANG_LIBDIR) mark_as_advanced(LIBCLANG_CXXFLAGS LIBCLANG_LIBRARY LIBCLANG_LLVM_CONFIG_EXECUTABLE LIBCLANG_LIBDIR) 其实我中间找到过两三个类似的，但那几个 FindLibClang 都写的不稳，在我的电脑上都不 work ，需要环境变量。\n而看这个实现，它是借助 llvm-config 这个程序（就是配置 llvm 环境的时候的标配啦，一般是 c++ $(llvm-config --cxxflags) main.cpp 这样用的）查找的 clang 配置，稳如老狗。\n根据它在注释里写的：\n# FindLibClang # # This module searches libclang and llvm-config, the llvm-config tool is used to # get information about the installed llvm/clang package to compile LLVM based # programs. # # It defines the following variables # # ``LIBCLANG_LLVM_CONFIG_EXECUTABLE`` # the llvm-config tool to get various information. # ``LIBCLANG_LIBRARIES`` # the clang libraries to link against to use Clang/LLVM. # ``LIBCLANG_LIBDIR`` # the directory where the clang libraries are located. # ``LIBCLANG_FOUND`` # true if libclang was found # ``LIBCLANG_VERSION_STRING`` # version number as a string # ``LIBCLANG_CXXFLAGS`` # the compiler flags for files that include LLVM headers 生成的变量都写清楚了，所以我们就可以很容易地导入它了。\n在我们自己项目里，先把上面的 FindLibClang.cmake 放进 项目根目录/cmake-modules/ 目录里（其实就是随便一个目录），然后在 CMakeLists.txt 里写：\nset(CMAKE_MODULE_PATH ${CMAKE_MODULE_PATH} \u0026#34;${CMAKE_SOURCE_DIR}/cmake-modules/\u0026#34;) find_package(LibClang REQUIRED) set(CMAKE_CXX_FLAGS \u0026#34;${CMAKE_CXX_FLAGS} ${LIBCLANG_CXXFLAGS}\u0026#34;) 就可以让 CMake 找到 clang 的头文件啦。\n然后就可以在 CLion 里面写这样的代码了：\n#include \u0026lt;clang/AST/ASTConsumer.h\u0026gt; #include \u0026lt;clang/AST/RecursiveASTVisitor.h\u0026gt; #include \u0026lt;clang/Frontend/CompilerInstance.h\u0026gt; #include \u0026lt;clang/Frontend/FrontendAction.h\u0026gt; #include \u0026lt;clang/Tooling/Tooling.h\u0026gt; 可以正确地 navigate 到对应的文件，自动生成 override 之类的特性都可以用了。\nclass FindNamedClassAction : public clang::ASTFrontendAction { protected: std::unique_ptr\u0026lt;clang::ASTConsumer\u0026gt; CreateASTConsumer(clang::CompilerInstance \u0026amp;CI, StringRef InFile) override { return std::unique_ptr\u0026lt;clang::ASTConsumer\u0026gt;(new FindNamedClassASTConsumer(CI.getASTContext())); } }; auto main(int argc, const char **const argv) -\u0026gt; int { if (argc \u0026gt; 1) clang::tooling::runToolOnCode(new FindNamedClassAction, argv[1]); return 0; } 目前我还没找到如何正确编译和链接程序的方法，这个代码会导致 unresolved reference:\ntarget_link_libraries(clang_test ${LIBCLANG_LIBRARIES}) 网上也找不到解决方案，只能自己脑补程序运行的样子。\n","description":"","id":61,"section":"posts","tags":["cmake","clang"],"title":"使用 CMake 不用路径地调用 libclang","uri":"https://starifly.github.io/posts/cmake-clang/"},{"content":"本教程系列将以 Travis CI 为主，我也不知道以后会不会讲 AppVeyor ，我也不知道以后会不会讲 Circle CI 和 CodeShip 。\n这篇文章你可以把它当成一个索引，我给出了使用 Travis 需要阅读的内容，读者可以根据自己的需求选择阅读文档的特定部分。\nCI 能做啥 能帮你在云端自动编译项目 每次你推送代码就会触发编译 可以保留编译生成的目标文件 自动上传 release 编译失败发邮件提醒你 编译失败发 Slack 消息提醒你 等等功能（这些都是最基本的）\n混开源社区的 friends 喜欢使用一些现有的 CI 服务，比如 Travis, AppVeyor, Circle CI, CodeShip 等，\n公司企业喜欢自己写 CI 自己用，因为这本来就是个高度定制的东西，要是你能提供高度定制的环境（比如装好了依赖的服务器）\n当然做 CI 就超级简单了。\n但是我们是混开源社区的 friends ，所以没有这种操作，首选当然是 Travis CI 。\n理由： Linux + 自动部署\ngetting started 我才懒得写， RTFM\n流程大概就是先注册 Travis 账号，然后在仓库里面写个配置文件 .travis.yml ，然后在 Travis 网站上选这个项目，\n然后就可以等 build 了。 Travis 提供了一个 build 的 badge ，可以放在 README 里面装逼（雾\n可以让 Travis 自动测试，这样可以在别人发 pull request 的时候先通过 CI 的检验， CI 上编译测试通过了再 merge ，\n明显有安全感的多。\n关于自动 release 就是每次 push 之后它不是会帮你编译吗，你可以让它把目标文件(artifact)上传到 release 里面。\n这里推荐先搭建本地的 Travis 环境（就是 Ruby 环境 + gem install travis）然后让它自己配置。\n这样不会泄露你的 GitHub Token 。教程见：\nRTFM\n注意 一定要记得加上\non: tags: true 否则你每次推送都会收到一个 artifact 。加上之后就只有在创建 release 的时候才会上传。\n于是就不需要在 release 的时候再上传目标文件了。\n还可以本地 debug build ，然后让 CI 跑 release build ，不需要编译二遍。\nTravis CI 的缺点 触发慢 触发慢 触发慢 触发慢 每次 push algo4j 后都能感受到 Travis CI 满满的恶意，\n一般得等个一分钟才能看到它开始编译（这是因为需要 sudo）。\n不过它编译本身速度还是可以的，还有 CI 上下载各种源啊， clone 各种仓库啊也都是快到没朋友。\n我部署了 Flutter 应用就是这样的，我先让 CI clone Flutter ，再编译 Flutter ，再下载 Android SDK ，再安装谷歌支持库，\n再用 Flutter 编译我的项目，整个过程不到 4 分钟。\n这个问题在 AppVeyor上得到了很好的解决， AppVeyor 非常非常快(触发和编译都快)，但是它是 Windows 的，而且各种预装的依赖略有些迷醉。\n我自己也在用，而且非常舒服，这里就先不说了。\n","description":"","id":62,"section":"posts","tags":["CI"],"title":"持续集成教程 1 通识科普","uri":"https://starifly.github.io/posts/travis-basis/"},{"content":"因为饱受 GFM 和 Redcarpet 两种 Markdown 引擎生成 TOC 链接的差异的折磨，而我又不得不同时使用它们——博客基于 Jekyll 使用 Redcarpet（Update 2016/09/16: GitHub Pages 现在已经改为只支持 kramdown），而其它放在 GitHub 仓库里的文档使用 GFM，我决定为我常用的 Markdown 编辑器 Vim 做一款同时支持 GFM 和 Redcarpet 两种 TOC 链接风格的 Table of Contents 自动生成插件。\n这算是我真正意义上完全独立开发的第一款实用 Vim 插件，当然开发过程中也参考了别人的做法。\n下载地址 vim-markdown-toc 功能 为 Markdown 文件生成 Table of Contents，目前支持 GFM 和 Redcarpet 两种链接风格。\n更新已经存在的 Table of Contents。\n保存文件时自动更新 Table of Contents。\n使用方法 生成 Table of Contents 将光标移动到想在后面插入 Table of Contents 的那一行，然后运行下面的某个命令：\n:GenTocGFM\n生成 GFM 链接风格的 Table of Contents。\n适用于 GitHub 仓库里的 Markdown 文件，比如 README.md，也适用用于生成 GitBook 的 Markdown 文件。\n:GenTocRedcarpet\n生成 Redcarpet 链接风格的 Table of Contents。\n适用于使用 Redcarpet 作为 Markdown 引擎的 Jekyll 项目或其它地方。\n更新已存在的 Table of Contents 通常不需要手动做这件事，保存文件时会自动更新已经存在的 Table of Contents。\n除非是在配置里关闭了保存时自动更新，并且维持插入 Table of Contents 前后的 \u0026lt;!-- vim-markdown-toc --\u0026gt;，此时可使用 :UpdateToc 命令手动更新。\n删除 Table of Contents :RemoveToc 命令可以帮你删除本插件生成的 Table of Contents。\n安装方法 推荐使用 Vundle 来管理你的 Vim 插件，这样你就可以简单三步完成安装：\n在你的 vimrc 文件中添加如下内容：\nPlugin \u0026#39;mzlogin/vim-markdown-toc\u0026#39; :so $MYVIMRC\n:PluginInstall\n使用 vim-plug 安装的过程的与此基本一样。\n配置选项 g:vmt_auto_update_on_save\n默认值：1\n插件会自动更新已经存在的 Table of Contents，如果你不想要这个功能，可以在你的 vimrc 文件里加入如下内容关闭：\nlet g:vmt_auto_update_on_save = 0 g:vmt_dont_insert_fence\n默认值：0\n在默认情况下，:GenTocXXX 命令会在插入的 Table of Contents 前后加上 \u0026lt;!-- vim-markdown-toc --\u0026gt;，这是为了实现自动和手动更新 Table of Contents 功能。\n如果你不想看到它们，可以在 vimrc 文件里加入如下内容移除：\nlet g:vmt_dont_insert_fence = 1 需要注意的是移除之后插件将无法再帮你保存文件时自动更新 Table of Contents 了，也无法使用 :UpdateToc 命令了。这里如果还想更新 Table of Contents，只能先手动删除已经存在的，然后重新运行 :GenTocXXX 命令。\ng:vmt_cycle_list_item_markers\n默认值：0\n在默认情况下，所有 Table of Contents 项目前面的标记都是 *：\n* [Level 1](#level-1)\r* [Level 1-1](#level-1-1)\r* [Level 1-2](#level-1-2)\r* [Level 1-2-1](#level-1-2-1)\r* [Level 2](level-2) 这里提供一个选项改变这个行为，如果设置：\nlet g:vmt_cycle_list_item_markers = 1 那标记将根据级别循环使用 *、- 和 +：\n* [Level 1](#level-1)\r- [Level 1-1](#level-1-1)\r- [Level 1-2](#level-1-2)\r+ [Level 1-2-1](#level-1-2-1)\r* [Level 2](level-2) 这不会影响 Markdown 文档解析后的显示效果，只用于提升源文件的可读性。\n屏幕截图 使用本插件生成 TOC 的英文文档在线示例\n使用本插件生成 TOC 的中文文档在线示例\n参考链接 GFM 与 Redcarpet 的不同点 ajorgensen/vim-markdown-toc ","description":"","id":63,"section":"posts","tags":["vim","markdown"],"title":"为 Markdown 生成 TOC 的 Vim 插件","uri":"https://starifly.github.io/posts/vim-markdown-toc/"},{"content":"使用 Ubuntu 过程中遇到的问题及解决方案。\n使用 git pull 遇到问题 提示\nAgent admitted failure to sign using the key.\rPermission denied (publickey).\rfatal: Could not read from remote repository.\rPlease make sure you have the correct access rights\rand the repository exists. 解决方法：\nssh-add ~/.ssh/id_rsa 图形界面编辑配置文件 安装 dconf-editor。\n配置 Exchange 为 ThunderBird 安装插件 ExQuilla，有时被墙。\nhttp://mesquilla.net/exquilla-currentrelease-tb-linux.xpi\n安装和配置 JDK 在 Terminal 运行：\nsudo add-apt-repository ppa:webupd8team/java sudo apt-get update sudo apt-get install oracle-java8-installer sudo vim /etc/profile export JAVA_HOME=/usr/lib/jvm/java-8-oracle export JRE_HOME=$JAVA_HOME/jre export CLASSPATH=.:$JAVA_HOME/lib:$JRE_HOME/lib export PATH=$JAVA_HOME/bin:$JRE_HOME/bin:$PATH 配置 adt 安装兼容 32 位 adb 运行的环境\nsudo apt-get install lib32z1 lib32ncurses5 lib32bz2-1.0 lib32stdc++6 添加路径到 $PATH 环境变量，修改 /etc/profile 或 ~/.profile 等皆可。\nexport ANDROID_SDK_HOME=/home/mzlogin/android/sdk\rexport PATH=$ANDROID_SDK_HOME/platform-tools:$ANDROID_SDK_HOME/tools:$PATH 安装 SVN 图形前端 RabbitVCS 在 Terminal 运行：\nsudo add-apt-repository ppa:rabbitvcs/ppa sudo apt-get update sudo apt-get install rabbitvcs-nautilus3 rabbitvcs-thunar rabbitvcs-gedit rabbitvcs-cli 创建 eclipse 快捷方式 /usr/share/applications 里新建 Eclipse.desktop，填如下内容：\n[Desktop Entry]\rName=Eclipse\rComment=Launch Eclipse\rExec=/home/mzlogin/android/eclipse/eclipse\rIcon=/home/mzlogin/android/eclipse/icon.xpm\rStartupNotify=true\rTerminal=false\rType=Application 安装 XMind 到 XMind 官网下载安装包，然后 ：\nsudo dpkg --ignore-depends=lame,libwebkitgtk-1.0-0 -i xmind-linux-3.5.0.201410310637_amd64.deb 切换输入法 添加一个英文，一个五笔，将切换到上一个源和下一个源的快捷键分别设为左和右 Shift，这样就可以使用左右 Shift 在中英之间来回切换了。\n安装 im-switch 会导致语言支持被移除，恢复用 sudo apt-get install language-selector-gnome。\n消除启动 gVim 在 terminal 中的警告 安装 vim-gnome 后运行 gVim 会提示：\nGLib-GObject-WARNING **: Attempt to add property GnomeProgram::sm-connect after class was initialised 改为安装 vim-gtk 就好了。\n解决 ibus 五笔候选词水平显示和个数的问题 修改 /usr/share/ibus-table/tables/wubi-jidian86.db 的 ime 表里的 orientation（水平 0 垂直 1）和 select_keys（有几个选择键就有几个项，从下面代码可知用 , 分隔）。\n/usr/share/ibus-tables/engine/tabsqlitedb.py 中\ndef get_page_size (self): return len(self.get_select_keys().split(\u0026#39;,\u0026#39;)) 将 Caps Lock 映射为 Ctrl 安装 Gnome Tweak Tool\nsudo apt-get install gnome-tweak-tool 打开 tweak-tool，找到「打字」－「大写锁定键行为」，选择「将 CapsLock 作为额外的 Ctrl」\n参考 http://askubuntu.com/questions/462021/how-do-i-turn-caps-lock-into-an-extra-control-key\n将个人文件夹下文件夹名改为英文 ~ 目录下的「桌面」和「文档」等文件夹是中文，在 Terminal 下输入很不方便，将其改为英文的方法：\n打开 ~/.config/user-dirs.dirs，将其中的中文改掉：\nXDG_DESKTOP_DIR=\u0026#34;$HOME/desktop\u0026#34;\rXDG_DOWNLOAD_DIR=\u0026#34;$HOME/downloads\u0026#34;\rXDG_TEMPLATES_DIR=\u0026#34;$HOME/templates\u0026#34;\rXDG_PUBLICSHARE_DIR=\u0026#34;$HOME/public\u0026#34;\rXDG_DOCUMENTS_DIR=\u0026#34;$HOME/documents\u0026#34;\rXDG_MUSIC_DIR=\u0026#34;$HOME/music\u0026#34;\rXDG_PICTURES_DIR=\u0026#34;$HOME/pictures\u0026#34;\rXDG_VIDEOS_DIR=\u0026#34;$HOME/videos\u0026#34; 在文件管理器中将 HOME 目录下的中文文件夹名改成与上面的配置对应。\n输入「」与『』 极点五笔中文输入状态下，按 [] 即输入「」，按 {} 即输入『』。\nVirtualBox 里 Ubuntu 分辨率无法调整 Ubuntu 14.04 LTS 在 VirtualBox 中刚安装完时，分辨率只有 640*480 一种选项，无法调整。\n解决方法：\n打开 xdiagnose\n勾选 Debug 下的所有选项\n重启\n安装增强功能\n然后：\ncd /media/\u0026lt;username\u0026gt;/VBOXADDITIONS_X.X.XX_XXXXX\rsudo ./VBoxLinuxAdditions.run （注意把 username 替换成自己的，VBOXADDITIONS 后面的 X 换成具体版本号）\n与 Win7 共享 SSH key 如下步骤适用于在 Ubuntu 上使用从 Win7 拷贝的 SSH key，反之应该也一样能用。\n创建 ~/.ssh 目录，确认其权限为 0700，将 Windows %userprofile%/.ssh 下的 id_rsa 和 id_rsa.pub 文件拷贝到 ~/.ssh 目录下，权限分别改为 0600 和 0644。\nmzlogin@ubuntu:~$ ll ~/.ssh total 20 drwx------ 2 mzlogin mzlogin 4096 Jun 22 01:03 ./ drwxr-xr-x 20 mzlogin mzlogin 4096 Jun 22 01:02 ../ -rw------- 1 mzlogin mzlogin 1679 Jun 21 05:17 id_rsa -rw-r--r-- 1 mzlogin mzlogin 399 Jun 21 05:17 id_rsa.pub 然后\nssh-add ~/.ssh/id_rsa ","description":"","id":64,"section":"posts","tags":["linux","ubuntu"],"title":"Ubuntu 使用笔记","uri":"https://starifly.github.io/posts/use-ubuntu-notes/"},{"content":"开始 拖延症与强迫症严重一直拖到了今天2月23号才开始写。这一段时间我也明白了博客最重要的是keep blogging,所以 想到什么就写什么，没有什么东西一次就能写的完美。这次我准备简单介绍一下当前非常火的代码托管与社会协作开发平台\u0026ndash;\rgithub 和版本控制系统\u0026ndash;\rgit。\n为什么推荐git\u0026amp;github? 你所知道的几乎所有很火的开源软件都能在github上面找到源代码。对于使用R软件的，你会发现\r谢益辉的开发的R包都在github上面，还有\rHadley Wickham\u0026ndash;\r一个改变了R的人 的全部R包都在他的github主页上面。Github上面不仅有这些大神级的人物存在，也有不少小牛们，今天看到了一个非常炫的可视化主页，大家点开看肯定会被astonished的，网址在\r这里.\nGithub上面还有各种当下最流行是算法包，比如\rDMLC 里的\rxgboost 打破kaggle竞赛Python垄断的局面; mxnet基于GPU的深度学习算法，\r一个例子 详细了解请看这篇cos访谈：\r陈天奇\n总的来说github就是一个巨大的宝库，去上面看看能让我们能紧跟时代潮流，通过查看源代码以及贡献代码(pull request)来实现学习与提高。这么好的东西，良心小赵会私藏吗？当然会无私的与你们分享。\nGit是个一个优秀的版本控制系统，开源中国给出\r八大理由使用Git.相比于其他的版本控制系统Git的优势有人总结过，我当然没有人家写的好，所有就贴上\r链接,供大家参考。\n简单入门 协作与进步 小赵第一次注册Github是2014年的暑假，没错，那个暑假也就是我考研的暑假，大学的最后一个暑假 \u0026#x1f62d; . 这个也是我在考研的时候弄的杂七杂八的事情之一，当时对Github完全没有概念，懵逼的状态。一直到了2015年9月份看了几篇写博客的各种好处的软文，才开始用起了这个账户。想知道如何用Github Pages \u0026amp; Jekyll搭建个人博客请自行搜索这个方面的文章。小赵到现在也是用了大半年了，算是基本入门了。我的主页在这里:\rhttps://github.com/BruceZhaoR\nGithub上面有一个organisation，可以进行团队开发与协作，这也是我萌发建立\rR共享学习社区的初衷。毕竟一个人的精力是有限的，多人合作才能发挥巨大作用。只需要加入这个组织，然后把自己学习的笔记、心得什么的以markdown的形式提交到上面去，后来人看到了这些无疑是巨大的帮助而且省时省力。当然也欢迎大家投稿(txt,word,markdown,Rmarkdown,html\u0026hellip;)，我会将其挂到学习社区的主页博客上面，这样你的文章不仅可以作为自己的备忘(等你哪天忘记了就可以到网上来看看自己当初是怎么做的)，而且还能帮助其他人。\n拥有一个Github账号 前面说了这么多，相信大家已经对Github有了初步的了解。接下来你只需要用你的常用邮箱注册一个Github账号就可以开始了。网址在这里：\rhttps://github.com .\n几点说明：\nContributions你贡献的仓库/项目 repository就是一个仓库，用于存放代码、文件之类 Origanizations 你所属的组织 点击右上角的 + 来新建一个repository吧，取名 hello-world . 创建新的repository的时候，一般会要你选择是否要README和license，我一般都选。License的区别看\r这里. GitHub公共(public)的仓库是免费的,private仓库是收费的，反正目前我们也用不到private的仓库。最后你需要点击右上角你的头像-\u0026gt;settings,来进行一些基本的设置。首先是你的profile，然后是通知设置，建议全部勾选；关于SSH Keys下面就要说到；security里面有一个两步验证，感觉目前还是用不到的。\n至此，你已经拥有了Github账号了，你可以去这里\rexplore流行的项目,或者在这里来看\r趋势,最近流行什么项目；当然不能少了R相关的资料,点击查看github上面\r流行的R包.\n简单提高 Git安装 Github最近添加了上传文件的功能，至此方便了不少不会git的用户。但是为了发挥Github的优势，还是需要去用git的。关于git网上有很多的快速教程命令，这里推荐官方的教程\rGit Pro v2.0-zh，可以系统了解git及其基本特性。下载就去官方网站\rhttp://git-scm.com/downloads 默认安装就行了(选第一个-\u0026gt;下一步就行了)。\n安装完成后，右键你会发现多了两个选项，git bash here和git GUI here。现在就可以使用git了。git是可以离线使用的，等你有网了，就可以push到Github上面，实现同步。\n关于git教程 随便搜一搜就有的，这里墙裂推荐这个\r网址1,\r网址2.再就是：\r看日记学git 和 廖雪峰git教程。\n小赵告诉你，新手其实只需会git的“三板斧”足矣。\ngit clone git@github.com:账户名/repository git add . git commit -m \u0026ldquo;随便说点什么\u0026rdquo; 等你有网了，需要同步了，你只需要 git push,就可以将本地文件同步到了Github上面了.Github上面的公开的项目任何人都是可以下载的，为了确保你的repository只能由你来管理，需要在自己的电脑里面设置ssh key，让Git与Github之间的连接起来,下面就要讲到SSH Key的生成与添加。\nSSH Keys Github上面有一篇\r官方教程，虽然是全英文的，但是不影响使用,打开Git,照着上面敲一敲代码就ok了。这里小赵总结一下：\n1. $ ssh-keygen -t rsa -C \u0026#34;你注册用的邮箱\u0026#34; 2. 一路`Enter`下去 3. $ eval $(ssh-agent -s) #确认ssh-agent可用 4. $ ssh-add ~/.ssh/id_rsa #添加到ssh-agent 5. $ clip \u0026lt; ~/.ssh/id_rsa.pub #复制到粘贴板 接下来需要去\r这里,按照步骤将ssh key添加到github账户。添加完成后，需要测试一下是否连接成功：ssh -T git@github.com .顺利的话就将会看到这样的一句话：\nHi 你的用户名! You\u0026rsquo;ve successfully authenticated, but GitHub does not provide shell access.\n一个小例子 弄到了这一步也是不容易，下面简单介绍一下git的使用方法。现假设你在github上面已经建立了hello-world这个repository，里面有一个README和License文件。在一个文件夹里面，右键git bash here,然后输入下面的代码：\n# 全局设置 $ git config --global user.name \u0026#34;username\u0026#34; $ git config --global user.email \u0026#34;user email\u0026#34; # github网页端同步到本地 $ git clone git@github.com:你的用户名/hello-world.git $ cd hello-world 这个时候你可以打开hello-world文件夹，然后打开README，用txt打开。这里小赵建议是去下个notepad++ 来打开。然后你可以随便写点上面东西。这个是markdown的格式，关于markdown基本语法看\r这里\u0026amp;FurtherReading。修改后，保存然后去git里面输入:\n$ git status #查看状态 $ git add . #暂存文件 $ git status $ git commit -m \u0026#34;update README\u0026#34; #commit以后就可以提交了 $ git push # 提交到github 这里就简单写一个例子，更多的请见上面提到的教程。\n总结 看一遍，再操作一遍应该就能掌握基本的使用。然后教程刷一遍，就算是入门了。下篇小赵将总结git常用的操作。\n","description":"","id":65,"section":"posts","tags":["git"],"title":"Git 和 GitHub简易入门","uri":"https://starifly.github.io/posts/git-intro/"},{"content":"自从几年前开始在 GitHub 玩耍，接触到 Markdown 之后，就一发不可收拾，在各种文档编辑上，有条件用 Markdown 的尽量用，不能用的创造条件也要用——README、博客、公众号、接口文档等等全都是，比如当前这篇文章就是用 Markdown 编辑而成。\n这几年也发现越来越多的网站和程序提供了对 Markdown 的支持，从最初接触的 GitHub、Jekyll，到简书、掘金、CSDN 等等，由此也从别人做得好的文档中，学到了一些『奇技淫巧』，所以本文不是对 Markdown 基础语法的介绍，而是一些相对高级、能将 Markdown 玩出更多花样的小技巧。\n注：如下技巧大多是利用 Markdown 兼容部分 HTML 标签的特性来完成，不一定在所有网站和软件里都完全支持，主要以 GitHub 支持为准。\n在表格单元格里换行 借助于 HTML 里的 \u0026lt;br /\u0026gt; 实现。\n示例代码：\n| Header1 | Header2 |\r|---------|----------------------------------|\r| item 1 | 1. one\u0026lt;br /\u0026gt;2. two\u0026lt;br /\u0026gt;3. three | 示例效果：\nHeader1 Header2 item 1 1. one\n2. two\n3. three 图文混排 使用 \u0026lt;img\u0026gt; 标签来贴图，然后指定 align 属性。\n示例代码：\n\u0026lt;img align=\u0026#34;right\u0026#34; src=\u0026#34;https://raw.githubusercontent.com/mzlogin/mzlogin.github.io/master/images/posts/markdown/demo.png\u0026#34;/\u0026gt;\r这是一个示例图片。\r图片显示在 N 段文字的右边。\rN 与图片高度有关。\r刷屏行。\r刷屏行。\r到这里应该不会受影响了，本行应该延伸到了图片的正下方，所以我要足够长才能确保不同的屏幕下都看到效果。 示例效果：\n这是一个示例图片。\n图片显示在 N 段文字的右边。\nN 与图片高度有关。\n刷屏行。\n刷屏行。\n到这里应该不会受影响了，本行应该延伸到了图片的正下方，所以我要足够长才能确保不同的屏幕下都看到效果。\n控制图片大小和位置 标准的 Markdown 图片标记 ![]() 无法指定图片的大小和位置，只能依赖默认的图片大小，默认居左。\n而有时候源图太大想要缩小一点，或者想将图片居中，就仍需要借助 HTML 的标签来实现了。图片居中可以使用 \u0026lt;div\u0026gt; 标签加 align 属性来控制，图片宽高则用 width 和 height 来控制。\n示例代码：\n**图片默认显示效果：**\r![](https://raw.githubusercontent.com/mzlogin/mzlogin.github.io/master/images/posts/markdown/demo.png)\r**加以控制后的效果：**\r\u0026lt;div align=\u0026#34;center\u0026#34;\u0026gt;\u0026lt;img width=\u0026#34;65\u0026#34; height=\u0026#34;75\u0026#34; src=\u0026#34;https://raw.githubusercontent.com/mzlogin/mzlogin.github.io/master/images/posts/markdown/demo.png\u0026#34;/\u0026gt;\u0026lt;/div\u0026gt; 示例效果：\n图片默认显示效果：\n加以控制后的效果：\n格式化表格 表格在渲染之后很整洁好看，但是在文件源码里却可能是这样的：\n|Header1|Header2|\r|---|---|\r|a|a|\r|ab|ab|\r|abc|abc| 不知道你能不能忍，反正我是不能忍。\n好在广大网友们的智慧是无穷的，在各种编辑器里为 Markdown 提供了表格格式化功能，比如我使用 Vim 编辑器，就有 vim-table-mode 插件，它能帮我自动将表格格式化成这样：\n| Header1 | Header2 |\r|---------|---------|\r| a | a |\r| ab | ab |\r| abc | abc | 是不是看着舒服多了？\n如果你不使用 Vim，也没有关系，比如 Atom 编辑器的 markdown-table-formatter 插件，Sublime Text 3 的 MarkdownTableFormatter 等等，都提供了类似的解决方案。\n使用 Emoji 这个是 GitHub 对标准 Markdown 标记之外的扩展了，用得好能让文字生动一些。\n示例代码：\n我和我的小伙伴们都笑了。:smile: 示例效果：\n我和我的小伙伴们都笑了。\u0026#x1f604;\n更多可用 Emoji 代码参见 https://www.webpagefx.com/tools/emoji-cheat-sheet/。\n行首缩进 直接在 Markdown 里用空格和 Tab 键缩进在渲染后会被忽略掉，需要借助 HTML 转义字符在行首添加空格来实现，\u0026amp;ensp; 代表半角空格，\u0026amp;emsp; 代表全角空格。\n示例代码：\n\u0026amp;emsp;\u0026amp;emsp;春天来了，又到了万物复苏的季节。 示例效果：\n春天来了，又到了万物复苏的季节。\n展示数学公式 如果是在 GitHub Pages，可以参考 http://wanguolin.github.io/mathmatics_rending/ 使用 MathJax 来优雅地展示数学公式（非图片）。\n如果是在 GitHub 项目的 README 等地方，目前我能找到的方案只能是贴图了，以下是一种比较方便的贴图方案：\n在 https://www.codecogs.com/latex/eqneditor.php 网页上部的输入框里输入 LaTeX 公式，比如 $$x=\\frac{-b\\pm\\sqrt{b^2-4ac}}{2a}$$；\n在网页下部拷贝 URL Encoded 的内容，比如以上公式生成的是 https://latex.codecogs.com/png.latex?%24%24x%3D%5Cfrac%7B-b%5Cpm%5Csqrt%7Bb%5E2-4ac%7D%7D%7B2a%7D%24%24；\n在文档需要的地方使用以上 URL 贴图，比如\n![](https://latex.codecogs.com/png.latex?%24%24x%3D%5Cfrac%7B-b%5Cpm%5Csqrt%7Bb%5E2-4ac%7D%7D%7B2a%7D%24%24) 示例效果：\n任务列表 在 GitHub 和 GitLab 等网站，除了可以使用有序列表和无序列表外，还可以使用任务列表，很适合要列出一些清单的场景。\n示例代码：\n**购物清单**\r- [ ] 一次性水杯\r- [x] 西瓜\r- [ ] 豆浆\r- [x] 可口可乐\r- [ ] 小茗同学 示例效果：\n购物清单\n一次性水杯 西瓜 豆浆 可口可乐 小茗同学 自动维护目录 有时候维护一份比较长的文档，希望能够自动根据文档中的标题生成目录（Table of Contents），并且当标题有变化时自动更新目录，能减轻工作量，也不易出错。\n如果你使用 Vim 编辑器，那可以使用我维护的插件 vim-markdown-toc 来帮你完美地解决此事：\n插件地址：\rhttps://github.com/mzlogin/vim-markdown-toc\n如果你使用其它编辑器，一般也能找到对应的解决方案，比如 Atom 编辑器的 markdown-toc 插件，Sublime Text 的 MarkdownTOC 插件等。\n后话 好了，这一波的奇技淫巧就此告一段落，希望大家在了解这些之后能有所收获，更好地排版，专注写作。\n如果你觉得本文对你有帮助，欢迎关注我的微信公众号 isprogrammer，获取更多有帮助的内容。\n参考 https://raw.githubusercontent.com/matiassingers/awesome-readme/master/readme.md https://www.zybuluo.com/songpfei/note/247346 ","description":"","id":66,"section":"posts","tags":["markdown"],"title":"关于 Markdown 的一些奇技淫巧","uri":"https://starifly.github.io/posts/markdown-odd-skills/"},{"content":"Hello world！测试内容\n","description":"","id":67,"section":"posts","tags":null,"title":"My First Post","uri":"https://starifly.github.io/posts/my-first-post/"},{"content":"I decide to publish things which are interesting to me, problem solutions, and another stuff. There are several aims for that.\nFirstly, it should help me to structure various information which i collected during work and study. Secondly, it’s a good chance to train my writing skills. And thirdly, it could be helpful for someone else. ","description":"博客简介","id":68,"section":"","tags":null,"title":"关于本站","uri":"https://starifly.github.io/about/"}]