<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Chat with LLMs from the command line | Laoluo's Blog</title>
<meta name=keywords content="project,llm-term"><meta name=description content="llm-term Chat with LLM models directly from the command line.
Screen Recording Your browser does not support the video tag. Installation pipx install llm-term Usage Then, you can chat with the model directly from the command line:
llm-term llm-term works with multiple LLM providers, but by default it uses OpenAI. Most providers require extra packages to be installed, so make sure you read the Providers section below. To use a different provider, you can set the --provider / -p flag:"><meta name=author content="James"><link rel=canonical href=http://localhost:1313/manual/linux/2023-09-26-llm-term/><link crossorigin=anonymous href=/assets/css/stylesheet.d2540e28ae03a9ed42429ed74228412a18d4ea29c639e31e4f0b1512e252624f.css integrity="sha256-0lQOKK4Dqe1CQp7XQihBKhjU6inGOeMeTwsVEuJSYk8=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=http://localhost:1313/icons/linux.png><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/manual/linux/2023-09-26-llm-term/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/lxgw-wenkai-screen-webfont@1.7.0/style.min.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fontsource/cascadia-code@4.2.1/index.min.css><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><meta property="og:title" content="Chat with LLMs from the command line"><meta property="og:description" content="llm-term Chat with LLM models directly from the command line.
Screen Recording Your browser does not support the video tag. Installation pipx install llm-term Usage Then, you can chat with the model directly from the command line:
llm-term llm-term works with multiple LLM providers, but by default it uses OpenAI. Most providers require extra packages to be installed, so make sure you read the Providers section below. To use a different provider, you can set the --provider / -p flag:"><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/manual/linux/2023-09-26-llm-term/"><meta property="article:section" content="manual"><meta property="article:published_time" content="2023-09-26T00:00:00+00:00"><meta property="article:modified_time" content="2023-09-26T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Chat with LLMs from the command line"><meta name=twitter:description content="llm-term Chat with LLM models directly from the command line.
Screen Recording Your browser does not support the video tag. Installation pipx install llm-term Usage Then, you can chat with the model directly from the command line:
llm-term llm-term works with multiple LLM providers, but by default it uses OpenAI. Most providers require extra packages to be installed, so make sure you read the Providers section below. To use a different provider, you can set the --provider / -p flag:"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Manuals","item":"http://localhost:1313/manual/"},{"@type":"ListItem","position":2,"name":"Chat with LLMs from the command line","item":"http://localhost:1313/manual/linux/2023-09-26-llm-term/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Chat with LLMs from the command line","name":"Chat with LLMs from the command line","description":"llm-term Chat with LLM models directly from the command line.\nScreen Recording Your browser does not support the video tag. Installation pipx install llm-term Usage Then, you can chat with the model directly from the command line:\nllm-term llm-term works with multiple LLM providers, but by default it uses OpenAI. Most providers require extra packages to be installed, so make sure you read the Providers section below. To use a different provider, you can set the --provider / -p flag:","keywords":["project","llm-term"],"articleBody":"llm-term Chat with LLM models directly from the command line.\nScreen Recording Your browser does not support the video tag. Installation pipx install llm-term Usage Then, you can chat with the model directly from the command line:\nllm-term llm-term works with multiple LLM providers, but by default it uses OpenAI. Most providers require extra packages to be installed, so make sure you read the Providers section below. To use a different provider, you can set the --provider / -p flag:\nllm-term --provider anthropic If needed, make sure you have your LLM’s API key set as an environment variable (this can also set via the --api-key / -k flag in the CLI). If your LLM uses a particular environment variable for its API key, such as OPENAI_API_KEY, that will be detected automatically.\nexport LLM_API_KEY=\"xxxxxxxxxxxxxx\" Optionally, you can set a custom model. llm-term defaults to gpt-3.5-turbo (this can also set via the --model / -m flag in the CLI):\nexport LLM_MODEL=\"gpt-4\" Want to start the conversion directly from the command line? No problem, just pass your prompt to llm-term:\nllm-term show me python code to detect a palindrome You can also set a custom system prompt. llm-term defaults to a reasonable prompt for chatting with the model, but you can set your own prompt (this can also set via the --system / -s flag in the CLI):\nexport LLM_SYSTEM_MESSAGE=\"You are a helpful assistant who talks like a pirate.\" Providers OpenAI By default, llm-term uses OpenAI as your LLM provider. The default model is gpt-3.5-turbo and you can also use the OPENAI_API_KEY environment variable to set your API key.\nAnthropic Anthropic is a new LLM provider that is currently in private beta. You can request access to the beta here. The default model is claude, and you can use the ANTHROPIC_API_KEY environment variable. To use anthropic as your provider you must install the anthropic extra.\npipx install \"llm-term[anthropic]\" llm-term --provider anthropic GPT4All GPT4All is a an open source LLM provider. These models run locally on your machine, so you don’t need to worry about API keys or rate limits. The default model is mistral-7b-openorca.Q4_0.gguf, and you can see what models are available on the GPT4All Website. Models are downloaded automatically when you first use them. To use GPT4All as your provider you must install the gpt4all extra.\npipx install \"llm-term[gpt4all]\" llm-term --provider gpt4all --model mistral-7b-openorca.Q4_0.gguf ","wordCount":"392","inLanguage":"en","datePublished":"2023-09-26T00:00:00Z","dateModified":"2023-09-26T00:00:00Z","author":{"@type":"Person","name":"James"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/manual/linux/2023-09-26-llm-term/"},"publisher":{"@type":"Organization","name":"Laoluo's Blog","logo":{"@type":"ImageObject","url":"http://localhost:1313/icons/linux.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Laoluo's Blog (Alt + H)">Laoluo's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=http://localhost:1313/about/ title=关于><span>关于</span></a></li><li><a href=http://localhost:1313/archives title=归档><span>归档</span></a></li><li><a href=http://localhost:1313/categories/ title=分类><span>分类</span></a></li><li><a href=http://localhost:1313/tags/ title=标签><span>标签</span></a></li><li><a href=http://localhost:1313/manual/ title=速查><span>速查</span></a></li><li><a href=http://localhost:1313/links/ title=链接><span>链接</span></a></li><li><a href=http://localhost:1313/search/ title=🔍><span>🔍</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Chat with LLMs from the command line</h1><div class=post-meta><span title='2023-09-26 00:00:00 +0000 UTC'>September 26, 2023</span>&nbsp;·&nbsp;James</div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#llm-termhttpsgithubcomjuftinllm-term aria-label=llm-term><a href=https://github.com/juftin/llm-term>llm-term</a></a><ul><li><a href=#installation aria-label=Installation>Installation</a></li><li><a href=#usage aria-label=Usage>Usage</a></li><li><a href=#providers aria-label=Providers>Providers</a><ul><li><a href=#openai aria-label=OpenAI>OpenAI</a></li><li><a href=#anthropic aria-label=Anthropic>Anthropic</a></li><li><a href=#gpt4all aria-label=GPT4All>GPT4All</a></li></ul></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><h1 id=llm-termhttpsgithubcomjuftinllm-term><a href=https://github.com/juftin/llm-term>llm-term</a><a hidden class=anchor aria-hidden=true href=#llm-termhttpsgithubcomjuftinllm-term>#</a></h1><p>Chat with LLM models directly from the command line.</p><p align=center><img width=600 alt=image src=https://i.imgur.com/1BUegLB.png></p><p><a href=https://github.com/juftin/llm-term><img loading=lazy src="https://img.shields.io/pypi/v/llm-term?color=blue&amp;label=%f0%9f%a4%96%20llm-term" alt=PyPI>
</a><a href=https://pypi.python.org/pypi/llm-term/><img loading=lazy src=https://img.shields.io/pypi/pyversions/llm-term alt="PyPI - Python Version"></a></p><details><summary>Screen Recording</summary><video controls>
<source src=https://user-images.githubusercontent.com/49741340/270871763-d872650e-bceb-4da3-8bc6-3e079d55e5a3.mov type=video/mp4>Your browser does not support the video tag.</video></details><h2 id=installation>Installation<a hidden class=anchor aria-hidden=true href=#installation>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pipx install llm-term
</span></span></code></pre></div><h2 id=usage>Usage<a hidden class=anchor aria-hidden=true href=#usage>#</a></h2><p>Then, you can chat with the model directly from the command line:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>llm-term
</span></span></code></pre></div><p><code>llm-term</code> works with multiple LLM providers, but by default it uses OpenAI.
Most providers require extra packages to be installed, so make sure you
read the <a href=#providers>Providers</a> section below. To use a different provider, you
can set the <code>--provider</code> / <code>-p</code> flag:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>llm-term --provider anthropic
</span></span></code></pre></div><p>If needed, make sure you have your LLM&rsquo;s API key set as an environment variable
(this can also set via the <code>--api-key</code> / <code>-k</code> flag in the CLI). If your LLM uses
a particular environment variable for its API key, such as <code>OPENAI_API_KEY</code>,
that will be detected automatically.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>export LLM_API_KEY<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;xxxxxxxxxxxxxx&#34;</span>
</span></span></code></pre></div><p>Optionally, you can set a custom model. llm-term defaults
to <code>gpt-3.5-turbo</code> (this can also set via the <code>--model</code> / <code>-m</code> flag in the CLI):</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>export LLM_MODEL<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;gpt-4&#34;</span>
</span></span></code></pre></div><p>Want to start the conversion directly from the command line? No problem,
just pass your prompt to <code>llm-term</code>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>llm-term show me python code to detect a palindrome
</span></span></code></pre></div><p>You can also set a custom system prompt. llm-term defaults to a reasonable
prompt for chatting with the model, but you can set your own prompt (this
can also set via the <code>--system</code> / <code>-s</code> flag in the CLI):</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>export LLM_SYSTEM_MESSAGE<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;You are a helpful assistant who talks like a pirate.&#34;</span>
</span></span></code></pre></div><h2 id=providers>Providers<a hidden class=anchor aria-hidden=true href=#providers>#</a></h2><h3 id=openai>OpenAI<a hidden class=anchor aria-hidden=true href=#openai>#</a></h3><p>By default, llm-term uses OpenAI as your LLM provider. The default model is
<code>gpt-3.5-turbo</code> and you can also use the <code>OPENAI_API_KEY</code> environment variable
to set your API key.</p><h3 id=anthropic>Anthropic<a hidden class=anchor aria-hidden=true href=#anthropic>#</a></h3><p>Anthropic is a new LLM provider that is currently in private beta. You can
request access to the beta <a href=https://www.anthropic.com/>here</a>. The default
model is <code>claude</code>, and you can use the <code>ANTHROPIC_API_KEY</code> environment variable.
To use <code>anthropic</code> as your provider you must install the <code>anthropic</code> extra.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>pipx install <span style=color:#e6db74>&#34;llm-term[anthropic]&#34;</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>llm-term --provider anthropic
</span></span></code></pre></div><h3 id=gpt4all>GPT4All<a hidden class=anchor aria-hidden=true href=#gpt4all>#</a></h3><p>GPT4All is a an open source LLM provider. These models run locally on your
machine, so you don&rsquo;t need to worry about API keys or rate limits. The default
model is <code>mistral-7b-openorca.Q4_0.gguf</code>, and you can see what models are available on the <a href=https://gpt4all.io/index.html>GPT4All
Website</a>. Models are downloaded automatically when you first use them.
To use GPT4All as your provider you must install the <code>gpt4all</code> extra.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pipx install <span style=color:#e6db74>&#34;llm-term[gpt4all]&#34;</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>llm-term --provider gpt4all --model mistral-7b-openorca.Q4_0.gguf
</span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/project/>Project</a></li><li><a href=http://localhost:1313/tags/llm-term/>Llm-Term</a></li></ul></footer></article></main><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><footer class=footer><span>&copy; 2024 <a href=http://localhost:1313/>Laoluo's Blog</a></span>
<span>| Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span><br><a href=https://beian.miit.gov.cn/ target=_blank>黔ICP备2022001597号</a></footer><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>