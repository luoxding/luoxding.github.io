<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Pytorch Tensor 基本操作 | Laoluo's Blog</title>
<meta name=keywords content="PyTorch"><meta name=description content="本文介绍了 PyTorch 中 Tensor 的一些常用操作。"><meta name=author content="James"><link rel=canonical href=http://localhost:1313/pytorch-tensor/><link crossorigin=anonymous href=/assets/css/stylesheet.d2540e28ae03a9ed42429ed74228412a18d4ea29c639e31e4f0b1512e252624f.css integrity="sha256-0lQOKK4Dqe1CQp7XQihBKhjU6inGOeMeTwsVEuJSYk8=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=http://localhost:1313/icons/linux.png><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/pytorch-tensor/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/lxgw-wenkai-screen-webfont@1.7.0/style.min.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fontsource/cascadia-code@4.2.1/index.min.css><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><meta property="og:title" content="Pytorch Tensor 基本操作"><meta property="og:description" content="本文介绍了 PyTorch 中 Tensor 的一些常用操作。"><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/pytorch-tensor/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-07-15T17:44:51+08:00"><meta property="article:modified_time" content="2023-07-15T17:44:51+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Pytorch Tensor 基本操作"><meta name=twitter:description content="本文介绍了 PyTorch 中 Tensor 的一些常用操作。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"Pytorch Tensor 基本操作","item":"http://localhost:1313/pytorch-tensor/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Pytorch Tensor 基本操作","name":"Pytorch Tensor 基本操作","description":"本文介绍了 PyTorch 中 Tensor 的一些常用操作。\n","keywords":["PyTorch"],"articleBody":"本文介绍了 PyTorch 中 Tensor 的一些常用操作。\n1. 初始化 操作 效果 ones(3, 4) 全 1 zeros(3, 4) 全 0 eye(3) 对角阵 arange(0, 5, 1) 从 0 到 4 步长为 1 rand(3, 4) 0 到 1 随机数 randn(3, 4) 正态分布随机数 2. 基本属性 操作 效果 shape 形状 numel() 元素个数 max() 最大值 device 所在的位置 abs() 绝对值 grad 梯度 reshape() 改变形状 在 reshape 时，我们可以通过 -1 来自动计算维度。如 x 为 size([12]) 的 tensor，x.reshape([3, -1])，可以得到 size([3, 4])。而在使用下标访问 tensor 元素时，-1 表示最后一个元素。如上文的 x[-1][-1] 就表示 x[2][3]。\n另外，函数名后加下划线意为原地操作，如 abs_() 为 abs() 的原地版本\n3. 拼接（cat/stack） cat 在现有维度上进行拼接，stack 在新的维度上进行拼接。\na = torch.rand(3, 4) b = torch.rand(3, 4) c = torch.cat((a, b), dim=0) d = torch.stack((a, b), dim=0) print(c.shape) # torch.Size([6, 4]) print(d.shape) # torch.Size([2, 3, 4]) 4. 广播机制 广播机制即对两个形状不同的矩阵来进行按元素操作，如 a 为 size([3, 1])，b 为 size([1, 2])，a + b 会将两个矩阵广播为一个 size([3, 2]) 的矩阵，即复制 a 的行，而复制 b 的列，来进行按元素运算。\na = torch.arange(3).reshape((3, 1)) b = torch.arange(2).reshape((1, 2)) print(a + b) # tensor([[0, 1], [1, 2], [2, 3]]) 5. 基本计算 求和 包括按所有轴求和，按指定轴求和，以及保持维度求和。\nA = torch.tensor([[1, 2], [3, 4]]) print(A.sum()) # tensor(10) print(A.sum(axis=0)) # tensor([4, 6]) print(A.sum(axis=0, keepdim=True)) # tensor([[4, 6]]) 按元素运算 对任意相同形状的 tensor，我们都可以使用常见的运算符（+，-，*，/和**，其中 ** 为求幂运算）来进行按元素运算。 除此之外，按元素还可以使用 exp() 自然指数这样的一元运算符。\nx = torch.tensor([1.0, 2, 4, 8]) y = torch.tensor([2, 2, 2, 2]) x + y, x - y, x * y, x / y, x ** y, x.exp() 上述结果分别为\ntensor([ 3., 4., 6., 10.]), tensor([-1., 0., 2., 6.]), tensor([ 2., 4., 8., 16.]), tensor([0.5000, 1.0000, 2.0000, 4.0000]), tensor([ 1., 4., 16., 64.]) tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03]) 乘法 包括按元素乘法，向量的数量积，向量和矩阵的乘法以及矩阵和矩阵的乘法。\n向量之间的乘法\nx = torch.tensor([1, 2]) y = torch.tensor([3, 4]) print(x * y) # tensor([3, 8]) print(x.dot(y)) # tensor(11) 向量和矩阵的乘法，注意这里的按元素乘法使用了广播机制。\nA = torch.tensor([[1, 2], [3, 4]]) x = torch.tensor([1, 2]) print(A * x) # tensor([[1, 4], [3, 8]]) print(A.mv(x)) # tensor([ 5, 11]) 矩阵之间的乘法\nA = torch.tensor([[1, 2], [3, 4]]) B = torch.tensor([[1, -1], [1, -1]]) print(A * B) # tensor([[ 1, -2], [ 3, -4]]) print(A.mm(B)) print(A @ B) # 上面两句均表示矩阵乘法 # tensor([[ 3, -3], [ 7, -7]]) 6. 自动求导 $$ y = e^{x_1} + e^{x_2} + e^{x_3}$$ $$ y’_{x_1} = e^{x_1}$$\nx = torch.arange(4.0, requires_grad=True) print(x) # tensor([0., 1., 2., 3.], requires_grad=True) y = x.exp() print(y) # tensor([ 1.0000, 2.7183, 7.3891, 20.0855], grad_fn=) y.sum().backward() print(x.grad) # tensor([ 1.0000, 2.7183, 7.3891, 20.0855]) $$ y = x_1^2 + x_2^2 + x_3^3 $$ $$ y’_{x_1} = 2x_1 $$\nx = torch.arange(4.0, requires_grad=True) print(x) # tensor([0., 1., 2., 3.], requires_grad=True) y = x * x print(y) # tensor([0., 1., 4., 9.], grad_fn=) y.sum().backward() print(x.grad) # tensor([0., 2., 4., 6.]) 注意，只有标量的输出才能求梯度，因此我们在对 y 反向传播求梯度之前要先求和。\n","wordCount":"406","inLanguage":"en","datePublished":"2023-07-15T17:44:51+08:00","dateModified":"2023-07-15T17:44:51+08:00","author":{"@type":"Person","name":"James"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/pytorch-tensor/"},"publisher":{"@type":"Organization","name":"Laoluo's Blog","logo":{"@type":"ImageObject","url":"http://localhost:1313/icons/linux.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Laoluo's Blog (Alt + H)">Laoluo's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=http://localhost:1313/about/ title=关于><span>关于</span></a></li><li><a href=http://localhost:1313/archives title=归档><span>归档</span></a></li><li><a href=http://localhost:1313/categories/ title=分类><span>分类</span></a></li><li><a href=http://localhost:1313/tags/ title=标签><span>标签</span></a></li><li><a href=http://localhost:1313/manual/ title=速查><span>速查</span></a></li><li><a href=http://localhost:1313/links/ title=链接><span>链接</span></a></li><li><a href=http://localhost:1313/search/ title=🔍><span>🔍</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Pytorch Tensor 基本操作</h1><div class=post-meta><span title='2023-07-15 17:44:51 +0800 CST'>July 15, 2023</span>&nbsp;·&nbsp;James</div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#1-%e5%88%9d%e5%a7%8b%e5%8c%96 aria-label="1. 初始化">1. 初始化</a></li><li><a href=#2-%e5%9f%ba%e6%9c%ac%e5%b1%9e%e6%80%a7 aria-label="2. 基本属性">2. 基本属性</a></li><li><a href=#3-%e6%8b%bc%e6%8e%a5catstack aria-label="3. 拼接（cat/stack）">3. 拼接（cat/stack）</a></li><li><a href=#4-%e5%b9%bf%e6%92%ad%e6%9c%ba%e5%88%b6 aria-label="4. 广播机制">4. 广播机制</a></li><li><a href=#5-%e5%9f%ba%e6%9c%ac%e8%ae%a1%e7%ae%97 aria-label="5. 基本计算">5. 基本计算</a><ul><li><a href=#%e6%b1%82%e5%92%8c aria-label=求和>求和</a></li><li><a href=#%e6%8c%89%e5%85%83%e7%b4%a0%e8%bf%90%e7%ae%97 aria-label=按元素运算>按元素运算</a></li><li><a href=#%e4%b9%98%e6%b3%95 aria-label=乘法>乘法</a></li></ul></li><li><a href=#6-%e8%87%aa%e5%8a%a8%e6%b1%82%e5%af%bc aria-label="6. 自动求导">6. 自动求导</a></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><p>本文介绍了 PyTorch 中 Tensor 的一些常用操作。</p><h3 id=1-初始化>1. 初始化<a hidden class=anchor aria-hidden=true href=#1-初始化>#</a></h3><table><thead><tr><th>操作</th><th>效果</th></tr></thead><tbody><tr><td>ones(3, 4)</td><td>全 1</td></tr><tr><td>zeros(3, 4)</td><td>全 0</td></tr><tr><td>eye(3)</td><td>对角阵</td></tr><tr><td>arange(0, 5, 1)</td><td>从 0 到 4 步长为 1</td></tr><tr><td>rand(3, 4)</td><td>0 到 1 随机数</td></tr><tr><td>randn(3, 4)</td><td>正态分布随机数</td></tr></tbody></table><h3 id=2-基本属性>2. 基本属性<a hidden class=anchor aria-hidden=true href=#2-基本属性>#</a></h3><table><thead><tr><th>操作</th><th>效果</th></tr></thead><tbody><tr><td>shape</td><td>形状</td></tr><tr><td>numel()</td><td>元素个数</td></tr><tr><td>max()</td><td>最大值</td></tr><tr><td>device</td><td>所在的位置</td></tr><tr><td>abs()</td><td>绝对值</td></tr><tr><td>grad</td><td>梯度</td></tr><tr><td>reshape()</td><td>改变形状</td></tr></tbody></table><p>在 reshape 时，我们可以通过 -1 来自动计算维度。如 x 为 size([12]) 的 tensor，x.reshape([3, -1])，可以得到 size([3, 4])。而在使用下标访问 tensor 元素时，-1 表示最后一个元素。如上文的 x[-1][-1] 就表示 x[2][3]。</p><p>另外，函数名后加下划线意为原地操作，如 abs_() 为 abs() 的原地版本</p><h3 id=3-拼接catstack>3. 拼接（cat/stack）<a hidden class=anchor aria-hidden=true href=#3-拼接catstack>#</a></h3><p>cat 在现有维度上进行拼接，stack 在新的维度上进行拼接。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>a <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>rand(<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>4</span>)
</span></span><span style=display:flex><span>b <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>rand(<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>4</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>c <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>cat((a, b), dim<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>d <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>stack((a, b), dim<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(c<span style=color:#f92672>.</span>shape)
</span></span><span style=display:flex><span><span style=color:#75715e># torch.Size([6, 4])</span>
</span></span><span style=display:flex><span>print(d<span style=color:#f92672>.</span>shape)
</span></span><span style=display:flex><span><span style=color:#75715e># torch.Size([2, 3, 4])</span>
</span></span></code></pre></div><h3 id=4-广播机制>4. 广播机制<a hidden class=anchor aria-hidden=true href=#4-广播机制>#</a></h3><p>广播机制即对两个形状不同的矩阵来进行按元素操作，如 a 为 size([3, 1])，b 为 size([1, 2])，a + b 会将两个矩阵广播为一个 size([3, 2]) 的矩阵，即复制 a 的行，而复制 b 的列，来进行按元素运算。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>a <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>3</span>)<span style=color:#f92672>.</span>reshape((<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>b <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>2</span>)<span style=color:#f92672>.</span>reshape((<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>))
</span></span><span style=display:flex><span>print(a <span style=color:#f92672>+</span> b)
</span></span><span style=display:flex><span><span style=color:#75715e># tensor([[0, 1], [1, 2], [2, 3]])</span>
</span></span></code></pre></div><h3 id=5-基本计算>5. 基本计算<a hidden class=anchor aria-hidden=true href=#5-基本计算>#</a></h3><h4 id=求和>求和<a hidden class=anchor aria-hidden=true href=#求和>#</a></h4><p>包括按所有轴求和，按指定轴求和，以及保持维度求和。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>A <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([[<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>], [<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>4</span>]])
</span></span><span style=display:flex><span>print(A<span style=color:#f92672>.</span>sum())
</span></span><span style=display:flex><span><span style=color:#75715e># tensor(10)</span>
</span></span><span style=display:flex><span>print(A<span style=color:#f92672>.</span>sum(axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>))
</span></span><span style=display:flex><span><span style=color:#75715e># tensor([4, 6])</span>
</span></span><span style=display:flex><span>print(A<span style=color:#f92672>.</span>sum(axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>, keepdim<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>))
</span></span><span style=display:flex><span><span style=color:#75715e># tensor([[4, 6]])</span>
</span></span></code></pre></div><h4 id=按元素运算>按元素运算<a hidden class=anchor aria-hidden=true href=#按元素运算>#</a></h4><p>对任意相同形状的 tensor，我们都可以使用常见的运算符（+，-，*，/和**，其中 ** 为求幂运算）来进行按元素运算。
除此之外，按元素还可以使用 exp() 自然指数这样的一元运算符。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([<span style=color:#ae81ff>1.0</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>8</span>])
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>])
</span></span><span style=display:flex><span>x <span style=color:#f92672>+</span> y, x <span style=color:#f92672>-</span> y, x <span style=color:#f92672>*</span> y, x <span style=color:#f92672>/</span> y, x <span style=color:#f92672>**</span> y, x<span style=color:#f92672>.</span>exp()
</span></span></code></pre></div><p>上述结果分别为</p><pre tabindex=0><code>tensor([ 3.,  4.,  6., 10.]),
tensor([-1.,  0.,  2.,  6.]),
tensor([ 2.,  4.,  8., 16.]),
tensor([0.5000, 1.0000, 2.0000, 4.0000]),
tensor([ 1.,  4., 16., 64.])
tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])
</code></pre><h4 id=乘法>乘法<a hidden class=anchor aria-hidden=true href=#乘法>#</a></h4><p>包括按元素乘法，向量的数量积，向量和矩阵的乘法以及矩阵和矩阵的乘法。</p><p>向量之间的乘法</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>])
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>4</span>])
</span></span><span style=display:flex><span>print(x <span style=color:#f92672>*</span> y)
</span></span><span style=display:flex><span><span style=color:#75715e># tensor([3, 8])</span>
</span></span><span style=display:flex><span>print(x<span style=color:#f92672>.</span>dot(y))
</span></span><span style=display:flex><span><span style=color:#75715e># tensor(11)</span>
</span></span></code></pre></div><p>向量和矩阵的乘法，注意这里的按元素乘法使用了广播机制。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>A <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([[<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>], [<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>4</span>]])
</span></span><span style=display:flex><span>x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>])
</span></span><span style=display:flex><span>print(A <span style=color:#f92672>*</span> x)
</span></span><span style=display:flex><span><span style=color:#75715e># tensor([[1, 4], [3, 8]])</span>
</span></span><span style=display:flex><span>print(A<span style=color:#f92672>.</span>mv(x))
</span></span><span style=display:flex><span><span style=color:#75715e># tensor([ 5, 11])</span>
</span></span></code></pre></div><p>矩阵之间的乘法</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>A <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([[<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>], [<span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>4</span>]])
</span></span><span style=display:flex><span>B <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([[<span style=color:#ae81ff>1</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>], [<span style=color:#ae81ff>1</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]])
</span></span><span style=display:flex><span>print(A <span style=color:#f92672>*</span> B)
</span></span><span style=display:flex><span><span style=color:#75715e># tensor([[ 1, -2], [ 3, -4]])</span>
</span></span><span style=display:flex><span>print(A<span style=color:#f92672>.</span>mm(B))
</span></span><span style=display:flex><span>print(A <span style=color:#f92672>@</span> B)
</span></span><span style=display:flex><span><span style=color:#75715e># 上面两句均表示矩阵乘法</span>
</span></span><span style=display:flex><span><span style=color:#75715e># tensor([[ 3, -3], [ 7, -7]])</span>
</span></span></code></pre></div><h3 id=6-自动求导>6. 自动求导<a hidden class=anchor aria-hidden=true href=#6-自动求导>#</a></h3><p>$$ y = e^{x_1} + e^{x_2} + e^{x_3}$$
$$ y&rsquo;_{x_1} = e^{x_1}$$</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>4.0</span>, requires_grad<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>print(x)
</span></span><span style=display:flex><span><span style=color:#75715e># tensor([0., 1., 2., 3.], requires_grad=True)</span>
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> x<span style=color:#f92672>.</span>exp()
</span></span><span style=display:flex><span>print(y)
</span></span><span style=display:flex><span><span style=color:#75715e># tensor([ 1.0000,  2.7183,  7.3891, 20.0855], grad_fn=&lt;ExpBackward0&gt;)</span>
</span></span><span style=display:flex><span>y<span style=color:#f92672>.</span>sum()<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>print(x<span style=color:#f92672>.</span>grad)
</span></span><span style=display:flex><span><span style=color:#75715e># tensor([ 1.0000,  2.7183,  7.3891, 20.0855])</span>
</span></span></code></pre></div><p>$$ y = x_1^2 + x_2^2 + x_3^3 $$
$$ y&rsquo;_{x_1} = 2x_1 $$</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>4.0</span>, requires_grad<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>print(x)
</span></span><span style=display:flex><span><span style=color:#75715e># tensor([0., 1., 2., 3.], requires_grad=True)</span>
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> x <span style=color:#f92672>*</span> x
</span></span><span style=display:flex><span>print(y)
</span></span><span style=display:flex><span><span style=color:#75715e># tensor([0., 1., 4., 9.], grad_fn=&lt;MulBackward0&gt;)</span>
</span></span><span style=display:flex><span>y<span style=color:#f92672>.</span>sum()<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>print(x<span style=color:#f92672>.</span>grad)
</span></span><span style=display:flex><span><span style=color:#75715e># tensor([0., 2., 4., 6.])</span>
</span></span></code></pre></div><p>注意，只有标量的输出才能求梯度，因此我们在对 y 反向传播求梯度之前要先求和。</p></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/pytorch/>PyTorch</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/clash-openai/><span class=title>« Prev</span><br><span>Clash for Windows OpenAI 分流</span>
</a><a class=next href=http://localhost:1313/vmware-p-core/><span class=title>Next »</span><br><span>VMware 虚拟机大小核调度问题</span></a></nav></footer></article></main><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><footer class=footer><span>&copy; 2024 <a href=http://localhost:1313/>Laoluo's Blog</a></span>
<span>| Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span><br><a href=https://beian.miit.gov.cn/ target=_blank>黔ICP备2022001597号</a></footer><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>